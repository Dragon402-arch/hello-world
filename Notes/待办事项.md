- 整理SFT、RLHF阶段模型训练的代码
- 整理前期工作中储备的资料
- 整理NLP领域中的经典任务以及应用场景，努力提升各种类型的项目经验。













面试经验：基础较为扎实，但缺乏项目经验，参与的NLP项目类型不多，场景问题回答的解决方案太过笼统。





TextRank算法，用于关键词抽取和对文档进行简明摘要。

在ChatGPT中，提问数学问题时，会给出整个问题的推导过程，这是因为加入了思维链，思维链可以用于提升大规模语言模型在算数、常识和符号上的推理能力，这种能力仅是在训练集中增加中问步骤的说明，无需重新训练或微调模型就可以获得。







































#### 一周待办事项

#### 简洁版

1. 模型量化：8-bit、4-bit
   - BaiChuan13B、ChatGLM2-6B涉及的量化
2. 模型微调：QLoRA、AdaLoRA（暂时先放放）原理
3. 模型训练：数据并行+模型并行（模型多卡分块运行）
4. 多轮对话数据处理
5. 模型细节：Falcon、BART、T5
6. 模型RLHF、PPO（强化学习）



多路并行：

- 数据并行

- 模型并行（流水线并行、张量并行）

  















CodeGeeX

https://github.com/THUDM/CodeGeeX/blob/main/codegeex/megatron/tools/pretrain_codegeex.py

https://github.com/THUDM/CodeGeeX/blob/main/codegeex/docker/Dockerfile

```shell
docker pull pytorch/pytorch:1.13.1-cuda11.6-cudnn8-runtime
```

降低显存占用方法：

- gradient checkpoint
- flash attention ：xformers



























- 模型细节：Falcon、BART、T5

- 模型微调：AdaLoRA、QLoRA原理

  资源：[QLoRA训练示例](https://github.com/shuxueslpi/chatGLM-6B-QLoRA/blob/main/train_qlora.py)

  [示例2](https://towardsdatascience.com/qlora-fine-tune-a-large-language-model-on-your-gpu-27bed5a03e2b)

- 模型训练：数据并行+模型并行（模型多卡分块运行）

  资源：[FSDP教程](https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html)、[FSDP进阶](https://pytorch.org/tutorials/intermediate/FSDP_adavnced_tutorial.html?highlight=transformer)、[模型分片作用](https://github.com/huggingface/transformers/issues/16884)、[讲解](https://www.guyuehome.com/39789)

  https://huggingface.co/docs/transformers/v4.15.0/parallelism

  https://medium.com/@esaliya/model-parallelism-in-deep-learning-is-not-what-you-think-94d2f81e82ed

  https://www.mishalaskin.com/posts/tensor_parallel

  

  

- 模型RLHF、PPO（强化学习）

- 多轮对话数据处理

  https://github.com/ssbuild/chatglm_finetuning/blob/dev/data_utils.py

  

- 剪切Attention head

  https://towardsdatascience.com/head-pruning-in-transformer-models-ec222ca9ece7

- 

- 

- 

- 

- 

- 

- 

- 

- 

- 

- 

- 

- ChatGLM模型生成输出实现代码细节、研究ChatGLM模型推理生成细节

- 完善GPT2训练代码

- 研究LLaMA模型细节

- 研究ChatGLM2、Bloom细节

- Flash Attention

- 优化器算法

- 研究ChatGLM模型代码，P-Tuning v2微调代码，模型生成回答部分代码

- BPE算法，WordPiece，SentencePiece，tokenizer的训练过程

- langchain使用学习

- 

- 模型加速推理方法：onnx、tensorRT、FastTransformer

- 量化4/8

- 模型bart、T5、

- 多轮对话生成模型训练数据代码

  

- 

- 网关、掩码、DNS关系













































待办事项规则

- 核心关键部分必须都要掌握，实用为主，而对于一般内容做到了解即可。
- 有抓有放，要有重点，不能一个都不放过。

与人交往相处规则

- 不要走极端，先暗示，再明示，





介绍一下杭州中奥科技有限公司

王宝强电影八角笼中票房

电影消失的她的剧情是什么

介绍一下情感分析任务

电影消失的她的票房有多少

















































#### 失败复盘

近距离与人相处，自我性格缺陷复盘

- 首先是对人期望过高，一旦别人的表现让自己不满意，就爱使性子，就爱生气，然后就开始意义用事，和别人对着干。（做事有点极端）
  - 首先是你对人有啥期望，你从来不表达出来，别人怎么知道你是咋想的，因此让人觉得你的生气有点莫名其妙。
  - 爱耍脾气，爱使性子。
- 只做选择，不做改变
  - 只会在心里衡量这个人和我是不是配合默契，是不是我的知己，是不是懂我，而不是改掉自己的坏毛病，让相处变得愉快
- 做事有点极端，不懂得变通
  - 投入就十分投入，退出就完全退出，完全没有考虑走其他的路。
  - **对人冷暴力**；遇到问题啦，不把问题讲出来，反倒是直接不再沟通啦，这一点很可怕。
- 对人不满，自己心里怎么想的，要不怕讲出来，不怕让人知道。
  - 比如对于刘露，我对其失望的地方在于：
    - 既然我和你不能有效沟通，我讲的话你都听不到心里去，那我就只能对你不再抱啥期望了
    - 你认为你不可能在一身债务的情况下组建家庭，那你有没有想过，即便你现在背债，我仍然不在乎，可是等到你境况好了，我又怎么可能厚着脸求你答应呢，（条件不好的时候都不选择你，条件好的时候恐怕更不会选择吧）既然这样还是各奔东西吧。
- 做事要善始善终，不要抱有幻想，要把话讲清楚
  - 首先你希望别人也主动一次，让我看到你的态度与诚意，然而令人遗憾的是，让你失望了。
  - 有啥不舒服的地方可以讲出来，让人知道自己是怎么死的，这也是顾全媒人脸面的做法。（我想了想，现在这么处下去没啥意思，感觉你很佛系，我不喜欢勉强别人，也不喜欢勉强自己，想想还是算了吧，就认识一下做个普通朋友吧，）
    - 除非是这个人你很讨厌，否则不要选择不再联系的做法
    - 把话讲清楚，做好收尾工作
- **说话过于含蓄，别人很难搞清楚你的意思**
  - 有的时候你不把话讲清楚，别人根本就不清楚你的意思、你的想法，那就很容易造成误会
  - 如果实在不能明着讲，那就只能先暗示，实在不行就明示。













































































#### 训练方式

- FSDP：（PyTorch）
- Megatron-LM：（NVIDIA）
- Deepspeed：（MicroSoft）
- SageMake：（Amazon）
- Mesh-TensorFlow ：（TensorFlow）
- MPU model parallel unit
- Megatron-DeepSpeed 最强组合

#### 并行类型

- Data Parallelism：Data parallelism refers to using multiple GPUs to increase the number of examples processed simultaneously.

  例如，原来一个GPU上batch_size=32，现在使用两个GPU，每个的batch_size都是32，则实际的batch_size=64

- Model  Parallelism：把模型的不同部分分发到不同的设备上。

  

#### 微调方法

- P-Tuning
- Prefix-Tuning
- LoRA：Low-Rank Adaptation (LoRA)

#### 多机多卡

- 使用deepspeed
- 不使用deepspeed

#### 推理加速

- 模型压缩
  - 量化
  - 剪切
  - 蒸馏
- 推理加速
  - ONNXRuntime
  - tensorRT
  - openvino
  - TorchScript（paddlepaddle，PyTorch，测试一下效果）

#### 大模型训练

- 机器互联、访存优化，模型参数存储
- 训练中断，恢复训练
- 显存带宽的I/O、 IB网络的I/O，显存利用率并非100%
- 多久做一次checkpointing



- ChatGPT
  - SFT：supervised fine-tuning
  - RM：reward model training
  - PPO：近端策略优化模型( reinforcement learning via proximal policy optimization)










#### 信息抽取

One of the common categories of problems encountered in the industry pertains to information extraction. How do we **extract entities** (person names, product names, etc.)**, events, and relations** from text? How do we **map the entity mentions in text to the entries in a knowledge base** (aka **entity discovery, entity linking, slot-filling**)? How do we **build and maintain that knowledge base** in the first place (knowledge base population)? These are a few of the questions routinely answered in information extraction research in different contexts.

#### 待完成任务

1. AI studio 关系抽取改写为 torch 版本，https://aistudio.baidu.com/aistudio/projectdetail/1639963?channelType=0&channel=0

2. GPT-2中文模型训练代码改写

3. ONNXRuntime and TorchScript

4. 尝试写出一个3x2网络结构的反向传播算法过程。

5. 跑通原始版本神经网络算法训练过程。

6. BERT：gradient_checkpointing，cross_attention，past_key_values的作用
   - 当设置 use_cache = False 时，也就是不使用 past_key_values，此时每次都从新计算
   - 当设置 use_cache = True 时，在计算第一个next token时，past_key_values没有存储，是None，等到计算第二个next_token时，就会使用 past_key_values，此时就可以避免重复计算。
   - cross_attention在decoder中输入encoder_hidden_states时使用，也就是经典的transformer结构解码器的计算。

7. 提示微调、指令微调、上下文学习之间的区别是啥。

   1. in-context learning：不更新模型参数，只给模型输入任务说明以及几个样本示例，然后进行预测，具备小样本学习能力。

   2. prompt tuning：更新模型参数。

   3. instructing tuning：更新模型参数

      

8. 出现顺序
   - In-Context Learning：（GPT-3：Language Models are Few-Shot Learners，2020）提出的概念
   - PET（Pattern-Exploiting Training）：Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference（2021）
   - Prompt Tuning
   - Instruction Tuning（在Prompt Tuning基础上增强）

9. 改善神经网络模型性能的相关方面

   - 激活函数选取
   - 损失函数选取
   - 权重初始化方法选取
   - 超参数选取

