- 系统目录介绍

  ![image-20220926224049159](C:\Users\千江映月\AppData\Roaming\Typora\typora-user-images\image-20220926224049159.png)
  
- 创建新用户，并添加root权限

  ```shell
  # 添加用户名
  adduser lee
  # 设置密码
  passwd lee
  
  # 提示两次输入新密码
  
  # 添加可写权限 
  chmod 640 /etc/sudoers
  
  vi /etc/sudoers
  
  # 添加一行     
  lee     ALL=(ALL)       ALL
  
  # 改为只读权限
  chmod 440 /etc/sudoers
  
  ```

- 查看IP

  - 查看linux系统 ip
  
    ```shell
    # interface config
    ifconfig
    
    ip addr
    # ip address
    ```
  
  - 查看Windows系统 ip
  
    ```shell
    ipconfig
    ```
  
- 修改主机名

  ```shell
  vi /etc/hostname
  
  # 集群不同节点添加对应名称
  HOSTNAME=Master
  
  
  vi /etc/hosts
  
  # 集群所有节点都添加
  192.168.56.104   Master
  192.168.56.105   Slave1
  
  
  # 重启使修改生效
  reboot
  
  # 测试是否相互 ping 得通
  ping Master -c 3   # 只ping 3次，否则要按 Ctrl+c 中断
  ping Slave1 -c 3
  ```

  

- 安装java jdk

  ```shell
  # 安装
  rpm -ivh jdk-8u281-linux-x64.rpm
  
  # 检验
  java -version
  ```

  

- 对指定文件为当前用户添加操作权限

  ```shell
  sudo chown -R hadoop:hadoop  /opt/hadoop-2.7.1
  ```

  

- 关闭防火墙

  ```shell
  sudo systemctl stop firewalld.service
  sudo systemctl disable firewalld.service
  ```

  

- 关闭Selinux

- 添加hadoop 环境变量

  ```shell
  # 新建文件
  sudo vim /etc/profile.d/hadoop.sh
  export HADOOP_HOME=/opt/hadoop-2.7.1/
  export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
  
  # 使其生效
  source /etc/profile
  ```

  

- 创建 HDFS的DataNode和NameNode的工作目录

  ```shell
  sudo mkdir /var/big_data
  sudo chown -R hadoop:hadoop /var/big_data
  ```

  

- 配置

  - 配置路径

    ```shell
    tar -zxvf hadoop-2.7.1.tar.gz -C /opt/
    
    /opt/hadoop-2.7.1/etc/hadoop/
    ```

    

  - core-site.xml 

  ```shell
  <property>
          <name>fs.default.name</name>
          <value>hdfs://master:9000</value>
          <description>HDFS的URI，文件系统://namenode标识:端口号</description>
      </property>
      <property>
          <name>hadoop.tmp.dir</name>
          <value>/usr/hadoop/tmp</value>
          <description>namenode上本地的hadoop临时文件夹</description>
      </property>
  
  ```

  - hdfs-site.xml 

  ```shell
      <property>
          <name>dfs.replication</name>
          <value>3</value>
          <description>副本个数，配置默认是3,应小于datanode机器数量</description>
      </property>
      <property>
          <name>dfs.namenode.secondary.http-address</name>
          <value>namenode02:50090</value>
          <description>副本个数，配置默认是3,应小于datanode机器数量</description>
      </property>
  </configuration>
  ```

  - mapred-site.xml 

    ```shell
    <configuration>
        <property>
                <name>mapreduce.framework.name</name>
                <value>yarn</value>
        </property>
    </configuration>
    ```

    

  - yarn-site.xml 

    ```shell
    <property>
            <name>yarn.nodemanager.aux-services</name>
            <value>mapreduce_shuffle</value>
    </property>
    <property>
            <name>yarn.resourcemanager.webapp.address</name>
            <value>192.168.56.104:8099</value>
    </property>
    <property>
            <name>yarn.resourcemanager.hostname</name>
            <value>master</value>
    </property>
    ```

    

  - 配置datanode节点信息

    ```shell
    vim slaves
    
    master 
    node01
    node02
    ```

    

  - 域名映射

    ```shell
    192.168.56.104 master
    192.168.56.105 node01
    192.168.56.106 node02
    
    ```

    

  - 

- 修改主机名和IP映射

  ```shell
  # 修改主机名
  vim /etc/hostname
  
  # 写法一
  localhost.localdomain
  HOSTNAME=node02
  
  # 写法二
  node02
  
  
  # 修改主机名和IP映射
  vim /etc/hosts
  
  
  ```

  

  

- 免密登录

  ```shell
  ssh-keygen -t rsa
  
  
  ssh-copy-id master
  
  # 验证一下
  ls -a
  
  cd .ssh
  
  ls
  # 可以看到只有master下面有authorized_keys文件
  
  
  
  ssh-copy-id node01
  ssh-copy-id node02
  
  # 在master服务器上登录node01节点，
  ssh node01
  
  # 退出
  exit
  
  ```

  

- 启动：[参考](https://www.cnblogs.com/chen-lhx/p/6867257.html)

  ```shell
  # 第一步:格式化namenode
  hdfs namenode -format
  
  
  # 验证
  ls /var/big_data/dfs/name/current
  
  # 输出
  fsimage_0000000000000000000  fsimage_0000000000000000000.md5  seen_txid  VERSION
  ```

  ```shell
  # 第二步:启动NameNode 和 DataNode 守护进程
  start-dfs.sh
  
  # 输出
  Starting namenodes on [master]
  master: starting namenode, logging to /opt/hadoop-2.7.1/logs/hadoop-hadoop-namenode-master.out
  node02: starting datanode, logging to /opt/hadoop-2.7.1/logs/hadoop-hadoop-datanode-node02.out
  node01: starting datanode, logging to /opt/hadoop-2.7.1/logs/hadoop-hadoop-datanode-node01.out
  master: starting datanode, logging to /opt/hadoop-2.7.1/logs/hadoop-hadoop-datanode-master.out
  Starting secondary namenodes [node02]
  node02: starting secondarynamenode, logging to /opt/hadoop-2.7.1/logs/hadoop-hadoop-secondarynamenode-node02.out
  
  
  ```

  ```shell
  # 第三步：启动ResourceManager 和 NodeManager 守护进程
  start-yarn.sh
  
  # 输出
  starting yarn daemons
  starting resourcemanager, logging to /opt/hadoop-2.7.1/logs/yarn-hadoop-resourcemanager-master.out
  node01: starting nodemanager, logging to /opt/hadoop-2.7.1/logs/yarn-hadoop-nodemanager-node01.out
  node02: starting nodemanager, logging to /opt/hadoop-2.7.1/logs/yarn-hadoop-nodemanager-node02.out
  master: starting nodemanager, logging to /opt/hadoop-2.7.1/logs/yarn-hadoop-nodemanager-master.out
  
  ```

  启动验证：执行jps命令，有如下进程，说明Hadoop正常启动

  ```shell
  jps
  
  # 输出
  4389 DataNode
  5815 Jps
  5272 ResourceManager
  4287 NameNode
  5375 NodeManager
  
  ```

  

  - 前端页面测试

    ```shell
    192.168.56.104:50070
    
    # 创建文件，在前端页面的文件系统中可以看到
    hdfs dfs -mkdir /test_temp
    ```

    ![image-20220911181839585](C:\Users\千江映月\AppData\Roaming\Typora\typora-user-images\image-20220911181839585.png)

  - hdfs 文件系统操作：

    - hadoop dfs 与 hdfs dfs 命令的作用相同，只适用于HDFS文件系统
    - hadoop fs 则是适用于任何不同的文件系统，如本地文件系统和HDFS文件系统

    ```shell
    # 创建用户家目录
    hdfs dfs -mkdir -p /user/hadoop
     
    # 在家目录下创建文件夹
    hdfs dfs -mkdir input
    
    # 查看家目录下面的文件夹
    hdfs dfs -ls .
    
    # 在根目录下面创建文件夹
    hdfs dfs -mkdir /test_temp
    
    #查看根目录下面的文件夹
    hdfs dfs -ls /
    
    # 删除根目录下面的文件夹
    hdfs dfs -rm -r /test_temp
    
    # 将linux本地文件上传到hdfs文件系统中
    hdfs dfs -put ~/test.txt input
    # 将linux系统当前用户目录下面的test.txt文件上传到hdfs当前用户的input文件夹下，都使用的是相对路径。
    
    # 查看hdfs文件系统的当前用户的input文件夹下有无test.txt文件
    hdfs dfs -ls input
    
    # 下载文件到本地
    hdfs dfs -get input/test.txt ~/
    hdfs dfs -get input/test.txt ~/BigData/
    # 输出 22/09/11 21:20:33 WARN hdfs.DFSClient: DFSInputStream has been closed already
    
    # hdfs 系统中复制文件到不同的目录下
    
    hdfs dfs -cp input/test.txt /test_temp
    
    # 验证
    hdfs dfs -ls /test_temp
    ```

    

  - 

- 

