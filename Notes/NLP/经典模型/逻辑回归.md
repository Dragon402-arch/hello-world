### Logistic Regression

假定：
$$
P(y=1|x;\theta)=h_{\theta}(x) = \frac{1}{1+e^{-\theta^Tx}}
$$
则有：
$$
P(y=0|x;\theta)=1 - h_{\theta}(x)
$$
合并上述两种情况则有：
$$
P(y|x;\theta)={h_{\theta}(x)}^y{(1-h_{\theta}(x))}^{1-y}
$$
给定	数据集$\{(x_i,y_i)\}_{i=1}^n$，可以通过极大似然法（MLE）来估计参数，可以使用负对数似然（negative log-likelihood，NLL）来代替，其形式如下：
$$
\begin{aligned} L(\theta) &= -\ln{\prod_{i=1}^nP(y|x_i;\theta)} \\
&= -\sum_{i=1}^n\ln{P(y_i|x_i;\theta)} \\
&= \large -\sum_{i=1}^n\ln{{h_{\theta}(x_i)}^{y_i}{(1-h_{\theta}(x_i))}^{1-y_i}} \\
&= \large -\sum_{i=1}^n\bigg[y_i\ln{{h_{\theta}(x_i)}+(1-y_i)\ln{(1-h_{\theta}(x_i))}}\bigg] \\
\end {aligned}
$$
损失函数关于参数 $\theta_j$ 的梯度为：
$$
\begin{aligned} \frac{∂L(\theta)}{∂\theta_j} &= -\sum_{i=1}^n\bigg[y_ix_{ij}(1-h(x_i)-(1-y_i)h(x_i)x_{ij}) \bigg] \\
 &= -\sum_{i=1}^n\bigg[x_{ij}(y_i-h(x_i))\bigg] \\
\end {aligned}
$$
在 `Mini-batch Gradient Descent` 下的参数更新规则为：
$$
\begin{aligned} \theta^{(t)}_j &= \theta^{(t-1)}_j  - \lambda ·\frac{∂L(\theta)}{∂\theta_j} \\
 &= \theta^{(t-1)}_j  - \lambda ·\sum_{i=1}^b\bigg[x_{ij}(h(x_i)-y_i)\bigg] \\
\end {aligned}
$$
其中 $i$ 表示第 $i$ 个样本，$j$ 表示样本的第 $j$ 个特征。

相关概念：

概率：
$$
p  =\large \frac{1}{ 1+e^{-\theta^Tx}}
$$
对数几率：
$$
\ln \frac{p}{1-p} = \theta^Tx
$$
