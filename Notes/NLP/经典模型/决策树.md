## Decision Tree 

决策树学习算法包含**特征选择**、**决策树生成**和**决策树剪枝**三个关键步骤。

### 1. 特征选择

#### 1.1 信息熵

**信息熵**（Information Entropy）是度量样本集合纯度最常用的一种指标，其数值越小，纯度越高，最小值为0，最大值为$\log_2K$。信息熵也被称为**经验熵**。
$$
Ent(D) = -\sum_{k=1}^Kp_k\log_2 p_k
$$
其中 $p_k$ 表示第 $k$ 类样本占样本结合 $D$ 的比例。

交叉熵 = 经验熵 + KL散度

#### 1.2 信息增益

信息增益（Information Gain），假定离散属性 $a$ 共有 $V$ 个可能的取值，则使用属性 $a$ 对样本集合 $D$ 进行划分所获得的信息增益可以定义为：
$$
Gain(D,a) = Ent(D) - \sum_{v=1}^V\frac{|D_v|}{|D|}Ent(D_v)
$$
信息增益越大，使用该属性对样本集合进行划分所获得的纯度提升也越大。

**著名的 ID3 决策树学习算法就是以信息增益为准则来进行特征选择的，从而选择最优划分属性。**

缺陷：对取值数目较多的属性有所偏好。

#### 1.3 信息增益比

信息增益比率（Information Gain Ratio），
$$
GainRatio(D,a) = \frac{Gain(D,a)}{IV(a)} \\
IV(a) = -\sum_{v=1}^V\frac{|D_v|}{|D|}\log_2\frac{|D_v|}{|D|}
$$
公式注解：按照属性 $a$ 的  $V$ 个可能的取值将样本集合划分为 $V$份，计算每份样本占样本集合的比例 $\frac{|D_v|}{|D|}$。

缺陷：对取值数目较少的属性有所偏好。

**C4.5算法并不是直接使用信息增益比为准则来进行特征选择的，而是先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择信息增益比最高的。**

#### 1.4 基尼指数

基尼值定义为：
$$
Gini(D) = \sum_{k=1}^Kp_k(1-p_k) = 1 - \sum_{k=1}^Kp_k^2
$$
$Gini(D)$ 值越小，则数据集D的纯度越高。

基尼指数（Gini Index）定义为：
$$
{\rm Gini Index}(D,a) =-\sum_{v=1}^V\frac{|D_v|}{|D|}Gini(D_v)
$$
**CART决策树（分类树生成时）使用了基尼指数作为准则来进行特征选择的，选取基尼指数最小的属性作为最优划分属性。**

### 2. 决策树剪枝

生成的决策树一般存在会存在过拟合问题，因此需要考虑降低决策树的复杂度，对决策树进行简化，也就是剪枝操作。

#### 2.1 预剪枝

在决策树生成过程中，对每个结点在划分前先进行估计，若当前结点的划分不能带来决策树泛化性能的提升，则停止划分并将当前结点标记为叶结点。

缺点：虽然可以降低过拟合风险，还显著减少了决策树的训练时间开销和测试时间开销，但是也会带来欠拟合的风险。

#### 2.2 后剪枝

先从训练集生成一颗完整的决策树，然后自底向上地对非叶结点进行考察，若将该结点对应的子树替换为叶结点能带来决策树泛化性能的提升，则将该子树替换为叶结点。

一般情况下，后剪枝决策树的欠拟合风险比较小，泛化性能往往优于预剪枝决策树，但后剪枝过程是在生成完决策树之后进行的，并且要自下而上地对树中的所有非叶结点进行逐一考察，因此其训练时间开销比未剪枝决策树和预剪枝决策树都要大得多。

### 3. CART 决策树生成

CART（Classification and Regression Tree）算法既可以用于回归任务，也可以用于分类任务。

CART算法由以下两个步骤组成：

- 决策树生成：基于训练数据集生成决策树，生成的决策树要尽量大。
- 决策树剪枝：用验证集对已经生成的决策树进行剪枝并选择最优子树，这时使用以损失最小化作为剪枝的标准。

#### 3.1 回归树生成算法

回归树生成算法默认使用了MSE作为误差损失函数，因此常称为**最小二乘回归树生成算法**。

给定训练数据集$ \{(x_1,y_1)\}_{i=1}^n$，其中 $x_i∈R^J$ ，也就是输入 $x_i$ 包含 $J$ 个特征。

在训练数据集所在的输入空间中，递归地将每个区域划分为两个子区域并决定每个子区域上的输出值，构建二叉决策树。

1. 选择最优切分特征 $j$ 与特征切分点 $s$ ，求解：
   $$
   \mathop{\min}\limits_{j,s} \bigg[\mathop{\min}\limits_{c_1} \sum_{x_i∈R_1(j,s)}(y_i-c_1)^2 + 
   \mathop{\min}\limits_{c_2} \sum_{x_i∈R_2(j,s)}(y_i-c_2)^2
   \bigg]
   $$
   遍历特征 $j$，对给定的切分特征 $j$ 扫描切分点 $s$，选择使得上式达到最小值的一对$(j,s)$。

2. 用选定的一对$(j,s)$划分区域并决定相应的输出值：
   $$
   R_1(j,s) = \{x|x^{(j)} \le s\},\ \   R_2(j,s) = \{x|x^{(j)} > s\}
   $$

   $$
   \hat c_m = \frac{1}{N_m} \sum_{x_i∈R_m(j,s)}y_i \ ,x∈R_m \ , m=1,2
   $$

3. 继续对两个子区域调用步骤1、2，直至满足停止条件。（经过步骤1和2，可以得到一个包含两个叶结点的树，然后再分别对两个叶结点使用步骤1和2，再次进行划分。）

4. 最终将输入空间划分为M个区域$R_1,R_2,…,R_M$，生成决策树：
   $$
   f(x) = \sum_{m=1}^M \hat c_mI(x∈R_m)
   $$

使用过程示例：（示例来源李航P168）

给定训练数据：

| $x_i$ | 1    | 2    | 3    | 4    | 5    | 6    | 7    | 8    | 9    | 10   |
| ----- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| $y_i$ | 5.56 | 5.70 | 5.91 | 6.40 | 6.80 | 7.05 | 8.90 | 8.70 | 9.00 | 9.05 |

由于训练数据只包含一个特征，因此可以省去遍历特征部分。根据所给数据，考虑如下切分点：$1.5, 2.5, 3.5, 4.5, 5.5, 6.5, 7.5, 8.5, 9.5$，对于各切分点，首先求出对应的 $R_1,R_2,c_1,c_2$ 以及最终的损失值：

```python
# 训练数据
data_dict = {1: 5.56,2: 5.70,3: 5.91,4: 6.40,5: 6.80,6: 7.05,7: 8.90,8: 8.70,9: 9.00,10: 9.05}

# 切分点
spots = [1.5, 2.5, 3.5, 4.5, 5.5, 6.5, 7.5, 8.5, 9.5]


def get_error(spot):
    r1 = []
    r2 = []
    for key, value in data_dict.items():
        if key <= spot:
            r1.append(value)
        else:
            r2.append(value)
    avg_r1 = sum(r1) / len(r1)
    avg_r2 = sum(r2) / len(r2)
    total_sum = 0
    for i in r1:
        total_sum += (i - avg_r1) ** 2
    for i in r2:
        total_sum += (i - avg_r2) ** 2
    return round(total_sum, 2) # 保留两位小数


print([get_error(spot) for spot in spots])
"""
[15.72, 12.08, 8.37, 5.78, 3.91, 1.93, 8.01, 11.74, 15.74]
avg_r1对应着c1，avg_r2对应着c2。
"""
```

可知当切分点 $s = 6.5 $ 时，损失或误差达到最小值，此时 $R_1=\{1,2,3,4,5,6\}，R_2=\{7,8,9,10\},c_1=6.24,c_2=8.91$，此时可得回归树：
$$
T(x) =\begin{cases}
6.24, & x \le 6.5 \\\\\
8.91, & x > 6.5 
\end{cases}
$$

#### 3.2 分类树生成算法

分类树以基尼指数为准则选择最优划分特征，同时决定该特征的最优切分点。

