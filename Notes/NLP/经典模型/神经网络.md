### 1. 前馈神经网络

前馈神经网络（Feedforward Neural Network，FNN），又称为人工神经网络。

#### 1.1 选取损失函数

##### 1.1.1 MSE 

MSE（mean squared error），又称为  quadratic cost function（平方差损失函数），计算公式如下：
$$
L(θ) = \frac{1}{2n}\sum_{x=1}^n||y - \hat{y}||^2
$$

其中 N表示训练样本的个数，$y$ 和 $\hat{y}$ 分别表示真实值和预测值，都是一个向量 。

##### 1.1.2 Binary Cross-Entropy Loss

$$
L_{CE} = -\frac{1}{N}\Bigg [\sum_{j=1}^N\Big[t_jlog(p_j)+(1-t_j)log(1-p_j)\Big]\Bigg ]
$$

For N data points  where $t_j$ is the truth value taking a value 0 or 1 and $p_j$ is the Softmax probablity for the `jth` data point.

#### 1.2 梯度下降算法

全微分在近似计算中的应用

当二元函数在 $z = f(x,y)$ 在 点$(x,y)$ 处的两个偏导数连续，并且 $|Δx|$  与  $|Δy|$ 都比较小时，有如下近似等式：
$$
Δz  ≈ \frac{∂z}{∂x} Δx + \frac{∂z}{∂y} Δy
$$
 对应到损失函数上，就是：
$$
ΔL  ≈ \frac{∂L}{∂θ_1} Δθ_1 + \frac{∂L}{∂θ_2} Δθ_2
$$
定义 $∇L=( \frac{∂L}{∂θ_1}, \frac{∂L}{∂θ_1})^T$ 表示梯度向量，$Δθ=(Δθ_1,Δθ_2)^T$  表示参数的变化量向量，根据定义上式可重写为：
$$
ΔL  ≈ ∇L ·Δθ
$$
This equation helps explain why $∇L$ is called the gradient vector: $∇L$ relates changes in $θ$ to changes in $L$.（$∇L$将 $θ$的变化关联到$L$的变化）同时由上式可知，how to choose  $Δθ$ so as to make $ΔL$ negative.

其中$ΔL$ 是一个标量，而 $∇L$  与 $Δθ$  都是向量。

假定我们选取：
$$
Δθ = −η ∇L
$$
其中，$η$  是一个较小且是一个正数的参数，又被称为学习率（ learning rate)。由上式可得：$ΔL≈ ∇L·Δθ=−η||∇L||^2$.由于$||∇L||^2\geq 0$，这可以保证了$ΔL\leq 0$，即 $L$ 总会下降，永远不会上升。由此可得参数更新规则：
$$
θ → θ_t = θ_{t-1} −η ∇L
$$
为了使梯度下降能够正确地运⾏，我们需要选择⾜够⼩的学习速率 *η* 使得等式能得到很好的近似。如果不这样，我们会以 ∆*C >* 0 结束，这显然不好。同时，我们也不想 *η* 太⼩，因为这会使 ∆*θ* 的变化极⼩，梯度下降算法就会运⾏得⾮常缓慢。

 Let's suppose that we're trying to make a move ∆*θ* in position so as to decrease $L$ as much as possible. This is equivalent to minimizing $ΔL  ≈ ∇L ·Δθ$. We'll constrain the size of the move so that $∥Δθ∥=ϵ$, for some small fixed $ϵ>0$. In other words, we want a move that is a small step of a fixed size, and we're trying to find the movement direction which decreases *L* as much as possible. It can be proved that the choice of  ∆*θ*  which minimizes $ΔL≈∇L ·Δθ$ is $Δθ=−η∇L$, where $η=ϵ/∥∇L∥$ is determined by the size constraint $∥Δθ∥=ϵ$. So gradient descent can be viewed as a way of taking small steps in the direction which does the most to immediately decrease $C$.

由柯西-施瓦茨不等式不等式（Cauchy-Schwarz inequality）可知：
$$
a · b \leq|a| · |b| ,a = (a_1,a_2,…,a_n),b = (b_1,b_2,…,b_n)
$$
当 $Δθ$ 与 $∇L$ 向量反向时，向量内积取得最小值（由于是一个负数，$ΔL$ 值约小，表示损失值下降得越多），此时$Δθ=-ϵ∇L/||∇L||=−η∇L$ ，此时可以将学习率定义为： $η=ϵ/∥∇L∥=||Δθ||/||∇L||$ ，在梯度向量$∇L$ 给定的情况下（上一轮更新后，使用更新后的参数值再次计算得到了损失值，此时梯度向量$∇L$已经是确定了的， 那么$||∇L||$的值是确定了的），如果在此时人为调大学习率 $η$，则 $∥Δθ∥$ 的值也将变大，则表示梯度下降过程中step就会变大。

#### 1.3 训练神经网络

##### 1.3.1 学习缓慢的问题：针对输出层神经元

在激活函数使用sigmoid、损失函数使用MSE进行二分类时，由于模型初始化权重不同，会有几次出现模型在最初训练阶段学习缓慢（也就是权重和偏置只发生了微小的变化）的问题，学习缓慢问题实际上是由于偏导数值较小导致的。

To understand the origin of the problem, consider that our neuron learns by changing the weight and bias at a rate determined by the partial derivatives of the cost function, ${∂L}/{∂w}$ and ${∂L}/{∂b}$. So saying "**learning is slow**" is really the same as saying that **those partial derivatives are small**. The challenge is to understand why they are small. 
$$
L = \frac{(y-a)^2}{2}
$$
其中 $a = σ(z)=σ(wx+b)$，当 x=1，y=0 ，则有 
$$
\frac{∂L}{∂w}=(a-y)σ^′(z)x=aσ^′(z)
$$

$$
\frac{∂L}{∂b}=(a-y)σ^′(z)=aσ^′(z)
$$

由sigmoid函数可知其导数为 $σ^′(x)=σ(x)(1-σ(x))$，可知当x为非常大正数或非常小负数时，其导数值$σ^′(x)$都接近于0，也就是梯度值接近于0，这就是学习缓慢的原因所在。可以通过改变损失函数来解决学习缓慢问题。

平方差损失函数对参数的偏导数中包含 $σ^′(x)$，从而导致了学习缓慢问题的出现。

##### 1.3.2 解决学习缓慢问题

假定 $a=σ(z)=σ(∑_jw_jx_j+b)$ ,损失函数为：
$$
C=-\frac{1}{n}\sum_{x}[ylna+(1−y)ln(1−a)]
$$

偏导数值为：
$$
\frac{∂C}{∂w_j}=\frac{1}{n}\sum_{x}x_j(σ(z)−y)
$$

$$
\frac{∂C}{∂b}=\frac{1}{n}\sum_{x}(σ(z)−y)
$$

It tells us that the rate at which the weight learns is controlled by $σ(z)−y$, i.e., by the error in the output. The larger the error, the faster the neuron will learn.In particular, it avoids the learning slowdown caused by the $σ^′(x)$​ term in the analogous equation for the quadratic cost.这个损失函数还避免了像在平方差损失函数中类似⽅程中 *σ* *′* (*z*) 导致的学习缓慢问题，因为该项并不存在与偏导数中。

如果输出神经元使用了sigmoid激活函数，此时应该选择交叉熵作为损失函数，而非平方差损失函数。如果输出神经元是线性的，此时选择平方差作为损失函数也是一种合理的选择。

This shows that if the output neurons are linear neurons then the quadratic cost will not give rise to any problems with a learning slowdown. In this case the quadratic cost is, in fact, an appropriate cost function to use.

- 使用组合
  -  log-likelihood cost function + softmax function，没有学习缓慢的问题
  - cross-entropy cost function + sigmoid function，没有学习缓慢的问题

##### 1.3.3 过拟合问题

Instead of using the `test_data` to prevent overfitting, we will use the `validation_data`. 通过使用验证集来防止过拟合，而不是使用测试集。为什么要使用验证集而非测试集来防止过拟合呢？

Why use the `validation_data` to prevent overfitting, rather than the `test_data`? In fact, this is part of a more general strategy, which is to use the `validation_data` to evaluate different trial choices of hyper-parameters such as the number of epochs to train for, the learning rate, the best network architecture, and so on. We use such evaluations to find and set good values for the hyper-parameters.

为何要使⽤验证集来替代测试集来防⽌过拟合问题？实际上，这是⼀个更为⼀般的策略的⼀部分，这个⼀般的策略就是使⽤ 验证集来衡量不同的超参数（如epoch，learning rate，最好的⽹络架构等等）的选择的效果。我们使⽤这样⽅法来找到超参数的合适值。

Of course, that doesn't in any way answer the question of why we're using the `validation_data` to prevent overfitting, rather than the `test_data`. Instead, it replaces it with a more general question, which is why we're using the `validation_data` rather than the `test_data` to set good hyper-parameters? To understand why, consider that when setting hyper-parameters we're likely to try many different choices for the hyper-parameters. **If we set the hyper-parameters based on evaluations of the `test_data` it's possible we'll end up overfitting our hyper-parameters to the `test_data`.** **That is, we may end up finding hyper-parameters which fit particular peculiarities of the `test_data`, but where the performance of the network won't generalize to other data sets.** We guard against that by figuring out the hyper-parameters using the `validation_data`. Then, once we've got the hyper-parameters we want, we do a final evaluation of accuracy using the `test_data`. That gives us confidence that our results on the `test_data` are a true measure of how well our neural network generalizes. To put it another way, you can think of the validation data as a type of training data that helps us learn good hyper-parameters. This approach to finding good hyper-parameters is sometimes known as the *hold out* method, since the `validation_data` is kept apart or "held out" from the `training_data`.

当然，这仍然没有回答为什么我们⽤ validation_data ⽽不是test_data 来防⽌过度拟合的问题。实际上，有⼀个更加⼀般的问题，就是为何⽤ validation_data 取代 test_data 来设置更好的超参数？为了理解这点，想想当设置超参数时，我们想要尝试许多不同的超参数选择。**如果我们设置超参数是基于 test_data 的话，可能最终我们就会得到过度拟合于test_data 的超参数。也就是说，我们可能会找到那些符合test_data 特点的超参数，但是⽹络的性能并不能够泛化到其他数据集合上。我们借助 validation_data 来克服这个问题。然后⼀旦获得了想要的超参数，最终我们就使⽤ test_data 进⾏准确率测量。**这给了我们在test_data 上的结果是⼀个⽹络泛化能⼒真正的度量⽅式的信⼼。换⾔之，你可以将验证集看成是⼀种特殊的训练数据集能够帮助我们学习好的超参数。这种寻找好的超参数的⽅法有时候被称为 hold out ⽅法，因为 validation_data是从traning_data 训练集中留出或者“拿出”的⼀部分。

**In general, one of the best ways of reducing overfitting is to increase the size of the training data.**With enough training data it is difficult for even a very large network to overfit.

- 正则化作用：
  - 防止过拟合，提升模型泛化性能
  - 增强模型效果复现性（由于模型参数随机初始化不同，有时会收敛到不同的局部最优解，模型效果相差较大）
  - 避免学到训练数据中的噪声

Suppose our network mostly has small weights, as will tend to happen in a regularized network. **The smallness of the weights means that the behaviour of the network won't change too much if we change a few random inputs here and there. That makes it difficult for a regularized network to learn the effects of local noise in the data**. Think of it as a way of making it so single pieces of evidence don't matter too much to the output of the network. **Instead, a regularized network learns to respond to types of evidence which are seen often across the training set.** By contrast, a network with large weights may change its behaviour quite a bit in response to small changes in the input. And so **an unregularized network can use large weights to learn a complex model that carries a lot of information about the noise in the training data.** In a nutshell, regularized networks are constrained to build relatively simple models based on patterns seen often in the training data, and are resistant to learning peculiarities of the noise in the training data.

假设神经⽹络⼤多数有很⼩的权重，这最可能出现在规范化的⽹络中。更⼩的权重意味着⽹络的⾏为不会因为我们随便改变了⼀个输⼊⽽改变太⼤。这会让规范化⽹络学习局部噪声的影响更加困难。将它看做是⼀种让单个的证据不会影响⽹络输出太多的⽅式。相对的，规范化⽹络学习去对整个训练集中经常出现的证据进⾏反应。对⽐看，⼤权重的⽹络可能会因为输⼊的微⼩改变⽽产⽣⽐较⼤的⾏为改变。所以⼀个⽆规范化的⽹络可以使⽤⼤的权重来学习包含训练数据中的噪声的⼤量信息的复杂模型。简⾔之，正则化化⽹络受限于根据训练数据中常⻅的模式来构造相对简单的模型，⽽能够抵抗训练数据中的噪声的特性影响。

- L1正则化

  L1 regularization tends to concentrate the weight of the network in a relatively small number of high-importance connections, while the other weights are driven toward zero.

- Dropout

  每个batch数据的训练，只对部分权重和偏置进行更新；相等于集成了多个不同结构的神经网络。

  We forward-propagate the input x through the modified network, and then backpropagate the result, also through the modified network. **After doing this over a mini-batch of examples, we update the appropriate weights and biases.** We then repeat the process, first restoring the dropout neurons, then choosing a new random subset of hidden neurons to delete, estimating the gradient for a different mini-batch, and updating the weights and biases in the network.

  By repeating this process over and over, our network will learn a set of weights and biases. Of course, those weights and biases will have been learnt under conditions in which half the hidden neurons were dropped out. When we actually run the full network that means that twice as many hidden neurons will be active. To compensate for that, we halve the weights outgoing from the hidden neurons.

  通过不断地重复，我们的⽹络会学到⼀个权重和偏置的集合。当然，这些权重和偏置是在⼀半的隐藏神经元被删除条件下学到的。当我们实际运⾏整个⽹络时，两倍的隐藏神经元将会被激活。为了补偿这个，我们将从所有隐藏层神经元的权重减半。

  因为神经元不能依赖其他神经元特定的存在，这个技术其实减少了复杂的互适应的神经元。所以，强制要学习那些在神经元的不同随机⼦集中更加健壮的特征。

- 数据增强

- 增加训练数据

##### 1.3.4 权重随机初始化

​	选择合适的权重初始化方式可以加快模型训练收敛的速度。**选择初始化方式不当，会导致隐藏层神经元饱和，进而出现学习缓慢的问题。**

- 作用
  - 加快训练速度
  - 提升模型性能

##### 1.3.5 超参数选取

**Learning rate schedule:** We've been holding the learning rate $η$ constant. However, it's often advantageous to vary the learning rate. **Early on during the learning process it's likely that the weights are badly wrong. And so it's best to use a large learning rate that causes the weights to change quickly. Later, we can reduce the learning rate as we make more fine-tuned adjustments to our weights.**

##### 1.3.6 梯度消失与梯度爆炸问题

梯度消失问题（Vanishing gradient problem），就是这样一个现象：In  some deep neural networks, the gradient tends to get smaller as we move backward through the hidden layers. This means that neurons in the earlier layers learn much more slowly than neurons in later layers.（在一些神经网络中，当我们对隐藏层进行反向传播时梯度倾向于变得更小。这意味着前层（earlier layers）神经元比后层（ later layers）神经元学习慢得多。）

梯度爆炸问题（Exploding gradient problem），也就是对隐藏层进行反向传播时，前层（earlier layers）权重的梯度倾向于变得非常大。

 More generally, it turns out that the gradient in deep neural networks is **unstable**, **tending to either explode or vanish in earlier layers. This instability is a fundamental problem for gradient-based learning in deep neural networks.** （基于梯度进行学习的深度神经网络中梯度的不稳定性是一个根本的问题。）

![简单神经网络](D:\Typora\Notes\NLP\经典模型\简单神经网络.png)

复合函数链式求导法则：

公式论证：
$$
\begin{aligned} \frac{∂C}{∂b_1} &= \frac{∂C}{∂a_4} ·\frac{∂a_4}{∂z_4} ·\frac{∂z_4}{∂a_3} ·\frac{∂a_3}{∂z_3} ·\frac{∂z_3}{∂a_2} ·\frac{∂a_2}{∂z_2} ·\frac{∂z_2}{∂a_1} ·\frac{∂a_1}{∂z_1} \\
& =\frac{∂C}{∂a_4} ·σ^′(z_4)·w_4·σ^′(z_3)·w_3·σ^′(z_2)·w_2·σ^′(z_1) \end{aligned}
$$

$$
\begin{aligned} \frac{∂C}{∂w_1} &= \frac{∂C}{∂a_4} ·\frac{∂a_4}{∂z_4} ·\frac{∂z_4}{∂a_3} ·\frac{∂a_3}{∂z_3} ·\frac{∂z_3}{∂a_2} ·\frac{∂a_2}{∂z_2} ·\frac{∂z_2}{∂a_1} ·\frac{∂a_1}{∂z_1}·\frac{∂z_1}{∂w_1} \\
& =\frac{∂C}{∂a_4} ·σ^′(z_4)·w_4·σ^′(z_3)·w_3·σ^′(z_2)·w_2·σ^′(z_1)·x \end{aligned}
$$

由于 $σ^′(x)\leq\frac{1}{4}$，若 $w_i\leq1$，则 $σ^′(z_i)·w_i\leq\frac{1}{4}$，由于每一项都小于1，数项连乘后乘积会指数级下降，且项数越多，乘积就越小。相反，如果$w_i$非常大，以至于$σ^′(z_i)·w_i\gg 1$，由于每一项都远大于1，数项连乘后乘积会指数级上升，且项数越多，乘积就越大。

**The unstable gradient problem:** The fundamental problem here isn't so much the vanishing gradient problem or the exploding gradient problem. **It's that the gradient in early layers is the product of terms from all the later layers.** When there are many layers, that's an intrinsically unstable situation.

### 2. 循环神经网络

[参考](https://dennybritz.com/posts/wildml/recurrent-neural-networks-tutorial-part-1/)

RNNs (Recurrent Neural Network, RNNs) are called *recurrent* because they perform the same task for every element of a sequence, with the output being depended on the previous computations. 

#### 2.1 计算过程

![image-20230521153641744](C:\Users\千江映月\AppData\Roaming\Typora\typora-user-images\image-20230521153641744.png)

计算隐藏状态:

 The first hidden state $s_0$ is typically initialized to all zeroes，$s_t$ is calculated based on the previous hidden state and the input at the current step：
$$
s_t = f(Ux_t + Ws_{t-1} + b)
$$
在前馈神经网络中，当前输出仅取决于当前输入，与之前的输入、输出都无关。而在循环神经网络中，由上式可知，当前输出不仅取决于当前输入，还取决于之前的所有输入。

计算预测输出:
$$
\hat{y}_t = o_t = softmax(Vs_t + c)
$$
其中 $x_t$ is the input at time step $t$.  $s_t$ is the hidden state at time step $t$. $o_t$ is the output at time step $t$.

代码实现：

```python
import torch
import torch.nn as nn


class ElmanRNN(nn.Module):
    """ an Elman RNN built using RNNCell """

    def __init__(self, input_size, hidden_size, batch_first=False):
        """
        Args:
        input_size (int): size of the input vectors
        hidden_size (int): size of the hidden state vectors
        batch_first (bool): whether the 0th dimension is batch
        """
        super(ElmanRNN, self).__init__()
        self.rnn_cell = nn.RNNCell(input_size, hidden_size)
        self.batch_first = batch_first
        self.hidden_size = hidden_size

    def _initialize_hidden(self, batch_size):
        return torch.zeros((batch_size, self.hidden_size))

    def forward(self, x_in, initial_hidden=None):
        """The forward pass of the ElmanRNN
        Args:
        x_in (torch.Tensor): an input data tensor.
        If self.batch_first: x_in.shape = (batch_size, seq_size, feat_size)
        Else: x_in.shape = (seq_size, batch_size, feat_size)
        initial_hidden (torch.Tensor): the initial hidden state for the RNN
        Returns:
        hiddens (torch.Tensor): The outputs of the RNN at each time step.
        If self.batch_first:
        hiddens.shape = (batch_size, seq_size, hidden_size)
        Else: hiddens.shape = (seq_size, batch_size, hidden_size)
        """

        if self.batch_first:
            batch_size, seq_size, feat_size = x_in.size()
            x_in = x_in.permute(1, 0, 2)
        else:
            seq_size, batch_size, feat_size = x_in.size()
        
        hiddens = []
        if initial_hidden is None:
            initial_hidden = self._initialize_hidden(batch_size)
        initial_hidden = initial_hidden.to(x_in.device)
        hidden_t = initial_hidden

        for t in range(seq_size):
            hidden_t = self.rnn_cell(x_in[t], hidden_t)
        hiddens.append(hidden_t)

        hiddens = torch.stack(hiddens)
        if self.batch_first:
            hiddens = hiddens.permute(1, 0, 2)
        return hiddens

```

#### 2.2 权重共享

 **The same parameters (matrices *U*,*V*,*W* ) are used at each time step.** Unlike a traditional deep neural network, which uses different parameters at each layer, **a RNN shares the same parameters (*U*,*V*,*W* above) across all steps**. This reflects the fact that we are performing the same task at each step, just with different inputs. This greatly reduces the total number of parameters we need to learn.

#### 2.3 BPTT

循环神经网络参数的学习使用随时间反向传播算法（Back Propagation Through Time，BPTT）

#### 2.4 RNN存在的问题

##### 2.4.1 无法建模长期依赖关系

Fail to capture long-range dependencies well or the inability to retain information for long-range predictions.

> The first issue with Elman RNNs is the difficulty in retaining long-range information.**With RNN,at each time step we simply updated the hidden state vector regardless of whether it made sense. As a consequence, the RNN has no control over which values are retained and which are discarded in the hidden state—that is entirely determined by the input.** Intuitively, that doesn’t make sense. What is desired is some way for the RNN to decide if the update is optional, or if the update happens, by how much and what parts of the state vector, and so on.
>
> 标准RNN的第一个问题是难以保留长期信息。对于标准RNN，在每个时间步，我们只是简单地更新隐藏状态向量，而不管它是否有意义。因此，RNN无法控制隐藏状态（其完全由当前时间步的输入决定）中保留哪些值，丢弃哪些值。直觉上，这没有意义。我们想要的是让RNN通过某种方式来决定是否进行更新（新增与删除），以及隐藏状态向量的多少和哪些部分进行更新，等等。

##### 2.4.2 gradient unstability

> The second issue with Elman RNNs is their tendency to cause gradients to spiral out of control to zero or to infinity.A really large absolute value of the gradient or a really small (less than 1) value can make the optimization procedure unstable.
>
> 标准RNN的第二个问题是其倾向于造成梯度失控，梯度值变为0或是无穷大，前者对应梯度消失，后者对应梯度爆炸。

梯度不稳定问题表现为梯度消失和梯度爆炸问题，其中梯度爆炸问题还可以使用梯度剪切（gradient clipping）方式进行缓解，而梯度消失问题则无法解决。

##### 2.4.3 问题解决方案——门控机制

> To intuitively understand gating, suppose that you were adding two quantities, $a$ and $b$, but you wanted to control how much of *b* gets into the sum.
>
> Mathematically, you can rewrite the sum *a* + *b* as:
>
>  $a+λb$
>
> where λ is a value between 0 and 1. If λ = 0, there is no contribution from *b* and if λ = 1, *b* contributes fully. Looking at it this way, you can interpret that λ acts as a “switch” or a “gate” in controlling the amount of *b* that gets into the sum. This is the intuition behind the gating mechanism.
>
> $μ(x)a+λ(x)b$
>
> **Gating mechanism can be incorporated into vanilla RNNs to make conditional updates.**
>
> **The gating mechanism not only makes the updates controlled, but also keeps the gradient issues under check and makes training relatively easier.**

#### 2.4 LSTM Networks

[参考](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)

LSTM（Long-Short Term Memory，LSTM），长短期记忆网络通过引入遗忘门、输入门、输出门来解决标准循环神经网络存在的梯度消失问题，能够学习长期依赖关系。LSTM的核心是cell state（**The key to LSTMs is the cell state**）.

The LSTM does have the ability to remove or add information to the cell state, carefully regulated by structures called gates.**Gates are a way to optionally let information through.** 

LSTM具有向cell state移除或添加信息的能力，这通过门控结构来实现。门控结构是一种让信息有选择性地通过的方式。

##### 2.4.1 内部结构

![LSTM](D:\Typora\Notes\NLP\经典模型\LSTM.png)

##### 2.4.2 计算公式

​	计算公式围绕着cell state设计的，从而使得长期记忆中的信息有选择性通过。

- forget gate ：决定**长期记忆cell state** 要遗忘的信息。

  The first step in our LSTM is to decide what information we’re going to throw away from the **cell state**.
  $$
  f_t = \sigma(W_fh_{t-1}+U_fx_t + b_f) =  \sigma(W_f·[h_{t-1},x_t] + b_f)
  $$
  
- input gate：决定要存储到**长期记忆cell state**中的信息，并更新长期记忆。

  The next step is to decide what new information we’re going to store in the **cell state**. 
  $$
  i_t = \sigma(W_ih_{t-1}+U_ix_t + b_f) =  \sigma(W_i·[h_{t-1},x_t] + b_i)
  $$

  $$
  \tilde{C}_t = tanh(W_ch_{t-1}+U_cx_t + b_c) =  tanh(W_c·[h_{t-1},x_t] + b_c)
  $$

  $$
  C_t = C_{t-1} ⊙ f_t + \tilde{C}_t ⊙ i_t  
  $$

  First, a sigmoid layer called the “input gate layer” **decides which values we’ll update**（对应第一个等式）. Next, a tanh layer creates a vector of **new candidate values**, $\tilde{C}_t$, that could be added to the state.（对应第二个等式） Finally, we’ll combine these two to create an update to the cell state（长期记忆）.（对应第三个等式，第一项是删除部分过去信息的，第二项是添加部分当前信息的）

- output gate：决定要存储到短期记忆中的信息，

  Finally, we need to decide what we’re going to output. **This output will be based on our cell state**, but will be a filtered version.
  $$
  o_t = \sigma(W_oh_{t-1}+U_ox_t + b_f) =  \sigma(W_o·[h_{t-1},x_t] + b_o)
  $$

  $$
  h_t = o_t ⊙ tanh(C_t)
  $$

  **hidden state 表示短期记忆，cell state 表示长期记忆**。

  

#### 2.5 GRU

[参考](https://towardsdatascience.com/understanding-gru-networks-2ef37df6c9be)

GRU (Gated Recurrent Unit)，循环门控单元通过引入更新门（update gate）与重置门（reset gate）旨在解决标准循环神经网络存在的梯度消失问题（vanishing gradient problem）。更新门与重置门这两个向量，可以决定传递什么信息到输出。

**相比于LSTM，GRU将LSTM中的遗忘门和输入门合并成了更新门，将cell state和 hidden state也进行了合并，此外又做了一些修改。**

##### 2.5.1 内部结构

![GRU](D:\Typora\Notes\NLP\经典模型\GRU.png)

其中，* is the Hadamard product ⊙, which is an element-wise multiplication between two tensors of the same size.

##### 2.5.2 计算公式 

- **Update gate**
  $$
  z_t = \sigma(W_{zh} h_{t-1}+ W_{zx}x_t) = \sigma(W_z·[h_{t-1},x_t]+b_z)
  $$
  **The update gate helps the model to determine how much of the past information (from previous time steps) needs to be passed along to the future.**更新门帮助模型确定有多少过去的信息(来自以前的时间步)需要传递到未来。

  更新门控制从之前一个隐藏状态传递到当前隐藏状态的信息量，类似于LSTM中的记忆单元，帮助GRU记住长期信息。

  将LSTM中的遗忘门和输入门成为了GRU中的更新门。

- **Reset gate**
  $$
  r_t =\sigma(W_{rh} h_{t-1}+ W_{rx}x_t) =  \sigma(W_r·[h_{t-1},x_t] + b_r)
  $$
  **The reset gate is used from the model to decide how much of the past information to forget**.模型使用重置门来决定要忘记多少过去的信息。

- **Current memory content**
  $$
  \tilde{h}_t = tanh(W_{hx}x_t+ r_t ⊙( W_{hh}h_{t-1})) =  tanh(W·[ r_t ⊙ h_{t-1},x_t] + b_h)
  $$
  第二项哈达玛积决定了了从previous time steps hidden states移除的信息。

  A tanh layer creates a vector of **new candidate values**, $\tilde{h}_t$, that could be added to the hidden state at current time step.

- **Final memory at current time step**
  $$
  h_t = (1-z_t)⊙h_{t-1} + z_t ⊙ \tilde{h}_t
  $$
  第一项确定了从the previous steps 获取的信息，第二项确定了从current memory content 获取的信息。

##### 2.5.3 代码示例

示例1：

```python
import torch
import torch.nn as nn

rnn = nn.GRU(input_size=10, hidden_size=20, num_layers=2,batch_first=True,bidirectional=False)

input = torch.randn(5, 3, 10) # input size: batch_size * seq_len * embed_size
h0 = torch.randn(2, 5, 20) # h0 size：(num_layers*num_directions) * batch_size * hidden_size
output, hn = rnn(input, h0)

print(output.size()) # torch.Size([5, 3, 20])
print(hn.size()) # hn size 和 h0 size 相同， torch.Size([2, 5, 20])

# 以下两个输出的结果相同
print(output[:,-1,:])
print(hn[-1,:,:])
```

其中 output为RNN最后一层输出的整个hidden state序列（水平方向），而hn则是每一层RNN的最后一个hidden state序列（竖直方向），两者都包含最后一层最后一个hidden state vector，可以输出tensor进行验证。

##### 2.5.4 双向GRU

下面通过测试来说明双向RNN返回值的output和hn的对应关系

在双向多层RNN中，分别用$h_T^{1\rightarrow}$和 $h_1^{1\leftarrow}$ 表示第一层RNN的final hidden state 则hn可以表示为：$[h_T^{1\rightarrow},h_T^{1\leftarrow},h_T^{2\rightarrow},h_T^{2\leftarrow},… ]$，如果想从中取出最后一层的 $h_T^\rightarrow$ 和 $h_T^\leftarrow$,则可以使用如下方式：

- 对于 $h_T^\rightarrow$，其可以表示为 $hn[-2,:,:]$
- 对于 $h_T^\leftarrow$，其可以表示为 $hn[-1,:,:]$

双向多层RNN的每个time step的hidden state可以表示为：$h_1=[h_1^\rightarrow;h_T^\leftarrow],h_2=[h_2^\rightarrow;h_{T-2}^\leftarrow],…,h_T=[h_T^\rightarrow;h_{1}^\leftarrow]$,$[a;b]$表示将向量a和b进行拼接，此时output可以表示为$[h_1,h_2,…h_T]$,如果想要从output中取出 $h_T^\rightarrow$ 和 $h_T^\leftarrow$,则可以使用如下方式：

- 对于 $h_T^\rightarrow$，其可以表示为 $output[:,-1,:hidden\_size]$

- 对于 $h_T^\leftarrow$，其可以表示为 $output[:,0,hidden\_size:]$

  ![bigru](D:\Typora\Notes\NLP\经典模型\bigru.png)

> dropout is used between each layer of a multi-layered RNN

验证：

```python
import torch
import torch.nn as nn


batch_size = 2
seq_len = 3
embed_size = 10
hidden_size = 20
num_layers = 2
bidirectional = True

rnn = nn.GRU(
    input_size=embed_size,
    hidden_size=hidden_size,
    num_layers=num_layers,
    batch_first=True,
    bidirectional=bidirectional,
)


rnn_input = torch.randn(batch_size, seq_len, embed_size)


h0 = torch.randn(num_layers * (2 if bidirectional else 1), batch_size, hidden_size)
# h0 size: (num_layers*num_directions, batch_size, hidden_size)

output, hn = rnn(rnn_input, h0)

# output size :(batch_size, seq_len, hidden_size * num_directions)
# hn size:(num_layers*num_directions, batch_size, hidden_size)

print(output.size())  # torch.Size([2, 3, 40])
print(hn.size())  # hn size 和 h0 size 相同 torch.Size([4, 2, 20])


# 验证forward表示
print(output[:, -1, :hidden_size])

# last forward
print(hn[-2, :, :])


# 验证backward表示
print(output[:, 0, hidden_size:])

# last backward
print(hn[-1, :, :])

```





### 3. 卷积神经网络

在前馈神经网络的基础上引入了卷积运算和池化操作的一种神经网络模型，卷积神经网络主要由卷积层、池化层和全连接层构成，其中卷积层和池化层是卷积神经网络进行特征提取的关键模块。卷积神经网络模型在设计上基于以下三个基本思想：

- 局部感受野（local receptive fields）
- 共享权重（share weights）
- 池化（pooling）

#### 3.1 卷积层

- 特征提取：

  卷积层本质上是一个特征提取器，每个卷积层通常包含多个卷积核，卷积核也常称为滤波器，通过每个卷积核提取输入的一种局部特征。每个卷积层所包含的卷积核的数量是一个超参数。每个卷积核包含一些参数，在模型训练过程中根据目标通过不断调整参数值使得每个滤波器学习输入的有效特征表示。

- 局部感受野

  与前馈神经网络神经元之间的全连接不同，卷积层神经元之间是局部连接，下一层的一个神经元只连接到上一层的一个局部区域，并在此区域上与卷积核进行卷积运算得到输出，而该局部区域被称为下一层神经元的**局部感受野**。同时该局部区域可视为一个滑动的窗口，在整个输入特征（如图像）上进行滑动，窗口大小与卷积核的尺寸相同，窗口可以向下向右移动，每次移动特定的距离称为步长，窗口大小和步长都是超参数。

- 卷积核参数共享

  对于一个给定的卷积核，其权重和偏置对于该卷积核的所有局部感受野都是共享的，不同卷积核之间不共享权重和偏置，这种参数共享机制可以降低模型的复杂度，也使得模型更易训练和更快地训练。

#### 3.2 池化层

池化操作存在的意义是对卷积层提取到的特征进行压缩，在卷积层之后应用，池化层对其输入进行下采样，最常用的池化操作是最大池化（max pooling）和平均池化（**average pooling**），具体做法分别是取给定窗口大小内数据的最大值和平均值。与卷积层类似，池化层的滑动窗口大小和步长都是超参数。同时池化操作不包含参数，不增加模型的参数数量。

**卷积的作用可以认为是发现一种特征，而池化的作用是基于局部相关性原理进行亚采样，从而减少数据量的同时保留有用的信息。**全连接层的作用主要是对提取到的特征进行整合，池化层的输出以全连接的形式传递给全连接层，输入到分类器得到分类，再将预测的结果与实际的结果进行比较，通过反向传播的方式进行网络参数的更新。

- 池化方式
  - Max Pooling
  - Average Pooling
  -  L2 Pooling：$pooling = \sqrt[2]{a^2_{11}+a^2_{12}+a^2_{21}+a^2_{22}}$

