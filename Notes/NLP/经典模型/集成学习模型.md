## Ensemble Learning Model

### 1. Bagging

#### 1.1 Random Forest

### 2. Boosting

#### 2.1 AdaBoost

- Fit an additive model (ensemble) $\sum_tρ_th_t(x)$ in a forward stage-wise manner.
- In each stage, introduce a weak learner to compensate the shortcomings of existing weak learners.
- In Adaboost,“shortcomings” are identifified by high-weight data points.

#### 2.2 Gradient Boosting Machine

**Gradient Boosting  = Gradient Descent  + Boosting**

The main idea of boosting is to add new models to the ensemble sequentially. At each particular iteration, a new weak, base-learner model is trained with respect to the error of the whole ensemble learnt so far. 

![GBM](D:\Typora\Notes\NLP\经典模型\GBM.png)

- Fit an additive model $F=\sum_tρ_th_t(x)$ in a forward stage-wise manner.
- In each stage, introduce a weak learner to compensate the shortcomings of existing weak learners.
- The “shortcomings” are identified by negative gradients.
- For any loss function, we can derive a gradient boosting algorithm.
- I Absolute loss and Huber loss are more robust to outliers than square loss.

​	集成学习方法依靠结合大量相对弱的简单模型来获得更好的预测结果。GBM（Gradient Boosting Machine）实际上是一种基于梯度下降算法的Boosting模型，该算法背后的思想是构造新的基学习器，使其与整个集成模型的损失函数的负梯度最大相关。

The principle idea behind GBMs is to construct the new base-learners to be maximally correlated with the negative gradient of the loss function,associated with the whole ensemble.

​	当待预测的变量时连续型数据，也就是进行回归（Regression）任务时，通常选择均方误差（Mean Square Error，MSE）作为损失函数。MSE损失函数背后的思想是惩罚那些与目标输出相差较大的数据点，同时忽略残差较小的数据点。**缺点**：对异常值或极端值比较敏感。	
$$
L(y,\hat y) = \frac{1}{n}\sum_{i=1}^n(y_i-\hat y_i)^2
$$
想要通过改变 $\hat y_1,\hat y_2,…，\hat y_n$来最小化损失函数，注意到  $\hat y_1,\hat y_2,…，\hat y_n$ 是一些数字，可以将其视为参数，同时将 $y_i$视为常数，对其求导有：
$$
\frac{∂L(y,\hat y)}{∂\hat y_i} = \frac{∂\sum_{i=1}^n(y_i-\hat y_i)^2}{2∂\hat y_i} = \hat y_i  - y_i
$$
从而可以将残差表示为负梯度：
$$
y_i - \hat y_i =- \frac {∂L(y,\hat y_)}{∂\hat y_i }
$$
**需要首先计算负梯度，然后添加弱学习器对负梯度进行拟合。**在使用 MSE 作为损失函数的情况下，拟合负梯度等价于拟合残差（residual）。当负梯度和残差不等价时，选择拟合负梯度而非残差的原因在于残差过于关注异常值（outliers）

**GBM模型正则化（Regularization）**

**1.子采样（Subsampling）**

The idea behind this method is to introduce some randomness into the fitting procedure. At each learning iteration only a random part of the training data is used to fit a consecutive base-learner. 

随机森林采用放回抽样，这里使用不放回抽样。子采样比例是指每次迭代使用的训练数据占总训练数据的比例，若子采样比例小于1，则让基学习器只拟合这部分样本。选择小于1的比例可以减少方差，即防止过拟合，但会增加样本拟合的偏差，因此子采样比例不宜过低，通常取 $[0.5,0.8]$ 之间。

**2. 收缩（ SHRINKAGE）**

> In the context of GBMs, shrinkage is used for reducing, or shrinking, the impact of each additional fitted base-learner. It reduces the size of incremental steps and thus penalizes the importance of each consecutive iteration. The intuition behind this technique is that **it is better to improve a model by taking many small steps than by taking fewer large steps. If one of the boosting iterations turns out to be erroneous, its negative impact can be easily corrected in subsequent steps.**
>
> **在GBMs的背景下，收缩用于减少或缩小每个新添加的基学习器的影响。**它减少了增量步骤的大小，从而惩罚了每个连续迭代的重要性。这种技术背后的直觉是，通过采取许多小步骤来改进模型比采取更少的大步骤要好。如果其中一个提升迭代被证明是错误的，那么它的负面影响可以在随后的步骤中很容易地被纠正。

由此，可以引入一个收缩超参数：
$$
F_m(x) = F_{m-1}(x) - \lambda ρ_mh(x, θ_m), \lambda ∈ (0, 1]
$$
It is a common pattern that the smaller parameter *λ* and therefore, the lower the shrinked boosted increments are, the better

generalization is achieved.

收缩参数降低了每个基学习器对最终预测结果的影响，使得每个基学习器的重要性都有所下降，这种技术背后的思想是通过采取许多的小步骤而非更少的大步骤可以更好地改进模型。如果某次迭代被证明是错误的，其负面影响在后续迭代中可以得到纠正。

收缩方法的使用增强了模型的泛化性能，但同时也增加了迭代的次数以及基学习器的数量。

**3.交叉验证**

Cross-validation provides means for testing the model on withheld portions of data,while still using all of the data at the processing stages.交叉验证可以在保留的数据上测试模型得性能，同时仍然可以在训练阶段使用所有训练数据。

##### 2.2.1 GBDT

Gradient Boosting Decision Tree，称为梯度提升决策树，GBDT限定了只能使用分类树（也称为分类回归树，Classification and Regression Tree，CART，其决策规则和决策树一样，且每个叶节点包含一个得分）作为基学习器，GBDT模型可以用于回归、分类、排名（ranking）任务。

**Xgboost and lightGBM are both subtypes/specific instances of the GBDT algorithm.**

GBDT回归算法：

对于输入数据 ${(x_i,y_i)}_{i=1}^n$以及一个可导损失函数 $L(y_i,F(x_i))$,

1. 初始化模型
   $$
   F_0(x) =  \arg \mathop{\min}\limits_{c} \sum_{i=1}^nL(y_i,c)
   $$

2. 对于 $m = 1,2,…,M$:

   - 对于 $i=1,2,…,N$，计算
     $$
     r_{mi} = -\bigg[ \frac{∂L(y_i,F(x_i))}{∂F(x_i)}\bigg]_{F(x)=F_{m-1}(x)}
     $$
     添加一棵回归树来拟合 $\large \{(x_i,r_{mi})\}_{i=1}^n$，从而得到第 m 棵回归树，该回归树对应的叶节点区域为 $R_{mj},j = 1 , 2 , . . J$。

   - 对于 $j = 1 , 2 , . . J$ ，计算每个叶节点区域的最佳拟合值：
     $$
     c_{mj} =  \arg \mathop{\min}\limits_{c} \sum_{x_i∈R_{mj}}L(y_i,F_{m-1}(x_i)+c)
     $$

   - 更新模型：
     $$
     F_m(x) = F_{m-1}(x) + \sum_{j=1}^Jc_{mj}I(x_i∈R_{mj})
     $$
     

3. 得到回归树：
   $$
   F_M(x) = \sum_{m=1}^M\sum_{j=1}^Jc_{mj}I(x_i∈R_{mj})
   $$

GBDT二分类算法：

选择使用交叉熵损失函数： 
$$
L_i = -y_i\ log\ \hat p_i- (1-y_i)\ log\ (1 - \hat p_i) = -y_i\ log\ \frac{\hat p_i}{1 - \hat p_i} -  log\ (1 - \hat p_i) \\
= -y_i\hat y_i + log(1+e^{\hat y_i})
$$
其导数为：
$$
\frac{∂L_i}{∂\hat y_i} = -y_i + \frac{e^{\hat y_i}}{1+e^{\hat y_i}}=\hat p_i -y_i
$$
叶节点值为：
$$
c = \frac{\sum_i(y_i-\hat p_i)}{\sum_i \hat p_i (1-\hat p_i)}
$$


##### 2.2.2 XGBoost 

##### 2.2.4 CatBoost

