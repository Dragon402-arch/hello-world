- æ–‡æœ¬å•å…ƒï¼štoken
  - byte-level å¯¹åº”å­—ç¬¦ç¼–ç 
  - character-level å­—ç¬¦çº§
  - subword-level ï¼šBPEç¼–ç ä»¥åŠFastText
  - word-level
  - phrase
  - n-gramï¼šN-gram is n continuous sequence of tokens in a document.
  - sentence
  - document
  - text
  
- å‘é‡è¡¨ç¤º
  - ç¦»æ•£è¡¨ç¤ºï¼šå­—ç¬¦åŒ¹é…
    - Bag of Wordsï¼ˆBoWï¼‰ï¼šæ²¡æœ‰è€ƒè™‘tokençš„é¡ºåº
    
      ä½¿ç”¨è¯¥è¡¨ç¤ºå¯å°†å¤šç¯‡æ–‡æ¡£ï¼ˆdocumentï¼‰è¡¨ç¤ºä¸ºä¸€ä¸ªçŸ©é˜µï¼ŒçŸ©é˜µçš„æ¯ä¸€è¡Œè¡¨ç¤ºä¸€ç¯‡æ–‡æ¡£ï¼Œæ¯ä¸€åˆ—è¡¨ç¤ºä¸€ä¸ªtokenï¼Œå¦‚æœä¸€ä¸ªtokenåœ¨ç»™å®šçš„documentä¸­å‡ºç°åˆ™å…¶å€¼ä¸º1ï¼Œæ²¡æœ‰å‡ºç°åˆ™ä¸º0ã€‚æ­¤å¤–å¯ä»¥ä½¿ç”¨è¯é¢‘æ•°å¯¹tokenè¿›è¡Œé™å®šï¼Œè¿‡æ»¤æ‰ä½é¢‘tokenï¼Œå› æ­¤ä¸ä¸€å®šä¼šä½¿ç”¨è¯­æ–™åº“ä¸­æ‰€æœ‰å‡ºç°çš„tokenã€‚
    
    - Count Vectorization
      
      ä½¿ç”¨è¯¥è¡¨ç¤ºå¯å°†å¤šç¯‡æ–‡æ¡£ï¼ˆdocumentï¼‰è¡¨ç¤ºä¸ºä¸€ä¸ªçŸ©é˜µï¼ŒçŸ©é˜µçš„æ¯ä¸€è¡Œè¡¨ç¤ºä¸€ç¯‡æ–‡æ¡£ï¼Œæ¯ä¸€åˆ—è¡¨ç¤ºä¸€ä¸ªtokenï¼Œå¦‚æœä¸€ä¸ªtokenåœ¨ç»™å®šçš„documentä¸­å‡ºç°ï¼Œåˆ™çŸ©é˜µçš„å…ƒç´ å€¼ä¸ºè¯¥tokenåœ¨æŒ‡å®šdocumentä¸­å‡ºç°çš„é¢‘æ•°ã€‚è¯¥è¡¨ç¤ºæ–¹å¼åŒæ ·å¯ä»¥ä½¿ç”¨è¯é¢‘æ•°å¯¹tokenè¿›è¡Œè¿‡æ»¤ã€‚
      
    - TF-IDF
    
      - TFï¼šstands for Term Frequency, which measures how many times a term occurs in a document.è¯é¢‘è¡¨ç¤ºä¸€ä¸ªtokenåœ¨ä¸€ç¯‡ç»™å®šçš„æ–‡æ¡£documentä¸­å‡ºç°çš„é¢‘ç‡ï¼Œæ¯”å¦‚å•è¯useåœ¨ä¸€ç¯‡æ–‡æ¡£ä¸­å‡ºç°äº†3æ¬¡ï¼Œè€Œè¯¥ç¯‡æ–‡æ¡£æ€»å…±æœ‰10ä¸ªå•è¯ï¼ˆæ–‡æ¡£çš„é•¿åº¦ç­‰äºå•è¯çš„ä¸ªæ•°ï¼‰ï¼Œåˆ™è¯¥tokenå¯¹åº”çš„TFå€¼ä¸º 3/10 = 0.3
      - IDFï¼šstands for Inverse Document Frequency, which measures how frequently a term appears in the whole corpus.é€†æ–‡æ¡£é¢‘ç‡çš„è®¡ç®—å…¬å¼ä¸ºï¼šè¯­æ–™åº“çš„æ€»æ–‡æ¡£æ•°é™¤ä»¥ ä¸€ä¸ªtokenåœ¨æ‰€æœ‰æ–‡æ¡£å‡ºç°çš„æ–‡æ¡£æ•°ï¼Œç„¶åå†å–å¯¹æ•°ï¼Œæ¯”å¦‚æ€»å…±æœ‰100ç¯‡æ–‡æ¡£ï¼Œè€Œuseè¿™ä¸ªå•è¯åœ¨è¿™100ç¯‡æ–‡æ¡£çš„32ç¯‡æ–‡æ¡£ä¸­å‡ºç°è¿‡ï¼Œé‚£ä¹ˆIDFçš„è®¡ç®—åº”è¯¥ç†è®ºä¸Šåº”è¯¥æ˜¯ log(100/32)ï¼Œè€Œåœ¨çœŸå®è®¡ç®—ä¸­ä¼šæ˜¯ log(100/32 + 1)ï¼Œå…¶ä¸­åŠ 1æ˜¯ä¸ºäº†é¿å…ä¸€ä¸ªtokenåœ¨æ‰€æœ‰æ–‡æ¡£ä¸­éƒ½å‡ºç°è¿‡çš„æƒ…å†µï¼Œæ­¤æ—¶ä¼šå¯¼è‡´IDF=0ï¼ŒTF-IDF=0ï¼ŒåŠ ä¸Š1ä¹‹åå°±ä¸å†ä¼šå‡ºç°è¿™ä¸ªæƒ…å†µã€‚
        - ğ¼ğ·ğ¹(ğ‘¡)=ğ‘™ğ‘œğ‘”((1+ğ‘)/(1+ğ·ğ¹(ğ‘¡,ğ‘‘))+1) where ğ‘ is the total number of documents in the corpus and ğ·ğ¹(ğ‘¡,ğ‘‘) is the document frequency of the term.
        -  Adding 1 to the numerator and denominator of the equation prevents zero division. åˆ†å­åˆ†æ¯åŠ 1æ˜¯ä¸ºäº†é¿å…é™¤0é—®é¢˜ã€‚
    
    - BM25ï¼š[è®²è§£](https://towardsdatascience.com/understanding-term-based-retrieval-methods-in-information-retrieval-2be5eb3dde9f)
    
      In information retrieval, **Okapi BM25** (*BM* is an abbreviation of *best matching*) is a ranking function used by search engines to **estimate the relevance of documents to a given search query**.BM25ï¼ˆBest matchingï¼‰æ‹“å±•äº†TF-IDFçš„æ€æƒ³ï¼Œå¹¶æ”¹å–„äº†TF-IDFåœ¨ term saturation å’Œ document length æ–¹é¢çš„ç¼ºé™·ã€‚
    
      - Term Saturationï¼štermå‡ºç°çš„è¯é¢‘æ•°è¶³å¤Ÿå¤š
    
        å¦‚æœä¸€ä»½æ–‡æ¡£ä¸­å‡ºç°äº†100æ¬¡â€œè®¡ç®—æœºâ€ä¸€è¯ï¼Œé‚£ä¹ˆå®ƒçš„ç›¸å…³æ€§çœŸçš„æ˜¯å‡ºç°äº†50æ¬¡çš„æ–‡æ¡£çš„ä¸¤å€å—ï¼Ÿæˆ‘ä»¬å¯ä»¥è¯´ï¼Œå¦‚æœâ€œè®¡ç®—æœºâ€ä¸€è¯å‡ºç°äº†è¶³å¤Ÿå¤šçš„æ¬¡æ•°ï¼Œé‚£ä¹ˆè¯¥æ–‡æ¡£å‡ ä¹è‚¯å®šæ˜¯ç›¸å…³çš„ï¼Œå†å¤šå‡ºç°ä¸€æ¬¡ä¹Ÿä¸ä¼šå¢åŠ ç›¸å…³æ€§çš„å¯èƒ½æ€§ã€‚æ‰€ä»¥ï¼Œ**å½“ä¸€ä¸ª term å¯èƒ½é¥±å’Œï¼ˆå‡ºç°è¶³å¤Ÿå¤šçš„æ¬¡æ•°ï¼‰æ—¶ï¼Œæˆ‘ä»¬æƒ³è¦æ§åˆ¶TFçš„è´¡çŒ®**ã€‚BM25é€šè¿‡å¼•å…¥å‚æ•° k1 æ¥æ§åˆ¶é¥±å’Œæ›²çº¿çš„å½¢çŠ¶æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚è¿™è®©æˆ‘ä»¬å¯ä»¥å°è¯•ä¸åŒçš„k1å€¼ï¼Œçœ‹çœ‹å“ªä¸ªå€¼æ•ˆæœæœ€å¥½ã€‚é€šè¿‡å‚æ•°å€¼k1çš„è°ƒæ•´ï¼Œå½“TFçš„å€¼å¢åŠ æ—¶ï¼Œå¯¹æœ€ç»ˆå¾—åˆ†å€¼çš„å½±å“åœ¨æ…¢æ…¢å˜å°ã€‚åˆæ­¥ä¿®æ­£ï¼š
        $$
        w_{t} = -\frac{(k_{1}+1)Â·tf(d;t)}{k_{1} + tf(d;t)}Â·idf(t)
        $$
    
      - Document Length Normalization
    
        å¦‚æœä¸€ç¯‡æ–‡æ¡£ç¢°å·§å¾ˆçŸ­ï¼Œè€Œâ€è®¡ç®—æœºâ€œè¿™ä¸ªè¯åªå‡ºç°äº†ä¸€æ¬¡ï¼Œè¿™å¯èƒ½å·²ç»æ˜¯ç›¸å…³æ€§çš„ä¸€ä¸ªå¾ˆå¥½çš„æŒ‡æ ‡ã€‚ä½†æ˜¯ï¼Œå¦‚æœæ–‡æ¡£éå¸¸é•¿ï¼Œè€Œâ€œè®¡ç®—æœºâ€ä¸€è¯åªå‡ºç°äº†ä¸€æ¬¡ï¼Œé‚£ä¹ˆè¯¥æ–‡æ¡£å¾ˆå¯èƒ½ä¸è®¡ç®—æœºæ— å…³ã€‚**æˆ‘ä»¬å¸Œæœ›å¯¹çŸ­æ–‡æ¡£çš„ term match ç»™äºˆå¥–åŠ±ï¼Œå¯¹é•¿æ–‡æ¡£ç»™äºˆæƒ©ç½šã€‚**ç„¶è€Œï¼Œä½ ä¸å¸Œæœ›è¿‡åº¦æƒ©ç½šï¼Œå› ä¸ºæœ‰æ—¶ä¸€ç¯‡æ–‡æ¡£å¾ˆé•¿ï¼Œå› ä¸ºå®ƒåŒ…å«è®¸å¤šç›¸å…³ä¿¡æ¯ï¼Œè€Œä¸ä»…ä»…æ˜¯æœ‰å¾ˆå¤šå•è¯ã€‚é‚£ä¹ˆï¼Œæˆ‘ä»¬æ€æ ·æ‰èƒ½åšåˆ°å‘¢ï¼Ÿæˆ‘ä»¬å°†å¼•å…¥å¦ä¸€ä¸ªå‚æ•°bï¼Œè¯¥å‚æ•°ç”¨äºæ„é€  normalizerï¼šåœ¨BM25å…¬å¼ä¸­çš„ (1-b) +b*dl/dlavg ã€‚å‚æ•°bçš„å€¼å¿…é¡»åœ¨0åˆ°1ä¹‹é—´æ‰èƒ½ä½¿å…¶æ­£å¸¸å·¥ä½œã€‚
        $$
        w_{t} = -\frac{(k_{1}+1)Â·tf(d;t)}{k_{1}(1-b+bÂ·\frac{l_{d}}{l_{avg}}) + tf(d;t)}Â·idf(t)
        $$
        é¦–å…ˆï¼Œè®©æˆ‘ä»¬ç†è§£æ–‡æ¡£çš„é•¿çŸ­æ„å‘³ç€ä»€ä¹ˆã€‚åŒæ ·ï¼Œæ–‡æ¡£æ˜¯é•¿æ˜¯çŸ­å–å†³äºè¯­æ–™åº“çš„ä¸Šä¸‹æ–‡ã€‚ä¸€ç§æ–¹æ³•æ˜¯ä½¿ç”¨è¯­æ–™åº“çš„å¹³å‡é•¿åº¦ä½œä¸ºå‚è€ƒç‚¹ã€‚é•¿æ–‡æ¡£å°±æ˜¯æ¯”è¯­æ–™åº“çš„å¹³å‡é•¿åº¦é•¿ï¼ŒçŸ­æ–‡æ¡£å°±æ˜¯æ¯”è¯­æ–™åº“çš„å¹³å‡é•¿åº¦çŸ­ã€‚å¯ä»¥çœ‹åˆ°ï¼Œå½“bä¸º1æ—¶ï¼Œnormalizeréƒ¨åˆ†å˜ä¸º(1â€“1 + 1Â· dl/dlavg)ï¼Œå¦ä¸€æ–¹é¢ï¼Œå¦‚æœbç­‰äº0ï¼Œæ•´ä¸ªéƒ¨åˆ†ç­‰äº1ï¼Œæ–‡æœ¬é•¿åº¦çš„å½±å“æ²¡æœ‰è€ƒè™‘åˆ°ã€‚
    
        å½“æ–‡æ¡£çš„é•¿åº¦(dl)å¤§äºå¹³å‡æ–‡æ¡£é•¿åº¦æ—¶ï¼Œè¯¥æ–‡æ¡£å°†å—åˆ°æƒ©ç½šå¹¶è·å¾—è¾ƒä½çš„åˆ†æ•°ã€‚å‚æ•°bæ§åˆ¶ç€è¿™äº›æ–‡æ¡£å—åˆ°çš„æƒ©ç½šæœ‰å¤šå¼º: bå€¼è¶Šé«˜ï¼Œé•¿æ–‡æ¡£å—åˆ°çš„æƒ©ç½šå°±è¶Šé«˜ï¼Œå› ä¸ºä¸å¹³å‡è¯­æ–™åº“é•¿åº¦ç›¸æ¯”ï¼Œæ–‡æ¡£é•¿åº¦çš„å½±å“ä¼šè¢«æ”¾å¤§ã€‚å¦ä¸€æ–¹é¢ï¼Œå°äºå¹³å‡æ°´å¹³çš„æ–‡æ¡£ä¼šå¾—åˆ°å¥–åŠ±ï¼Œå¹¶ç»™å‡ºæ›´é«˜çš„åˆ†æ•°ã€‚
    
        In summary, TF-IDF rewards term frequency and penalizes document frequency. BM25 goes beyond this to account for document length and term frequency saturation.
    
        é€šè¿‡ä¸Šé¢çš„éƒ¨åˆ†ï¼Œå¯ä»¥ç†è§£ä¸ºBM25æ˜¯TF-IDFç®—æ³•çš„æ‹“å±•ï¼Œå› æ­¤ä¹Ÿæ˜¯å¯ä»¥ç”Ÿæˆå‘é‡è¡¨ç¤ºçš„ã€‚BM25 ranks a set of documents based on the query terms appearing in each documentï¼Œ**é€šå¸¸ç”¨æ¥è®¡ç®—ä¸€ä¸ªqueryå’Œä¸€ç»„documentä¹‹é—´çš„ç›¸å…³æ€§å¾—åˆ†**ã€‚
        
        Given a query *Q*, containing keywords $q_1, ..., q_n$, the BM25 score of a document *D* is:
        $$
        Score(D,Q)= \sum_{i=1}^n w_{q_i}= \sum_{i=1}^n\frac{(k_{1}+1)Â·TF(D;q_i)}{k_{1}Â·(1-b+bÂ·\frac{|D|}{avgdl}) + TF(D;q_i)}Â·IDF(q_i)
        $$
        å…¶ä¸­ï¼Œ$TF(D;q_i)$ è¡¨ç¤º term  $q_i$ åœ¨æ–‡æ¡£ D ä¸­å‡ºç°çš„é¢‘ç‡ï¼Œ $|D|$  è¡¨ç¤ºæ–‡æ¡£Dçš„æ–‡æœ¬é•¿åº¦ï¼Œä¹Ÿå°±æ˜¯åŒ…å«çš„termçš„ä¸ªæ•°ï¼Œavgdlè¡¨ç¤ºä¸€ç»„æ–‡æ¡£çš„å¹³å‡æ–‡æ¡£é•¿åº¦ã€‚å¸¸ç”¨å€¼ $b=0.75,k_1=[1.2,2.0]$ã€‚
        
        ç®€å•æ¥è®²å°±æ˜¯ï¼Œå…ˆå°†queryè¿›è¡Œåˆ†è¯ï¼Œç„¶åå–æ¯ä¸ªtermæˆ–æ˜¯tokenä»¥åŠä¸€ç¯‡ç»™å®šçš„æ–‡æ¡£å¸¦å…¥å…¬å¼è®¡ç®—å¾—åˆ°ä¸€ä¸ªå¾—åˆ†ï¼Œç„¶åå°†queryä¸­æ‰€æœ‰tokençš„å¾—åˆ†ç›¸åŠ å°±å¾—åˆ°äº†æœ€ç»ˆç›¸ä¼¼åº¦å¾—åˆ†ã€‚
        
        è¯¥æ–¹æ³•å¹¶ä¸æ˜¯é‡‡ç”¨å°†queryä¹Ÿç®—åœ¨è¿™ç»„æ–‡æ¡£ä¸­ï¼Œä½¿ç”¨BM25ç®—æ³•åˆ†åˆ«å¾—åˆ°å¯¹åº”çš„å‘é‡è¡¨ç¤ºå†è®¡ç®—ç›¸ä¼¼åº¦å¾—åˆ†ï¼Œè€Œæ˜¯æ–‡æ¡£ä¸åšå˜åŠ¨ï¼Œåªå¯¹queryè¿›è¡Œæ‹†åˆ†ï¼Œtokenå¾—åˆ†ç›¸åŠ æ±‚å’Œå¾—åˆ°æœ€ç»ˆç»“æœçš„ã€‚
        
        
    
  - è¿ç»­è¡¨ç¤ºï¼šè¯­ä¹‰åŒ¹é…
    - Word2Vecï¼š[ä»£ç å®ç°](https://towardsdatascience.com/word2vec-with-pytorch-implementing-original-paper-2cd7040120b0)
    
      The continuous bag-of-words (CBOW) architecture predicts the current word based on the context words, and the skip-gram predicts surrounding words given the current word.
    
      - CBOWï¼šåŸºäºä¸Šä¸‹æ–‡è¯é¢„æµ‹å½“å‰è¯ã€‚window sizeï¼šå¤§çº¦æ˜¯5ï¼Œè®­ç»ƒæ›´å¿«
      - Skip-Gramï¼šåŸºäºå½“å‰è¯é¢„æµ‹ä¸Šä¸‹æ–‡è¯ã€‚window sizeï¼šå¤§çº¦æ˜¯10ï¼Œè®­ç»ƒè¾ƒæ…¢
    
      ![image-20230320135711664](C:\Users\åƒæ±Ÿæ˜ æœˆ\AppData\Roaming\Typora\typora-user-images\image-20230320135711664.png)
    
      ![image-20230320145846352](C:\Users\åƒæ±Ÿæ˜ æœˆ\AppData\Roaming\Typora\typora-user-images\image-20230320145846352.png)
    
      â€‹	[å›¾ç‰‡æ¥æº](https://towardsdatascience.com/understanding-feature-engineering-part-4-deep-learning-methods-for-text-data-96c44370bbfa)
    
      - samples
    
        - (word, word in the same window), with label 1. 
    
        - (word, random word from the vocabulary), with label 0.
    
    - Glove
    
      åƒLSAè¿™æ ·çš„æ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°åˆ©ç”¨ç»Ÿè®¡ä¿¡æ¯ï¼Œä½†å®ƒä»¬åœ¨å•è¯ç±»æ¯”ä»»åŠ¡ä¸Šè¡¨ç°ç›¸å¯¹è¾ƒå·®ï¼Œæ¯”å¦‚æˆ‘ä»¬å¦‚ä½•æ‰¾åˆ°è¯­ä¹‰ç›¸ä¼¼çš„å•è¯ï¼ˆcontext-countingçš„ç¼ºé™·ï¼‰ã€‚åƒskip-gramè¿™æ ·çš„æ–¹æ³•å¯èƒ½åœ¨ç±»æ¯”ä»»åŠ¡ä¸Šåšå¾—æ›´å¥½ï¼Œä½†å®ƒä»¬æ²¡æœ‰åœ¨å…¨å±€å±‚é¢ä¸Šå¾ˆå¥½åœ°åˆ©ç”¨è¯­æ–™åº“çš„ç»Ÿè®¡ä¿¡æ¯ï¼ˆcontext-predictingçš„ç¼ºé™·ï¼‰ã€‚
    
      ![image-20230320085407927](C:\Users\åƒæ±Ÿæ˜ æœˆ\AppData\Roaming\Typora\typora-user-images\image-20230320085407927.png)
    
      ![image-20230320085435656](C:\Users\åƒæ±Ÿæ˜ æœˆ\AppData\Roaming\Typora\typora-user-images\image-20230320085435656.png)
    
    - fastText
    
       In general, predictive models like the Word2Vec model typically considers each word as a distinct entity (e.g. **where**) and generates a dense embedding for the word. However this poses to be a serious limitation with languages having massive vocabularies and many rare words which may not occur a lot in different corpora. The Word2Vec model typically ignores the morphological structure of each word and considers a word as a single entity. The FastText model *considers each word as a Bag of Character n-grams.* This is also called as a subword model in the paper.
    
      ![image-20230320094724797](C:\Users\åƒæ±Ÿæ˜ æœˆ\AppData\Roaming\Typora\typora-user-images\image-20230320094724797.png)
    
      - è®­ç»ƒè¿‡ç¨‹ç»†èŠ‚
    
        ä¸€èˆ¬æƒ…å†µä¸‹ï¼Œä½¿ç”¨fastTextè¿›è¡Œæ–‡æœ¬åˆ†ç±»çš„åŒæ—¶ä¹Ÿä¼šäº§ç”Ÿè¯çš„embeddingï¼Œå³embeddingæ˜¯fastTextåˆ†ç±»çš„äº§ç‰©ã€‚
    
        å…ˆè¿›è¡Œæ–‡æœ¬åˆ†è¯ï¼Œç„¶åé’ˆå¯¹ä¸€ä¸ªtokenï¼ˆæ¯”å¦‚whereï¼‰ï¼Œå†è·å–å…¶å¯¹åº”çš„n-gramç‰¹å¾ï¼Œæ¯”å¦‚ï¼ˆå­—è¯ï¼šâ€œ<whâ€ã€â€œwheâ€ã€â€œherâ€ã€â€œereâ€ã€â€œre>â€œä»¥åŠç‰¹æ®Šå­—è¯â€œ\<where\>"ï¼‰ã€‚åœ¨fastTextä¸­ï¼Œå¯¹äºä»»æ„ä¸€ä¸ªè¯wï¼Œè·å–å…¶â»“åº¦åœ¨3å’Œ6ä¹‹é—´çš„æ‰€æœ‰â¼¦è¯ä¸å…¶ç‰¹æ®Šâ¼¦è¯çš„å¹¶é›†ï¼Œä»è€Œå°†å…¶æ‰€æœ‰å­—è¯å‘é‡è¡¨ç¤ºæ±‚å’Œæˆ–æ±‚å¹³å‡å€¼ä½œä¸º w çš„å‘é‡è¡¨ç¤ºï¼Œå¥å­çš„å‘é‡è¡¨ç¤ºå°†æ‰€æœ‰è¯å‘é‡è¡¨ç¤ºå–å¹³å‡å€¼ï¼ˆThe word representations are then averaged into a text representation, which is in turn fed to a linear classifier.ï¼‰ï¼Œç„¶åè¿›è¡Œæ–‡æœ¬åˆ†ç±»ä»»åŠ¡ï¼ˆæºç ä¸­æ˜¯å¤šæ ‡ç­¾åˆ†ç±»ä»»åŠ¡ï¼‰ã€‚é€šè¿‡è®­ç»ƒåˆ†ç±»ä»»åŠ¡ï¼Œå¾—åˆ°å¯¹åº”çš„subwordå­—è¯çš„å¯¹åº”åµŒå…¥è¡¨ç¤ºï¼Œå¹¶è¿›ä¸€æ­¥è·å¾—å¯¹åº”è¯çš„å‘é‡è¡¨ç¤ºã€‚
    
      - æ–¹æ³•ä¼˜ç‚¹ï¼š
    
        - å¯ä»¥ç†è§£å‰ç¼€å’Œåç¼€çš„å«ä¹‰ã€‚
        - ä¸å¸¸è§è¯å’Œæœªè§è¿‡è¯éƒ½å¯ä»¥åœ¨æ‹†åˆ†ä¸ºå­—ç¬¦çº§n-gramsä¹‹åè·å¾—å¯¹åº”çš„åµŒå…¥è¡¨ç¤ºã€‚
    
    - ELMO
    
    - BERT

#### Notes

- BoWè¯è¢‹æ¨¡å‹ç¼ºé™·

  - å‘é‡è¡¨ç¤ºç¨€ç–
  - ä¸èƒ½æ•è·è¯ä¹‹é—´çš„è¯­ä¹‰å…³ç³»

- Vector Space Models 

  - åœ¨åŒä¸€ä¸ªä¸Šä¸‹æ–‡ä¸­å‡ºç°å’Œä½¿ç”¨çš„å•è¯åœ¨è¯­ä¹‰ä¸Šå½¼æ­¤ç›¸ä¼¼ï¼Œå…·æœ‰ç›¸ä¼¼çš„å«ä¹‰ã€‚

  - context-countingï¼šLatent Semantic Analysis (LSA)

    åŸºäºè®¡æ•°çš„æ–¹æ³•å¯ç”¨äºè®¡ç®—æŸäº›å•è¯åœ¨è¯­æ–™åº“ä¸­çš„ä¸é‚»è¿‘å•è¯å…±ç°çš„é¢‘ç‡ï¼Œç„¶åä»è¿™äº›ç»Ÿè®¡æŒ‡æ ‡ä¸­ä¸ºæ¯ä¸ªå•è¯æ„å»ºç¨ å¯†çš„å•è¯å‘é‡è¡¨ç¤ºã€‚

  - context-predictingï¼š

    åŸºäºç¥ç»ç½‘ç»œçš„è¯­è¨€æ¨¡å‹ä¹‹ç±»çš„é¢„æµ‹æ–¹æ³•è¯•å›¾ä»è¯­æ–™åº“ä¸­çš„å•è¯åºåˆ—ä¸­çš„é‚»è¿‘å•è¯æ¥é¢„æµ‹å•è¯ï¼Œå¹¶åœ¨æ­¤è¿‡ç¨‹ä¸­å­¦ä¹ åˆ†å¸ƒå¼è¡¨ç¤ºå½¢å¼ï¼Œä»è€Œä¸ºæˆ‘ä»¬æä¾›ç¨ å¯†çš„å•è¯åµŒå…¥è¡¨ç¤ºã€‚

