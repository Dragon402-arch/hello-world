- 文本单元：token
  - byte-level 对应字符编码
  - character-level 字符级
  - subword-level ：BPE编码以及FastText
  - word-level
  - phrase
  - n-gram：N-gram is n continuous sequence of tokens in a document.
  - sentence
  - document
  - text
  
- 向量表示
  - 离散表示：字符匹配
    - Bag of Words（BoW）：没有考虑token的顺序
    
      使用该表示可将多篇文档（document）表示为一个矩阵，矩阵的每一行表示一篇文档，每一列表示一个token，如果一个token在给定的document中出现则其值为1，没有出现则为0。此外可以使用词频数对token进行限定，过滤掉低频token，因此不一定会使用语料库中所有出现的token。
    
    - Count Vectorization
      
      使用该表示可将多篇文档（document）表示为一个矩阵，矩阵的每一行表示一篇文档，每一列表示一个token，如果一个token在给定的document中出现，则矩阵的元素值为该token在指定document中出现的频数。该表示方式同样可以使用词频数对token进行过滤。
      
    - TF-IDF
    
      - TF：stands for Term Frequency, which measures how many times a term occurs in a document.词频表示一个token在一篇给定的文档document中出现的频率，比如单词use在一篇文档中出现了3次，而该篇文档总共有10个单词（文档的长度等于单词的个数），则该token对应的TF值为 3/10 = 0.3
      - IDF：stands for Inverse Document Frequency, which measures how frequently a term appears in the whole corpus.逆文档频率的计算公式为：语料库的总文档数除以 一个token在所有文档出现的文档数，然后再取对数，比如总共有100篇文档，而use这个单词在这100篇文档的32篇文档中出现过，那么IDF的计算应该理论上应该是 log(100/32)，而在真实计算中会是 log(100/32 + 1)，其中加1是为了避免一个token在所有文档中都出现过的情况，此时会导致IDF=0，TF-IDF=0，加上1之后就不再会出现这个情况。
        - 𝐼𝐷𝐹(𝑡)=𝑙𝑜𝑔((1+𝑁)/(1+𝐷𝐹(𝑡,𝑑))+1) where 𝑁 is the total number of documents in the corpus and 𝐷𝐹(𝑡,𝑑) is the document frequency of the term.
        -  Adding 1 to the numerator and denominator of the equation prevents zero division. 分子分母加1是为了避免除0问题。
    
    - BM25：[讲解](https://towardsdatascience.com/understanding-term-based-retrieval-methods-in-information-retrieval-2be5eb3dde9f)
    
      In information retrieval, **Okapi BM25** (*BM* is an abbreviation of *best matching*) is a ranking function used by search engines to **estimate the relevance of documents to a given search query**.BM25（Best matching）拓展了TF-IDF的思想，并改善了TF-IDF在 term saturation 和 document length 方面的缺陷。
    
      - Term Saturation：term出现的词频数足够多
    
        如果一份文档中出现了100次“计算机”一词，那么它的相关性真的是出现了50次的文档的两倍吗？我们可以说，如果“计算机”一词出现了足够多的次数，那么该文档几乎肯定是相关的，再多出现一次也不会增加相关性的可能性。所以，**当一个 term 可能饱和（出现足够多的次数）时，我们想要控制TF的贡献**。BM25通过引入参数 k1 来控制饱和曲线的形状来解决这个问题。这让我们可以尝试不同的k1值，看看哪个值效果最好。通过参数值k1的调整，当TF的值增加时，对最终得分值的影响在慢慢变小。初步修正：
        $$
        w_{t} = -\frac{(k_{1}+1)·tf(d;t)}{k_{1} + tf(d;t)}·idf(t)
        $$
    
      - Document Length Normalization
    
        如果一篇文档碰巧很短，而”计算机“这个词只出现了一次，这可能已经是相关性的一个很好的指标。但是，如果文档非常长，而“计算机”一词只出现了一次，那么该文档很可能与计算机无关。**我们希望对短文档的 term match 给予奖励，对长文档给予惩罚。**然而，你不希望过度惩罚，因为有时一篇文档很长，因为它包含许多相关信息，而不仅仅是有很多单词。那么，我们怎样才能做到呢？我们将引入另一个参数b，该参数用于构造 normalizer：在BM25公式中的 (1-b) +b*dl/dlavg 。参数b的值必须在0到1之间才能使其正常工作。
        $$
        w_{t} = -\frac{(k_{1}+1)·tf(d;t)}{k_{1}(1-b+b·\frac{l_{d}}{l_{avg}}) + tf(d;t)}·idf(t)
        $$
        首先，让我们理解文档的长短意味着什么。同样，文档是长是短取决于语料库的上下文。一种方法是使用语料库的平均长度作为参考点。长文档就是比语料库的平均长度长，短文档就是比语料库的平均长度短。可以看到，当b为1时，normalizer部分变为(1–1 + 1· dl/dlavg)，另一方面，如果b等于0，整个部分等于1，文本长度的影响没有考虑到。
    
        当文档的长度(dl)大于平均文档长度时，该文档将受到惩罚并获得较低的分数。参数b控制着这些文档受到的惩罚有多强: b值越高，长文档受到的惩罚就越高，因为与平均语料库长度相比，文档长度的影响会被放大。另一方面，小于平均水平的文档会得到奖励，并给出更高的分数。
    
        In summary, TF-IDF rewards term frequency and penalizes document frequency. BM25 goes beyond this to account for document length and term frequency saturation.
    
        通过上面的部分，可以理解为BM25是TF-IDF算法的拓展，因此也是可以生成向量表示的。BM25 ranks a set of documents based on the query terms appearing in each document，**通常用来计算一个query和一组document之间的相关性得分**。
        
        Given a query *Q*, containing keywords $q_1, ..., q_n$, the BM25 score of a document *D* is:
        $$
        Score(D,Q)= \sum_{i=1}^n w_{q_i}= \sum_{i=1}^n\frac{(k_{1}+1)·TF(D;q_i)}{k_{1}·(1-b+b·\frac{|D|}{avgdl}) + TF(D;q_i)}·IDF(q_i)
        $$
        其中，$TF(D;q_i)$ 表示 term  $q_i$ 在文档 D 中出现的频率， $|D|$  表示文档D的文本长度，也就是包含的term的个数，avgdl表示一组文档的平均文档长度。常用值 $b=0.75,k_1=[1.2,2.0]$。
        
        简单来讲就是，先将query进行分词，然后取每个term或是token以及一篇给定的文档带入公式计算得到一个得分，然后将query中所有token的得分相加就得到了最终相似度得分。
        
        该方法并不是采用将query也算在这组文档中，使用BM25算法分别得到对应的向量表示再计算相似度得分，而是文档不做变动，只对query进行拆分，token得分相加求和得到最终结果的。
        
        
    
  - 连续表示：语义匹配
    - Word2Vec：[代码实现](https://towardsdatascience.com/word2vec-with-pytorch-implementing-original-paper-2cd7040120b0)
    
      The continuous bag-of-words (CBOW) architecture predicts the current word based on the context words, and the skip-gram predicts surrounding words given the current word.
    
      - CBOW：基于上下文词预测当前词。window size：大约是5，训练更快
      - Skip-Gram：基于当前词预测上下文词。window size：大约是10，训练较慢
    
      ![image-20230320135711664](C:\Users\千江映月\AppData\Roaming\Typora\typora-user-images\image-20230320135711664.png)
    
      ![image-20230320145846352](C:\Users\千江映月\AppData\Roaming\Typora\typora-user-images\image-20230320145846352.png)
    
      ​	[图片来源](https://towardsdatascience.com/understanding-feature-engineering-part-4-deep-learning-methods-for-text-data-96c44370bbfa)
    
      - samples
    
        - (word, word in the same window), with label 1. 
    
        - (word, random word from the vocabulary), with label 0.
    
    - Glove
    
      像LSA这样的方法可以有效地利用统计信息，但它们在单词类比任务上表现相对较差，比如我们如何找到语义相似的单词（context-counting的缺陷）。像skip-gram这样的方法可能在类比任务上做得更好，但它们没有在全局层面上很好地利用语料库的统计信息（context-predicting的缺陷）。
    
      ![image-20230320085407927](C:\Users\千江映月\AppData\Roaming\Typora\typora-user-images\image-20230320085407927.png)
    
      ![image-20230320085435656](C:\Users\千江映月\AppData\Roaming\Typora\typora-user-images\image-20230320085435656.png)
    
    - fastText
    
       In general, predictive models like the Word2Vec model typically considers each word as a distinct entity (e.g. **where**) and generates a dense embedding for the word. However this poses to be a serious limitation with languages having massive vocabularies and many rare words which may not occur a lot in different corpora. The Word2Vec model typically ignores the morphological structure of each word and considers a word as a single entity. The FastText model *considers each word as a Bag of Character n-grams.* This is also called as a subword model in the paper.
    
      ![image-20230320094724797](C:\Users\千江映月\AppData\Roaming\Typora\typora-user-images\image-20230320094724797.png)
    
      - 训练过程细节
    
        一般情况下，使用fastText进行文本分类的同时也会产生词的embedding，即embedding是fastText分类的产物。
    
        先进行文本分词，然后针对一个token（比如where），再获取其对应的n-gram特征，比如（字词：“<wh”、“whe”、“her”、“ere”、“re>“以及特殊字词“\<where\>"）。在fastText中，对于任意一个词w，获取其⻓度在3和6之间的所有⼦词与其特殊⼦词的并集，从而将其所有字词向量表示求和或求平均值作为 w 的向量表示，句子的向量表示将所有词向量表示取平均值（The word representations are then averaged into a text representation, which is in turn fed to a linear classifier.），然后进行文本分类任务（源码中是多标签分类任务）。通过训练分类任务，得到对应的subword字词的对应嵌入表示，并进一步获得对应词的向量表示。
    
      - 方法优点：
    
        - 可以理解前缀和后缀的含义。
        - 不常见词和未见过词都可以在拆分为字符级n-grams之后获得对应的嵌入表示。
    
    - ELMO
    
    - BERT

#### Notes

- BoW词袋模型缺陷

  - 向量表示稀疏
  - 不能捕获词之间的语义关系

- Vector Space Models 

  - 在同一个上下文中出现和使用的单词在语义上彼此相似，具有相似的含义。

  - context-counting：Latent Semantic Analysis (LSA)

    基于计数的方法可用于计算某些单词在语料库中的与邻近单词共现的频率，然后从这些统计指标中为每个单词构建稠密的单词向量表示。

  - context-predicting：

    基于神经网络的语言模型之类的预测方法试图从语料库中的单词序列中的邻近单词来预测单词，并在此过程中学习分布式表示形式，从而为我们提供稠密的单词嵌入表示。

