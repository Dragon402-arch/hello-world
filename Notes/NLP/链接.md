- 知识蒸馏：

  - https://medium.com/aviation-software-innovation/on-distillation-knowledge-from-teachers-to-students-dcd18f7c49ab

    ![image-20220119153417067](C:\Users\千江映月\AppData\Roaming\Typora\typora-user-images\image-20220119153417067.png)

    分段训练 ，将添加的线性层和block内部的线性层合并，

  - 关系：

    - teacher bert-base ：12层

    - student bert-tiny ：6层

      方案一：最后六层，与六层进行匹配，

      方案二：每隔一层进行匹配，[1,2,3,4,5,6]，[2,4,6,8,10,12]

      

  - 模型压缩（ Model compression）

    - 引出：
  
      - 原因：硬件资源有限导致模型部署困难。
      - 替代方案：云计算，问题一是网络连接需要保持始终在线

    - 方案：Model compression can be divided into two broad categories,

      - **Pruning** : Removing redundant connections present in the architecture. Pruning involves cutting out unimportant weights (which are usually defined as weights with small absolute value).通常定义为具有小绝对值的权重。

        显然，剪切后所形成的新模型的准确性较低，因为该模型实际上是使用原始连接而训练的。这就是为什么在剪枝后对模型进行微调以恢复精度的原因。

      - **Quantization** (reducing the number of bits required to store each weight), focuses on reducing the size of the model weights (from 32-bit numbers to 16 or 8-bit numbers).

      - **Parameter sharing** (this reduces the overall parameters needed by the model) 

      -  **Knowledge distillation** (creating a new “student” model with fewer layers but similar accuracy as the original “teacher” model).
  
        > When a language model is compressed by traditional KD techniques, it loses a lot of accuracy when applied to downstream tasks like sentence classification. This is because previous KD methods focused on teaching the student model the final probability logit that the teacher predicted without considering any of the hidden layer values. This is analogous to a real-life scenario where a child learns what the final answer to a question is but has no clue how they got to that answer, the child is more likely to make a mistake when confronted with a slightly different problem. This is true of the student model that is learned using traditional KD methods as well. Based on this assumption, our paper proposes a loss function that not only tries to match the student model’s final probabilities to those of the teacher model, but also tries to match their hidden representations, thereby making the student model’s generalization ability stronger. The model is called “Patient Knowledge Distillation” (PKD).
        >
        > [链接](https://towardsdatascience.com/patient-knowledge-distillation-88969fffb4ba)

        ![在这里插入图片描述](https://img-blog.csdnimg.cn/20210222130001255.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzE2OTQ5NzA3,size_16,color_FFFFFF,t_70)

  - 

  - 

  - 

  - 

  - https://poshai.medium.com/ai-academy-introduction-to-natural-language-processing-e4b8f9bf6396

  - https://medium.com/posh-engineering/experiments-with-knowledge-distillation-of-dual-encoder-models-aae0c0f7100
  
  - https://medium.com/nlplanet/two-minutes-nlp-33-important-nlp-tasks-explained-31e2caad2b1b
  
  - FastText 生成词向量和分类的原理
  
  - Tokenization or word segmentation is a simple process of separating sentences or words from the corpus into small units, i.e. tokens.
  
    

