#### Weight Decay vs. L2 Regularization

- 讲解

  - [讲解1](https://arxiv.org/pdf/1711.05101.pdf)
  - [讲解2](https://benihime91.github.io/blog/machinelearning/deeplearning/python3.x/tensorflow2.x/2020/10/08/adamW.html)

- L2 正则化

  损失函数
  $$
  C= C_{0} + \frac{λ}{2n}\sum_{w}w^2
  $$
  梯度计算
  $$
  \frac{∂C}{∂w} = \frac{∂C_0}{∂w} + \frac{λ}{n}w
  $$
  权重更新
  $$
  \begin{aligned} w_{t+1} &= w_{t} -  α\frac{∂C}{∂w_{t}} \\
  & = w_{t} -  α(\frac{∂C_0}{∂w_{t}} + \frac{λ}{n}w_{t})\\
  & =(1-\frac{λα}{n})w_{t} -   α\frac{∂C_0}{∂w_{t}} \end{aligned}
  $$
  其中  α 表示学习率，λ表示正则化项系数。

  The above weight equation is similar to the usual gradient descent learning rule, except the now we first rescale the weights **w** by **(**1−(α*λ)/n). This term is the reason why L2 regularization is often referred to as **weight decay** since **it makes the weights smaller**. Hence you can see why regularization works, it makes the weights of the network smaller. **The smallness of weights implies that the network behaviour won’t change much if we change a few random inputs here and there which in turn makes it difficult for the regularized network to learn local noise in the data. This forces the network to learn only those features which are seen often across the training set.**

  正则化方法使得权重的取值变得更小，较小的权重取值意味着如果随机的输入发生了一点变化，而神经网络模型的行为不会发生太大的变化，这反过来使得正则化的网络模型很难学习到数据中的局部噪声，并迫使模型只学习整个训练集中经常见到的那些特征。

  代码内部实现：不修改损失函数，而是梯度计算部分进行修改

  ```python
  # compute the gradients to update w
  # grad_w is the gradients of loss wrt to w
  gradients = grad_w + lamdba * w
  # update step
  w = w - learning_rate * gradients
  ```

- 权重衰减（默认针对所有参数，也就是包括weight 和 bias，而L2正则化则只是针对weight的）

  Weight decay can be incorporated directly into the weight update rule, rather than just implicitly by defining it through to an objective function.权重衰减可以直接添加到权重更新规则中，而不是通过修改目标损失函数而间接添加它。

  代码内部实现：不修改损失函数，也不修改梯度计算部分，而是对权重更新部分进行修改。

  ```python
  # compute the gradients to update w
  # grad_w is the gradients of loss wrt to w
  gradients = grad_w
  # update step 有所简化,实际上learning_rate * lamdba等于权重衰减因子
  w = w - learning_rate * gradients - learning_rate * lamdba * w
  ```

  权重更新规则（不修改梯度计算部分，直接修改权重更新规则）
  $$
  w_{t+1} = (1-η)w_{t}- α\frac{∂C_0}{∂w_t}
  $$
  其中  η 为权重衰减因子（decay factor），α表示学习率，学习率不需要乘上 ηw。

  权重衰减因子取值规则： the larger the number of batch passes to be performed, the smaller the optimal weight decay.

  Smaller datasets and architectures seem to require larger values for weight decay while larger datasets and deeper architectures seem to require smaller values.

  L2正则化权重更新规则为：
  $$
  w = (1-αλ)w- α\frac{∂C_0}{∂w}
  $$
  其中 λ 为正则化项系数，α表示学习率

  可以看到如果令  λ = η / α ，可以使得两个等式等同。 

- 比较

  The major difference between **L2 regularization** & **weight decay** is while the former modifies the gradients to add `lamdba * w` , **weight decay** does not modify the gradients but instead it subtracts `learning_rate * lamdba * w` from the weights in the update step.

- 结论

  权重衰减与L2 正则化只有在SGD优化器时是等价的，在其他优化器（adaptive gradient algorithms 自适应梯度算法）使用时是不同的更新规则。

- 代码中添加L2/L1正则化项

  ```python
  def train(epoch):
    print('\nEpoch : %d'%epoch)
    
    model.train()
  
    running_loss=0
    correct=0
    total=0
  
    for data in tqdm(trainloader):
      
      inputs,labels=data[0].to(device),data[1].to(device)
      
      outputs=model(inputs)
      
      loss=loss_fn(outputs,labels)
      
      #Replaces pow(2.0) with abs() for L1 regularization
      
      l2_lambda = 0.001
      l2_norm = sum(p.pow(2.0).sum()
                    for p in model.parameters())
  
      loss = loss + l2_lambda * l2_norm
      
      optimizer.zero_grad()
      loss.backward()
      optimizer.step()
  
      running_loss += loss.item()
      
      _, predicted = outputs.max(1)
      total += labels.size(0)
      correct += predicted.eq(labels).sum().item()
        
    train_loss=running_loss/len(trainloader)
    accu=100.*correct/total
    
    train_accu.append(accu)
    train_losses.append(train_loss)
    print('Train Loss: %.3f | Accuracy: %.3f'%(train_loss,accu))
  ```

  
