#### 正确衡量模型性能：拆分数据集

> To accomplish this goal of generalizing well, it is standard practice to either split a dataset into three randomly sampled partitions (called the ***training*, *validation*, and *test* datasets**) or do ***k-fold cross validation***. Splitting into three partitions is the simpler of the two methods because it only requires a single computation. **You should take precautions to make sure the distribution of classes remains the same between each of the three splits, however**. In other words, it is good practice to aggregate the dataset by class label and then randomly split each set separated by class label into the training, validation, and test datasets. A common split percentage is to reserve 70% for training, 15% for validation, and 15% for testing.
>
> 为实现泛化较好的目标，标准的做法要么是随机拆分数据集为训练集、验证集、测试集（需要确保拆分后与拆分前数据集的类别分布保持一致，比如原数据集中类别A、B、C的样本比例为4:3:2，那么在拆分后的三个数据集中也要保持比例4:3:2），要么是K折交叉验证。
>
> In some cases, a predefined training, validation, and test split might exist; this is common in datasets for benchmarking tasks. In such cases, **it is important to use training data only for updating model parameters, use validation data for measuring model performance at the end of every epoch, and use test data only once, after all modeling choices are explored and the final results need to be reported.** This last part is extremely important because the more the machine learning engineer peeks at the model performance on a test dataset, the more they are biased toward choices which perform better on the test set. When this happens, it is impossible to know how the model will perform on unseen data without gathering more data.
>
> 最后一部分非常重要，因为机器学习工程师在测试数据集上观察模型的性能越多，他们就越倾向于选择在测试集上表现更好的模型。当这种情况发生时，如果不收集更多的数据，就不可能知道模型如何处理未见过的数据（无法评估泛化性能）。
>
> To summarize, you should use the training partition of a dataset to derive model parameters, the validation partition of a dataset for selecting among hyperparameters (making modeling decisions), and the testing partition of the dataset for final evaluation and reporting..
>
> 训练集用来学习模型参数，验证集用来挑选超参数组合，测试集仅使用一次来评估模型性能。
>
> Model evaluation with *k*-fold cross validation is very similar to evaluation with predefined splits, but is preceded by an extra step of **splitting the entire dataset into *k* equally sized “folds.”** **One of the folds is reserved for evaluation, and the remaining *k-1* folds for training.** This is iteratively repeated by swapping out the fold used for evaluation. Because there are *k* folds, each fold gets a chance to become an evaluation fold, resulting in *k* accuracy values. **The final reported accuracy is simply the average with standard deviation. *k*–fold evaluation is computationally expensive but extremely necessary for smaller datasets**, for which the wrong split can lead to either too much optimism (because the testing data wound up being too easy) or too much pessimism (because the testing data wound up being too hard)
>
> 十折交叉验证就意味着每一折做验证集时，其余数据都用来再训练一个模型，最后计算十个模型的准确率以及准确率的标准差。

#### 确定何时停止训练

> The example earlier trained the model for a fixed number of epochs. Although this is the simplest approach, it is arbitrary and unnecessary. One key function of correctly measuring model performance is to use that measurement to determine when training should stop. **The most common method is to use a heuristic called *early stopping*. Early stopping works by keeping track of the performance on the validation dataset from epoch to epoch and noticing when the performance no longer improves. Then, if the performance continues to not improve, the training is terminated. The number of epochs to wait before terminating the training is referred to as the *patience*.** In general, the point at which **a model stops improving on some dataset is said to be when the model has *converged*.** In practice, we rarely wait for a model to completely converge because convergence is time-consuming, and it can lead to overfitting

#### 激活函数作用

> Using a nonlinearity between two Linear layers is essential because without it, two Linear layers in sequence are mathematically equivalent to a single Linear layer and thus unable to model complex patterns.
>
> Nonlinearities are used between each pair of Linear layers to break the linear relationship and allow for the model to twist the vector space around.

#### 语言建模

> Given a sequence of words, predict the next word. This is also called the *language modeling task*.

seq2seq 与 encoder-decoder的关系

> **Sequence-to-sequence  (S2S) models are a special case of a general family of models called encoder–decoder models**.An encoder–decoder model is a composition of two models, an “encoder” and a “decoder,” that are typically jointly trained. The encoder model takes an input and produces an encoding or a representation (ϕ) of the input, which is usually a vector. **The goal of the encoder is to capture important properties of the input with respect to the task at hand. The goal of the decoder is to take the encoded input and produce a desired output.** From this understanding of encoders and decoders, we define S2S models as encoder–decoder models in which the encoder and decoder are sequence models and the inputs and outputs are both sequences, possibly of different lengths.

#### 条件生成模型

conditioned generation model（条件生成模型），当 conditioning context（条件上下文）来自一个encoder模型时，条件生成模型可以视作是encoder-decoder模型，然而并非所有的条件生成模型都是encoder-decoder模型，因为条件上下文可能是从结构化数据中获取的。常见条件生成任务的有：机器翻译、摘要、问答、聊天机器人等。

> **The mapping between the output (target sequence) and the input (source sequence) is called an *alignment***.

#### 注意力机制

> These encoder hidden states are, somewhat uninformatively, called *values* (or in some situations, *keys*). Attention also depends on the previous hidden state of the decoder, called the *query*.
>
> 编码器encoder的所有hidden states被视为values（或是keys），而解码器decoder在当前时间步的前一个时间步的hidden state被视为query。
>
> **A score function takes as input the *query* vector and the *key* vectors to compute a set of weights that select among the *value* vectors.**

- 相关术语
  - attention weights
  - context vector
  - global attention：depends on the encoder hidden states for all the time steps in the input.
  - local attention：depended only on a window of the input around the current time step.

- 评估序列生成模型效果

  - 人工评估（human evaluation）

  - 自动评估（automatic evaluation）：

    > There are two kinds of metrics for automated evaluation of generated sequences: *n-gram overlap–based metrics* and *perplexity*.
    >
    > *N*-gram overlap–based metrics tend to measure how close an output is with respect to a reference by computing a score using ngram overlap statistics. BLEU, ROUGE, and METEOR are examples of *n*-gram overlap*–*based metrics.
    >
    > For a output sequence *x*, if *P*(*x*) is the probability of the sequence, perplexity is defined as follows:
    >
    > $Perplexity(x)=2^{-P(x)logP(x)}$


#### 机器翻译

The “decoder” is a conditional language model. The output is based on the sequence generated so far *and* the original text to be translated:

![seq2seq](D:\Typora\Notes\NLP\概念与理论\seq2seq.png)

The decoder is trained with a method called “**teacher forcing**”. The target sequence is the input sequence offset by one. It is learning to predict the word that comes next.

#### 系统划分

1. 单语言与多语言系统

   我们经常发现多语言产品实现为单独优化的单语言系统的集合，语言识别组件将输入分派给对应的单语言系统。

2. Closed-domain versus open-domain systems（封闭域与开放域系统）

   - 封闭域系统：针对特定领域做专门的优化
   - 开放域系统：注重系统的通用使用。

3. End-to-end systems versus piecewise systems（端到端系统与分段系统）

   - 端到端系统
     - 包括pipeline和joint两种形式的端到端系统，大大降低实现和部署的复杂性，降低代码行数。
   - 分段系统
     - 将复杂的NLP任务分解为子任务，每个任务分别优化，独立于最终任务目标。分段系统中的子任务使其非常容易模块化，易于解决单个模块中出现的问题，但是也会存在错误传播的问题。

4. Unimodal versus multimodal systems（单模态与多模态系统）

5. Interactive versus noninteractive systems（交互式与非交互式系统）

6. Online versus offline systems（在线系统和离线系统）

   - Online system：Online systems are those in which the model predictions need to be made in real time or near real time.实时预测
     - 一些在线系统甚至可以进行在线学习（online learning）
     - 绝大多数在线系统是通过定期更新离线模型并部署到生产环境中实现的。
   - Offline systems：don’t need to run in real time. We can build them to run efficiently on a batch of inputs at once. 批量预测

