#### 经典名词

- 学习率预热和衰减 learning rate warmup

- 混合精度训练，FP16、BF16
  - loss scale 损失值缩放
  
- 梯度积累

- 梯度剪切 Gradient clipping

  - 计算公式

    $$
    H(X) =
    \begin{cases}
     
    \nabla _{\theta }L, & if\ ||\nabla _{\theta }L||<\epsilon, \\\\\
    \epsilon ·\frac{\nabla _{\theta }L}{||\nabla _{\theta }L||}, & else
    \end{cases}
    $$
    

- 梯度爆炸、梯度消失

- 权重衰减
  - 权重分组衰减
  
- 残差连接

- 过拟合与欠拟合
  - 过拟合：使用交叉验证方法评估模型的性能，训练集表现好，验证集表现差，则可能是过拟合。
  - 欠拟合：使用交叉验证方法评估模型的性能，训练集和验证集都表现差，则可能是欠拟合。
    - 加数据
    - 换复杂模型

- 相对位置编码与绝对位置编码

- 预训练模型类型
  - Left-to-Right LM:GPT
  - Masked LM:BERT
  - Prefix LM:UniLM1
  - Encoder-Decoder:T5,BART


- 有哪些防止模型过拟合的方法？

  - weight decay 权重衰减

  - residual connection 残差连接

  - batch norm and layer norm

  - dropout 

  - Regularization
    
    In general, regularization is a technique that helps reduce overfitting or reduce variance in our network by penalizing for complexity. 
    
    - L2 Regularization
    - L1 Regularization
    - While regularizing an estimator, there is a tradeoff where we have to choose a model with increased bias and reduced variance. An effective regularizer is one which makes a profitable trade, reducing variance significantly while not overly increasing the bias.
    
  - Early Stopping

    Another way to use the [EarlyStopping](https://androidkt.com/save-the-best-model-using-modelcheckpoint-and-earlystopping-in-keras/) callback. It will interrupt training when it measures no progress on the validation set for a number of epochs (defined by the patience argument), and it will optionally roll back to the best model. 

  - Data Augmentation

- 梯度消失与梯度爆炸的原因

- 分类问题没有使用平方差损失的原因

- 残差连接 residual connection or Skip connection

  - 名称由来

    Some functions *H*(*x*) are very difficult for a sequence of layers to learn, but learning the corresponding residual functions *H*(*x*) - *x* are much easier.

    With residual connection, our layers only have to learn *F*(*x*) = *H*(*x*) - *x*, and the final output of the residual block is *F*(*x*) + *x = H*(*x*) - *x* + *x = H*(*x*) as desired. 

  - 深度前馈神经网络存在的问题

    - One of the major benefits of a very deep network is that it can represent very complex functions.
    - However, a huge barrier to training them is vanishing gradients: very deep networks often have a gradient signal that goes to zero quickly, thus making gradient descent prohibitively slow.
    - More specifically, during gradient descent, as we backprop from the final layer back to the first layer, we are multiplying by the weight matrix on each step. If the gradients are small, due to large number of multiplications, the gradient can decrease exponentially quickly to zero (or, in rare cases, grow exponentially quickly and “explode” to take very large values).

  对于前馈神经网络，由于存在梯度爆炸和梯度消失等问题，训练深度网络通常是非常困难的。另一方面，即使网络具有数百层，具有残差连接的神经网络的训练过程也更容易收敛。换句话说，残差连接并不能解决梯度爆炸或消失的问题。相反，它通过在集成中使用浅层网络来避免这些问题。（In other words, residual connection does not resolve the exploding or vanishing gradient problems. Rather, it avoids those problems by having shallow networks in the “ensembles”.）

  **The key characteristics of residual connection is that it provides short paths from early layers to later layers。**

