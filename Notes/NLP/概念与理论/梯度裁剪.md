### Gradient Clipping

- [讲解](https://androidkt.com/how-to-apply-gradient-clipping-in-pytorch/)

- 作用：训练神经网络时，容易出现梯度消失和梯度爆炸的问题，此时可以**使用梯度裁剪方法解决梯度爆炸的问题，不能解决梯度消失的问题**。

  通过使用梯度向量的范数对梯度向量进行缩放或是将梯度值裁剪到指定范围内，可以使得训练过程变得稳定。

  

#### 1. Gradient Scaling 梯度缩放

Whenever **the gradient norm** is greater than **a particular threshold**, we **clip the gradient norm** so that it stays within the threshold. This threshold is sometimes set to 1. You probably want to clip the whole gradient by its global norm.

`nn.utils.clip_grad_norm_()`

```python
epochs=10
for epoch in range(epochs):
      total_loss = 0.0
      total_acc=0.0
      for i, batch in enumerate(train_iterator):
            (feature, batch_length), label = batch.comment_text, batch.toxic
            
            optimizer.zero_grad()
            output = model(feature, batch_length).squeeze() 
            # print(output)
            
            loss = loss_function(output, label)
            acc=model_accuracy(output,label)
            loss.backward()
            nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0, norm_type=2)
            optimizer.step()

            total_loss += loss.item()
            total_acc+=acc.item() 
      
      print(f"loss on epoch {epoch} = {total_loss/len(train_iterator)}")
      print(f"accuracy on epoch {epoch} = {total_acc/len(train_iterator)}")
```

The norm is computed over all gradients together as if they were concatenated into a single vector. All of the gradient coefficients are multiplied by the same `clip_coef`.

将所有参数的梯度值好像拼接成为一个向量，然后所有梯度值在缩放后乘上一个裁剪系数。

- 计算公式
  $$
  H(X) =
  \begin{cases}
   
  \nabla _{\theta }L, & if\ ||\nabla _{\theta }L||<\epsilon, \\\\\
  \epsilon ·\frac{\nabla _{\theta }L}{||\nabla _{\theta }L||}, & else
  \end{cases}
  $$
  

#### **2. Gradient Clipping 梯度裁剪** 

It forces the gradient values to a specific minimum or maximum value if the gradient exceeded an expected range. We set a threshold value and if the gradient is more than that then it is clipped.

For example, we could specify a norm of 0.5, meaning that if a gradient value was less than -0.5, it is set to -0.5 and if it is more than 0.5, then it will be set to 0.5.

梯度裁剪代码：

```python
nn.utils.clip_grad_value_(model.parameters(), clip_value=1.0)
```

其中 clip_value (float or int): maximum allowed value of the gradients.The gradients are clipped in the range 【-clip\_value，clip\_value】



