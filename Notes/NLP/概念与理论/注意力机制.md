#### 1. Luong Attention Mechanism

##### 1.1 计算过程（2015）

[参考](https://blog.floydhub.com/attention-mechanism/)

给定source sequence 序列生成的encoder hidden states为 $s_1,s_2,…,s_K$,则在已经获得解码器decoder在time step为 $t$ 时的隐藏状态为$h_t$ ，则解码器生成每个预测的计算过程为：

 given the target hidden state $h_t$​ and the source-side context vector $c_t$

**1.Calculating Alignment Scores**
$$
score(h_t,s_k) =
\begin{cases}
h_t^\mathrm T s_t, & dot \\\\\
h_t^\mathrm T W_as_t, & general \\\\\
W_a[h_t;s_t], & concat
\end{cases}
$$

其中 $[h_t;s_t]$ 表示将两个向量进行拼接。

**2.Softmaxing the Alignment Scores**
$$
a_{t}(k) = align(h_t,s_k) = \frac{exp{(score(h_t,s_k)}}{\sum_{k=1}^Texp{(score(h_t,s_k))}}
$$
其中 $a_{t}(k)$是一个标量，将解码器所有 time step 的注意力权重值拼起来可以组成一个 $K*T$ 的矩阵。

**3.Calculating the Context Vector**
$$
c_t = \sum_ks_ka_t(k) = [s_1,s_2,…,s_K]· a_t
$$
**4.Producing the Final Output**
$$
\tilde{h}_t = tanh(W_c[c_t;h_t])
$$

$$
p(y_t|y_{<t}, x) = softmax(W_h·\tilde{h}_t )
$$



##### 1.2 转换过程

1. Producing the Encoder Hidden States - Encoder produces hidden states of **each** element in the input sequence
2. Decoder RNN- the previous decoder hidden state ($h_{t-1}$) and decoder output  ($\hat{y}_{t-1}$) is passed through the **Decoder RNN** to generate a **new hidden state** for that time step ($h_{t}$) 
3. Calculating Alignment Scores- using the new decoder hidden state ($h_{t}$)  and the encoder hidden states ($s_1,s_2,…,s_K$), **alignment scores** are calculated 
4. Softmaxing the Alignment Scores - the alignment scores for each encoder hidden state are combined and represented in a single **vector** and subsequently **softmaxed** ($a_t$)
5. Calculating the Context Vector - the encoder hidden states and their respective alignment scores are multiplied to form the **context vector **($c_t$)
6. Producing the Final Output - the context vector is *concatenated* with the decoder hidden state generated in step 2 as passed through a fully connected layer to produce a **new output **($\tilde{h}_t$)
7. The process (steps 2-6) **repeats** itself for each time step of the decoder until an token is produced or output is past the specified maximum length

#### 2. Bahdanau Attention Mechanism

##### 2.1 计算过程（2014）

**1.Calculating Alignment Scores**
$$
score(h_{t-1},s_k) =
\begin{cases}
h_{t-1}^\mathrm T s_t \\\\\
v^\mathrm T tanh(W[h_{t-1};s_k]+b)
\end{cases}
$$
**2.Softmaxing the Alignment Scores**
$$
a_{t}(k) = align(h_{t-1},s_k) = \frac{exp{(score(h_{t-1},s_k)}}{\sum_{k=1}^Texp{(score(h_{t-1},s_k))}}
$$
**3.Calculating the Context Vector**
$$
c_t = \sum_ks_ka_t(k) = [s_1,s_2,…,s_K]· a_t
$$
**4.Calculating the Hidden Statet Vector**
$$
h_t = RNN(c_t,\hat{y}_{t-1},h_{t-1})
$$
**5.Producing the Final Output**
$$
p(y_t|y_{<t}, x) = softmax(c_t,\hat{y}_{t-1},h_t)
$$


##### 2.2 转换过程

1. Producing the Encoder Hidden States - Encoder produces hidden states of **each** element in the source sequence
2. Calculating Alignment Scores - between the previous decoder hidden state and each of the encoder’s hidden states are calculated *(Note: The last encoder hidden state can be used as the first hidden state in the decoder)*
3. Softmaxing the Alignment Scores - the alignment scores for each encoder hidden state are combined and represented in a single **vector**  ($[score(h_{t-1},s_1),score(h_{t-1},s_2),…,score(h_{t-1},s_K)]$) and subsequently **softmaxed**
4. Calculating the Context Vector - the encoder hidden states and their respective alignment scores are *multiplied* to form the **context vector**
5. Decoding the Output- the context vector **($c_t$)** is *concatenated* with the previous decoder output **($\hat{y}_{t-1}$)** and fed into the **Decoder RNN** for that time step along with the previous decoder hidden state **($h_{t-1}$)** to produce a **new output** ($h_t$)
6. The process (steps 2-5) **repeats** itself for each time step of the decoder until an token is produced or output is past the specified maximum length

#### 3 seq2seq模型

##### 3.1 输入与输出

- RNN encoder 与 RNN decoder

  ```python
  def _get_source_indices(self, text):
      """Return the vectorized source text，编码器输入
  		[<sos>] + indeices + [<eos>]
          Args:
              text (str): the source text; tokens should be separated by spaces
          Returns:
              indices (list): list of integers representing the text
          """
      indices = [self.source_vocab.begin_seq_index]
      indices.extend(self.source_vocab.lookup_token(token) for token in text.split(" "))
      indices.append(self.source_vocab.end_seq_index)
      return indices
  
  
  def _get_target_indices(self, text):
      """Return the vectorized source text
  
          Args:
              text (str): the source text; tokens should be separated by spaces
          Returns:
              a tuple: (x_indices, y_indices)
                  x_indices (list): list of integers representing the observations in target decoder  [<sos>]+indices
                  y_indices (list): list of integers representing predictions in target decoder  indices + [<eos>]
      """
      indices = [self.target_vocab.lookup_token(token) for token in text.split(" ")]
      x_indices = [self.target_vocab.begin_seq_index] + indices  # 解码器输入
      y_indices = indices + [self.target_vocab.end_seq_index]  # 解码器输出
      return x_indices, y_indices
  
  # x_indices = [<sos>,1,2,3]
  # y_indices = [1,2,3,<eos>]
  ```

- GPT与GLM模型中的输入与输出

  ```python
  # 数据处理部分
  inputs = prompt + completion + [eop]
  labels = [-100] * len(prompt) + completion + [eop]
  
  # inputs与labels中的索引是对齐的。
  
  # 模型内部处理部分
  # Shift so that tokens < n predict n
  shift_logits = logits[..., :-1, :].contiguous()
  shift_labels = labels[..., 1:].contiguous()
  # Flatten the tokens
  loss_fct = CrossEntropyLoss()
  loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))
  ```
  

#### 3. seq2seq模型版本

##### 3.1 初级版本

- 对应文章： Sequence to Sequence Learning with Neural Networks

- 计算过程

  ![seq2seq1](D:\Typora\Notes\NLP\概念与理论\seq2seq1.png)

  其中使用了*start of sequence* (`<sos>`) 和 *end of sequence* (`<eos>`) token 表示序列的开始和结束；$h_0$向量通常初始化为零向量或可学习的模型参数。

  > Once the final word, has been passed into the RNN via the embedding layer, we use the final hidden state, $h_T$, as the context vector, i.e. $h_T=z$. This is a vector representation of the entire source sentence. **The initial decoder hidden state is the final encoder hidden state.** 
  >
  > We always use `<sos>` for the first input to the decoder, $y_1$, but for subsequent inputs, $y_{t>1}$, we will sometimes use the actual, ground truth next word in the sequence, $y_t$ and sometimes use the word predicted by our decoder, $\hat{y}_{t-1}$. This is called ***teacher forcing***.
  >
  > When training/testing our model, we always know how many words are in our target sentence, so we stop generating words once we hit that many. During inference it is common to keep generating words **until the model outputs an `<eos>` token or after a certain amount of words have been generated**.

  编码器输出：

   The initial hidden state, $s_0$, is usually either initialized to zeros or a learned parameter.
  $$
  s_t = EncoderRNN(x_t^{source},s_{t-1})
  $$
  解码器输出：

  其中 $h_0=s_T$,This is a vector representation of the entire source sentence.We always use `<sos>` for the first input to the decoder, $y_1$, but for subsequent inputs, $y_{t>1}$, we will sometimes use the actual, ground truth next word in the sequence, $y_t$ and sometimes use the word predicted by our decoder, $\hat{y}_{t-1}$. This is called ***teacher forcing***.
  $$
  h_t = DecoderRNN(x_t^{target},h_{t-1})
  $$
  最终预测输出：

$$
p(\hat{y}_t|y_{<t}, x^{source}) = f(h_t)
$$

##### 3.2 升级版本1

- 对应文章：Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation

- 计算过程

  ![seq2seq2](D:\Typora\Notes\NLP\概念与理论\seq2seq2.png)

  编码器输出：
  $$
  s_t = EncoderRNN(x_t^{source},s_{t-1})
  $$
  解码器输出
  $$
  h_t = DecoderRNN(x_t^{target},h_{t-1},c)
  $$
  其中 $c=s_T$，$c$ 作为编码器的上下文向量（context vector），并没有下标 $t$，意味着在解码器的每个time step都会使用该上下文向量。

  > Note how this context vector, $c$, does not have a $t$ subscript, meaning we re-use the same context vector returned by the encoder for every time-step in the decoder.
  >
  > the initial hidden state, $h_0$, is still the context vector, $c$, so when generating the first token we are actually inputting two identical context vectors into the GRU.也即是$DecoderRNN(x_t^{target},h_0=c,c)$

​		最终预测输出：
$$
p(\hat{y}_t|y_{<t}, x^{source}) = f(x_t^{target},h_t,c)
$$
​		以上两个版本要求Encoder和Decoder使用的RNN中 hidden_size 超参数要保持相同，使用注意力机制时就不会出现该问题。

##### 3.3 升级版本2——Bahdanau Attention

##### 3.4 升级版本3——Luong Attention

##### 3.5 升级版本4——Transformer

Byte-pair encoding overcome the OOV problem

Regardless, many such tasks require the model to be able to produce tokens in the target sequence that appear in the source sequence but that are out-of-vocabulary (OOV) with respect to the target vocab. 

无论如何，许多这样的任务要求模型能够在目标序列生成在源序列中出现过的token，但该token并不在目标序列对应的词汇表中。

- OOV问题解决方案：
  - Incorporating Copying Mechanism in Sequence-to-Sequence Learning
  - Get To The Point: Summarization with Pointer-Generator Networks
  - Abstractive Text Summarization using Seq2Seq RNNs and Beyond







#### 3.5 参考博客

 [参考1](https://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&mid=2650428907&idx=2&sn=a65aee9d27f9515d7803b2615848fa18&chksm=becdd3b189ba5aa711d1da8aa44626966a6456658ee43c55b5fa6d4ce4da43c45fde30c20459&mpshare=1&scene=24&srcid=0130AqwQntOVj3cWjlvkdyYk&sharer_sharetime=1643518939341&sharer_shareid=8ce393c5a94f02ed5f009a140f1f10db#rd)

[参考2](https://mp.weixin.qq.com/s?__biz=MjM5ODkzMzMwMQ==&mid=2650429060&idx=2&sn=0e6a87d32a0d00e9c63368d6b557df77&chksm=becdd0de89ba59c8eb85d8d9371bc5918c6971ba82ae326c182c562d7ffdedadaf161933da98&mpshare=1&scene=24&srcid=0214mUzNpAwwrmKuprHgdbCn&sharer_sharetime=1644811314472&sharer_shareid=8ce393c5a94f02ed5f009a140f1f10db#rd)

[参考3](https://mp.weixin.qq.com/s?__biz=Mzk0NzMwNjU5Nw==&mid=2247483826&idx=1&sn=f7a075197652f96389ea902e57ae1d3c&chksm=c379a8b8f40e21ae57a3fe965c9121e082ee32432e8c9a964cae8ff0a3d6f99f10377b501cee&cur_album_id=2226786624636518404&scene=189#wechat_redirect)

[文本纠错](https://mp.weixin.qq.com/s?__biz=Mzk0NzMwNjU5Nw==&mid=2247484078&idx=1&sn=763bcffbe3098f1c5cb2176996bee4b5&chksm=c379aba4f40e22b25e68dde18be1669d5af36da39d9ffe1c82177315544cfa1843c8da90f533&scene=126&sessionid=1685687212#rd)

