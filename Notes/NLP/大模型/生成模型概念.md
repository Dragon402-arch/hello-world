#### In-Context Learning

- 出处：GPT-3：Language Models are Few-Shot Learners，2020
- 首次提出 Prompt （提示） 概念，Prompt Learning

- 提出 In-context Learning（语境学习或上下文学习）概念，给模型输入一个或多个示例，并添加上提示语，不更新模型权重，也能完成任务。
  - 示例：”美国的首都是华盛顿，法国的首都是巴黎，英国的首都是 ____“

#### Pattern-Exploiting Training

- 出处：Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference（2021）

#### Instruction Turning：训练模型更好地遵循指令

- 出处：Finetuned Language Models Are Zero-Shot Learners，2022

- 目的：需要模型根据不同的任务要求（指令），做出相匹配的正确回复

  > **The motivation of instruction tuning is to improve the ability of language models to respond to NLP instructions. The idea is that by using supervision to teach an LM to perform tasks described via instructions, the LM will learn to follow instructions and do so even for unseen tasks.**
  >
  > 指令微调目的是提升自然语言模型响应自然语言指令的能力，使用监督学习方法让模型学习通过指令描述执行任务，模型将会学习到遵循指令，即便是在没有学过的任务上也能做到。

- 指令微调是在预训练语言模型的基础上，在多个已知任务上通过对训练数据集添加自然语言形式的指令进行微调，从而激活模型的各方面性能，提高模型对未知任务的泛化能力和与人类期待的一致性，使其可以在某个新任务上进行零样本推理。

- 指令示例：
  1. 请将下面这句话翻译成英文“ChatGPT 都用到了哪些核心技术？”
  2. 请帮我把下面这句话进行中文分词“我太喜欢 ChatGPT 了!” 
  3. 请帮我写一首描绘春天的诗词，诗词中要有鸟、花、草。
  
- 研究表明，当“指令”任务的种类达到一定量级后，大模型甚至可以在没有见过的零样本（Zero-shot）任务上有较好的处理能力。

- 对比：
  - 与 In-Context Learning 对比：
    - In-Context Learning：不更新模型权重（不进行训练），推理时直接小样本学习，few-shot leaning
    - Instruction Turning：更新模型权重（进行训练），推理时直接零样本学习，zero-shot leaning ，提升模型在未见过任务上的推理能力
  - 与训练阶段对比：
    - 预训练阶段：在大规模语料上进行无监督学习
    - 指令微调阶段：在多任务上进行有监督微调（SFT，Supervised Fine-tuning）学习，训练语言模型遵循指令

####  Reinforcement Learning from Human Feedback (RLHF)

- 出处：**InstructGPT**：Training language models to follow instructions with human feedback，2022
- 核心概念
  - SFT（Supervised Fine-tuning，有监督微调）：fine-tune a pretrained GPT-3 model on  demonstration data using supervised learning.
    - Prompt输入类型
      - 直接给出一个自然语言指令：Write a story about a wise frog
      - 先给出几个样例，然后再给出指令：giving two examples of frog stories, and prompting the model to generate a new one
      - 给出指令，对内容进行续写： providing the start of a story about a frog
  - Reward Model（RLHF，基于人类反馈的强化学习）：train a reward model to predict the human-preferred output.
    - 奖励模型以提示和回复作为输入，计算标量奖励值作为输出
    - 实现过程：首先针对同一提示采样多条不同回复。然后，将回复两两组合构成一条奖励模型训练样本，由人类给出倾向性标签。最终，奖励模型通过每条样本中两个回复的奖励值之差计算倾向性概率拟合人类标签，进而完成奖励模型的训练。
  - PPO（proximal policy optimization，近端策略优化算法）：fine-tune the supervised policy to optimize this scalar reward using the PPO algorithm

- 目的：Aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback.
  - 首先要遵循人类指令，生成符合用户意图的回答（避免答非所问情况的出现）
  - 其次要学习人类偏好，生成符合人类偏好的回答（从多个回答中选出更符合人类偏好的那个）



- https://zhuanlan.zhihu.com/p/399705201
- 多轮对话模型
  - 对其用户的意图，
  - 结合历史对话，以及当前问题，给出回复。
- FLAN：Finetuned Language Models Are Zero-Shot Learners，2022
  - 提出 **Instruction Tuning** 概念，提升模型在没有见过的任务上的零样本学习能力。
  - 目的：提高语言模型对NLP指令做出响应的能力，其想法是，通过使用监督学习方法来教语言模型执行通过指令描述的任务，语言模型将学会遵循指令，甚至对于看不见的任务也是如此。
  - 与 prompt tuning 比较
    - 与不进行权重更新的prompt tuning 相比，instruction tuning则**必须进行监督微调**，对模型权重进行更新，权重更新后的模型在推理（Inference）时，仍然可以使用prompt进行提问，此时的prompt就是任务指令（instruction）。
    - 与进行权重更新的prompt tuning 相比，prompt tuning是针对一个特定下游任务进行微调，而instruction tuning 则是同是在多个下游任务上进行微调，最终可以在没有微调过的任务上得到较好的结果。

**部署方式** Online 方式：首先在移动端做初步预处理，然后把数据传到服务

器进行预测后返回移动端。Offline 方式：根据硬件的性能选择模型，在服务

器训练得到模型，在移动端进行预测的过程。

项目在线部署是指，是指将所有的资源都托管在云服务器上。对应的项目离线部署，就是脱离云服务器，将项目资源打包下载，部署在私有服务器上。

离线部署，是指在用户自己的服务器上进行部署。所有的资源都会下载到用户的服务器。进行离线部署

上线或发布：上线指的是将一个网站、应用程序或其他产品正式推出到公众面前，让用户可以访问和使用。

