#### GPT（Generative Pretrained Transformer）

##### GPT-2

- 使用绝对位置编码，通常在序列的右侧而非左侧进行填充。

- 语言建模类型
  - 因果语言建模，causal language modeling (CLM) ， predicting the next token in a sequence
  - 掩码语言建模，masked language modeling（MLM），predicting the mask token in a sequence

- tokenizer：byte-level Byte-Pair-Encoding.

- 创建模型

  ```python
  from transformers import GPT2Config, GPT2Model
  
  # Initializing a GPT2 configuration
  configuration = GPT2Config()
  
  # Initializing a model (with random weights) from the configuration
  model1 = GPT2Model(configuration)
  
  
  # load the model pretrained weights.
  model2 = GPT2Model.from_pretrained(pretranined_model_path)
  ```

- 分类任务

  GPT2ForSequenceClassification uses the last token in order to do the classification, as other causal models (e.g. GPT-1) do.

  使用最后一个token的表示进行序列分类任务，这与其他因果语言模型如GPT-1一样。如果进行了填充则需要找到序列中非填充的最后一个token，如果没有进行填充则直接取最后一个token表示即可。

- 填充

  - 训练时，批量输入需要在序列的右侧进行填充padding，模型知道预测下一个token的任务何时停止。
  - 测试时，批量输入需要在序列的左侧进行填充padding。若还是在右侧填充，模型根本不会生成新的token。
  - GPT2Tokenizer没有默认的【pad_token】，需要自己设置，一般和eos_token设为一样。而且GPT2Tokenizer不会自动在句末增加eos_token，需要自己手动添加，否则模型generate的时候永远不会停下来直到最大长度，因为它不会生成eos_token！！！
    而且train的时候需要padding在右边，否则模型永远学不会什么时候停下！！！
    而且test的时候需要padding在左边，否则模型生成的结果可能全为eos！！！

- 输入

   generate时的设置不同，因为input本身也是output的一部分，所以最好设置max_new_tokens