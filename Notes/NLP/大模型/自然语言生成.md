### Natural Language Generation

- task

  - machine translation 机器翻译
  - Abstractive Summarization 生成式摘要
  - Dialogue Generation 对话生成（多轮对话）
  - Image Captioning 图像描述
  - Creating Writing 创造性写作

- 条件语言模型 conditional Language Modeling

  ![image-20230221234106656](C:\Users\千江映月\AppData\Roaming\Typora\typora-user-images\image-20230221234106656.png)

- 训练策略

  - Teacher Forcing

    ![image-20230222205930544](C:\Users\千江映月\AppData\Roaming\Typora\typora-user-images\image-20230222205930544.png)

  

  - Scheduled Sampling

    Teacher Forcing 表示在训练的时候，只使用真实数据，而不使用模型在前一个 时间步 的预测输出，**结果就会出现训练和测试时不一致的问题**。训练时使用的是真实数据，而在预测时则只能使用在上一个时间步生成的预测输出。（一步错，步步错）针对这个问题，提出一种 Scheduled Sampling 的方法，也就是在每个时间步对真实数据和预测数据进行一个随机采样。之所以没有在训练时全部都使用 预测输出的原因是可能会最终训练出来的模型效果比较差，训练达到一定效果的时间也会比较久。在使用 Scheduled Sampling 方法时，在训练的前期以较大的概率从 真实数据中取样更好，而在训练后期以较大的概率从预测数据中取样更好。

- 解码算法

  在获得一个训练好的条件语言模型之后，解码算法决定了如何从语言模型生成文本。

  - Greedy：找到每个时间步概率最大的 word 作为模型预测输出。

    - 如果词汇表 vocabulary 中的 word 数量非常大，则计算输出序列的复杂度会比较高（V^T）。
    - 按照该方法找到的输出序列可以满足局部最优，但不是全局最优，不能同时保证预测到该序列的概率也是最大的。

  - Beam search

    ![image-20230222212901270](C:\Users\千江映月\AppData\Roaming\Typora\typora-user-images\image-20230222212901270.png)

    选取所有序列中最有可能的前两个序列保持追踪，beam size 的选取要适中，过大过小都不好。过小的话，生成的输出序列，会和输入的主题比较接近，但是却会存在不太通顺的问题，而过大的话，就会出现虽然通顺了，但是和输入主题不太相关的问题。

    ![image-20230222213143299](C:\Users\千江映月\AppData\Roaming\Typora\typora-user-images\image-20230222213143299.png)

  - Sampling

    ![image-20230222214232399](C:\Users\千江映月\AppData\Roaming\Typora\typora-user-images\image-20230222214232399.png)

    k等于1 时，就是一个单纯的贪婪算法，k等于vocabulary size 时，就变成随机采样了。k 取一个中间值，则是一个折中的方案。 

  - Top -k Sampling 

    - top-k 是一个固定的静态值，选好了就定下来了。

  - Top-p Sampling（Nucleus Sampling）

    ![image-20230222231743794](C:\Users\千江映月\AppData\Roaming\Typora\typora-user-images\image-20230222231743794.png)

    这个公式的意思，就是从vocabulary中选取一个子集，而这个子集包含的 word 的个数（top-k）是会随着前面的序列（w1，wi-1）而动态变化的，在每个时间步时该子集必须满足子集内的word的概率值之和超过指定的 p 值，从而获取得到这个满足上述条件的子集，但该子集中所包含的word个数是不固定的。

- Probability Distribution

  ![image-20230222232711800](C:\Users\千江映月\AppData\Roaming\Typora\typora-user-images\image-20230222232711800.png)





- 生成评价指标

  - N-Gram Precision

  - Brevity Penalty

  - BLEU：常用于机器翻译方面，较为关注 precision 方面

    ![image-20230222233656422](C:\Users\千江映月\AppData\Roaming\Typora\typora-user-images\image-20230222233656422.png)

  - ROUGE：常用语生成摘要方面，较为关注 recall 方面

    ![image-20230222233818427](C:\Users\千江映月\AppData\Roaming\Typora\typora-user-images\image-20230222233818427.png)

    ![image-20230222233910421](C:\Users\千江映月\AppData\Roaming\Typora\typora-user-images\image-20230222233910421.png)

- 补充一下强化学习的内容

- 自然语言生成

- 

- 

- 

- 

- 

- 

  - https://medium.com/sfu-cspmp/evolution-of-natural-language-generation-c5d7295d6517
  - [Seq2Seq + Attention](https://medium.com/@adam.wearne/seq2seq-with-pytorch-46dc00ff5164)
  - [Seq2Seq](https://towardsdatascience.com/a-comprehensive-guide-to-neural-machine-translation-using-seq2sequence-modelling-using-pytorch-41c9b84ba350)
  - https://towardsdatascience.com/a-comprehensive-guide-to-neural-machine-translation-using-seq2sequence-modelling-using-pytorch-41c9b84ba350
  - 

  源文本经过编码器输出一个上下文表示的向量(即便是使用注意力机制，只不过是对每个时间步生成不同的上下文向量)，对于解码器，在训练时可以直接使用以下方式进行训练：

  - input：sos，A，B，C

  - output：A，B，C，eos

    训练时可以按照输入一个序列，输出一个序列进行训练，而在预测时则会存在问题，因为解码器的输入序列是逐步生成的，不可能一次输入所有。因此在训练时需要每次输入一个token，然后预测出结果，作为下一次的输入。在获取预测结果时，不是直接使用当前token得到概率最大的下一个token，而是要参考当前token之前序列的元素，这些之前的序列元素信息被整合到了hidden_states中了。

    During prediction the only word available to us is `<sos>` since all the output sentences start with `<sos>`.

    - 解码器每个时间步的输入：
      - 编码器获取的表示（有的仅在第一个时间步使用，有的则在每个时间步都使用）
      - 解码器上一步得到的h、c
      - 解码器上一步的预测结果yt-1

    ​	

    - 文本摘要、智能问答、聊天机器人、机器翻译、自动生成、知识图谱、预训练模型、推荐系统

    - 

      

    
    
    