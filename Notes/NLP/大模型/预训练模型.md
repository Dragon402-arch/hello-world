- Three types of model Pre-Training

  - Encoder

    - BERT and its variants
    - 同时考虑双向上下文

  - Decoder

    - 语言建模，用于生成任务

    - GPT-1/2/3，DialoGPT、LamDA、BLoom、GPT-J、OPT

      ![image-20230223142223314](C:\Users\千江映月\AppData\Roaming\Typora\typora-user-images\image-20230223142223314.png)

    - In-Context Learning：

      - zero-shot learning：不允许输入任何示例（例题），只允许输入一则任务说明（题型说明）
      - one-shot learning：只允许输入一条示例和一则任务说明
      - few-shot learning：输入数条示例和一则任务说明
  
      ![image-20230223142914152](C:\Users\千江映月\AppData\Roaming\Typora\typora-user-images\image-20230223142914152.png)
  
      
  
  - Encoder-Decoder
  
    - 序列到序列的模型
    
    - examples：
    
      - Transformer
      - T5
      - BART
      - mBART：Multilingual BART
      - mT5:Multilingual T5
    
      ![image-20230223151124924](C:\Users\千江映月\AppData\Roaming\Typora\typora-user-images\image-20230223151124924.png)
    
      ![image-20230223151237787](C:\Users\千江映月\AppData\Roaming\Typora\typora-user-images\image-20230223151237787.png)
    
    - BART：absolute position encoding（效果略好过T5）
    - T5：relative position encoding

- Pre-trained Language Model 存在存在的问题

  - 数据稀缺性
    - 下游特定任务的标注数据量可能不太大。
    - 更多的实际情况是小样本、单样本甚至是零样本的场景。
    - 数据不够多时，再进行微调容易出现过拟合的情况。 
  - 预训练模型的参数体量越来越大
    - 较高的算力计算成本（训练和微调时要更新所有的参数，就会有较高的算力要求）
    - 较大的存储空间要求（实际应用时，下游每个特定的任务都要对一个大模型进行一次微调（复制））

  针对预训练模型存在的这些问题，解决方案就是 Prompt-Based Learning

####  Prompt-Based Learning 提示学习

- Leveraging big pre-trained Language models

- 一种做法是选取像GPT-3这样非常强大的预训练语言模型，不对模型参数进行微调就可以直接根据提示完成指定的任务。但是该做法所需的模型无法免费获得和方便使用，另一种做法是选取稍微小一点的预训练语言模型，经过微调之后还可以实现类型大模型的效果。

  ![image-20230223170653268](C:\Users\千江映月\AppData\Roaming\Typora\typora-user-images\image-20230223170653268.png)

  - Prompt Template：针对一个给定的任务，手动设计自然语言输入

  - PLM：执行语言建模，掩码语言模型或是自回归模型都可以

  - Verbalizer：完成从词汇表中 token 到 任务标签的映射

    ![image-20230224104235841](C:\Users\千江映月\AppData\Roaming\Typora\typora-user-images\image-20230224104235841.png)



​	提示微调也是一样在微调预训练模型，用prompt方式处理后的数据来微调，但预训练模型所有的参数还是被调整了，结果反倒是提示微调在一些稀缺数据上的的效果更好一点。原因在于在针对特定任务进行微调时，需要在预训练模型的基础上再添加额外的层，然后再针对该任务微调模型的所有参数，由于预训练和微调时任务的不一致，如果针对给定任务的数据进行微调时可用的标注数据不够多的话，就容易过拟合到这部分数据上，而Prompt-Turning方法在将下游任务转换为和预训练时很相似的任务，可以在原来任务的基础上再进行微调，预训练的知识可以得到保存，在微调时还能利用原来的预训练得到的知识。

- Hard Prompt-Tuning

- Soft Prompt-Tuning

  - P-Tuning

    - idea：direct optimize the embeddings instead of prompt tokens
    - 针对每个任务都要保存 Prompt Encoder（用于生成 prompt部分的嵌入表示） 的参数以及预训练模型微调后的参数。

    ![image-20230224111420988](C:\Users\千江映月\AppData\Roaming\Typora\typora-user-images\image-20230224111420988.png)

  - Prefix-Tuning

    ![image-20230224111445683](C:\Users\千江映月\AppData\Roaming\Typora\typora-user-images\image-20230224111445683.png)

​		微调的时候只更新Prefix部分的权重，而不更新预训练模型的权重，

- 另外的一种 提示学习方法，需要保存的权重是最少的。

​		![image-20230224112816102](C:\Users\千江映月\AppData\Roaming\Typora\typora-user-images\image-20230224112816102.png)



- Instruction tuning

  ![image-20230224133626315](C:\Users\千江映月\AppData\Roaming\Typora\typora-user-images\image-20230224133626315.png)

  ![image-20230224133933633](C:\Users\千江映月\AppData\Roaming\Typora\typora-user-images\image-20230224133933633.png)

​		![image-20230224134153121](C:\Users\千江映月\AppData\Roaming\Typora\typora-user-images\image-20230224134153121.png)



































实际上,大型语言模型在AI领域已经诞生好几年了,但开发人员仍然还在探索与其互动的最佳方式 一些较小的大型语言模型例如BERT,开发人员可以下载下来并根据自己的数据进行训练;但是,对于一些更大的大型语言模型例如GPT-3等,只能在云端进行训练和托管,开发人员只能通过API与其进行交互。