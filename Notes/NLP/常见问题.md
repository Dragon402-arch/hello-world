1. 神经网络中模型权重能不能初始化为0？
   - 神经网络模型不可以，会导致模型停止学习，以最简单的神经网络为例（一共有两层，每层只有1个节点），在计算梯度时发现乘积中包含有参数值（反向传播的链式求导法则），因此参数值取0会使得梯度值一直取0，从而导致模型停止学习，因此不能初始化为全为0的情况。
   - 逻辑回归模型可以，因为梯度值不会因为参数值取0而取0，梯度值会随样本的取值变化而变化，因此是可以的。、
2. 为什么NLP不用batchnorm？
   - 首先BN是对样本维度进行归一化的，而LN是对特征维度进行归一化的。
   - 在NLP中，一个batch内部，不同样本序列的长度不等，虽然进行填充或是截断后可以实现等长，但是如果对一个batch内的样本进行BN（相同位置的token进行BN），反而会加大特征的方差。
   - NLP中，一个batch内部，不同样本序列同一位置的token相关性不大，对其进行BN，反而会破坏token的向量表示。
   - 在NLP中，BN需要计算均值和方差统计量，
   - 使用LN进行归一化处理可以降低方差，加速模型收敛。
3. 为什么分类问题不使用MSE损失函数？
   - 使用MSE（非凸函数）在进行二分类问题时（使用sigmoid激活函数）容易出现学习缓慢的问题。
   - 容易出现多个局部最优解，得到的最优解比较依赖权重初始化方式。
   - 而使用交叉熵容易找到全局最优解（凸函数）。
4. 梯度消失和梯度爆炸产生的原因？
   - 当梯度消失发生时，最后一个隐层梯度更新基本正常，但是越往前的隐层内更新越慢，甚至有可能会出现停滞，此时，多层深度神经网络可能会退化为浅层的神经网络（只有后面几层在学习），因为浅层基本没有学习，对输入仅仅做了一个映射而已。
   - 当梯度爆炸发生时，最后一个隐层梯度同样更新正常，但是向前传播的梯度累计过程中，浅层网络可能会产生剧烈的波动，从而导致训练下来的特征分布变化很大，同时输入的特征分布可能与震荡幅度不同，从而导致最后的损失存在极大的偏差。
   - 梯度消失和梯度爆炸本质上是一样的，均因为网络层数太深而引发的梯度反向传播中的连乘效应。
   - 解决办法：残差连接；更改权重初始化；更换激活函数
5. 大模型如何处理长文本的输入问题？
   - 截断
   - 分段池化
   - 分块输入，生成摘要，再二次拼接生成
6. 

