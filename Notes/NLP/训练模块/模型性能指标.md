### 分类模型性能指标（Classification Model Performance Metrics）

[讲解1](https://towardsdatascience.com/top-10-model-evaluation-metrics-for-classification-ml-models-a0a0f1d51b9)

[讲解2](https://medium.com/analytics-vidhya/how-to-measure-the-performance-of-a-model-f3e1c52d78b0)

- #### Class Labels

  - Balanced Dataset

    - Confusion Matrix

    - Accuracy

      Accuracy doesn’t consider probability scores

  - Imbalanced Dataset

    - Confusion Matrix

    - Precision

      - 单个类别的Precision

        该类别预测正确的样本个数与预测结果中该类别样本总数的比值。

        比如：**假定真实结果中A标签的样本有100个，而预测结果中A标签的样本120个，假定两者取交集的结果为70个，则该类别的Precision值就是70/120，同时该类别的Recall值就是70/100。由数据可知，存在Other标签的50个样本被预测为A标签，同时存在A标签的30个样本被预测为Other标签。**

      - 所有类别的综合Precision

        - macro-Precision 

          所有单个类别的Precision的算术平均值。

          如 A 、B、C 三个标签的Precision值分别为0.6、0.7、0.8，则macro-Precision的结果为（0.6+0.7+0.8）/ 3 = 0.7
        
        - micro-Precision
        
          所有类别预测正确的样本个数与预测结果中样本总数的比值。所有类别预测正确的样本个数实际上就是将预测结果与真实结果取交集。可以总结为：将预测结果和真实结果取交集，然后除以预测结果就是Precision
        
          如 A 、B、C 三个标签的预测正确的样本个数分别为60、70、80，而样本总数为300，则micro-Precision的结果为（60+70+80）/ 300  = 0.7

    - Recall

      - 单个类别的Recall

        该类别预测正确的样本个数与真实结果结果中该类别样本总数的比值。

      - 所有类别的综合Recall

        - macro-Recall

          所有单个类别的Recall的算术平均值。

        - micro-Recall
        
          所有类别预测正确的样本个数与真实结果中样本总数的比值。所有类别预测正确的样本个数实际上就是将预测结果与真实结果取交集。可以总结为：将预测结果和真实结果取交集，然后除以真实结果就是Recall
    
    - F1 Score：实际上是Precision和Recall的调和平均值。
    
      - 单个类别的F1 Score
    
        单个类别的Precision与Recall的调和平均值，调和平均值对异常值（离群点）的鲁棒性更强。
        $$
        \frac {1}{F1} = \frac {1}{2} \bigg (\frac {1}{P} + \frac {1}{R} \bigg )
        $$
        
        $$
        F1 = \frac {2*P*R}{P+R}
        $$
        
      - 所有类别的综合F1 Score
      
        - macro-F1
          $$
          macro-F1 = \frac {2*macro-P*macro-R}{macro-P+macro-R}
          $$
          
        - micro-F1
          $$
          microF1 = \frac {2*micro-P*micro-R}{micro-P+micro-R}
          $$
          
      
    - coding
    
      ```python
      from sklearn.metrics import f1_score, precision_score,recall_score,accuracy_score
      
      y_true = [0, 1, 2, 0, 1, 2, 2, 2]
      y_pred = [0, 2, 1, 0, 0, 1, 2, 1]
      
      print(precision_score(y_true, y_pred, average='micro'))
      print(recall_score(y_true, y_pred, average='micro'))
      print(f1_score(y_true, y_pred, average='micro'))
      print(accuracy_score(y_true, y_pred))
      
      
      """
      输出：
      0.375
      0.375
      0.375
      0.375
      可以看到，micro-precision、micro-recall的值是相等的，因为都是将混淆矩阵中主对角线元素的和除以混淆矩阵所有元素的和，而这恰恰是准确率accuracy的计算方式，因此这三个值是相等的。由于precision和recall是相等的，其调和平均值也必然是相等的。日常使用中主要是以算术平均值marco avg (precision、macro-recall)、或是加权算术平均值weighted avg(precision、macro-recall),后者在求平均值考虑了各个类别的样本数量。
      
      In classification tasks for which every test example is guaranteed to be assigned to exactly one class, micro-F is equivalent to accuracy. It won't be the case in multi-label classification.
      
      Note that for “micro”-averaging in a multiclass setting with all labels included will produce equal precision, recall and F, while “weighted” averaging may produce an F-score that is not between precision and recall.
      """
      ```
      
      [Micro-average讲解](https://simonhessner.de/why-are-precision-recall-and-f1-score-equal-when-using-micro-averaging-in-a-multi-class-problem/)

- #### Probabilities 

  **Assigning different thresholds to create different data points to generate the ROC Curve or Precision-Recall curve.**

  - ##### Receiver Operating Characteristics curve (ROC) and Area under the ROC Curve (AUC)：平衡数据集

    通过分类阈值（thresholds）移动得到一系列坐标点，从而可以绘制得到ROC曲线，**横坐标表示假正例率，纵坐标表示真正例率**，二分类混淆矩阵中第一列除以矩阵对行求和的结果，可以得到一组坐标点。

     **The area under the ROC curve is known as AUC. The more the AUC the better your model is.** The farther away your ROC curve is from the middle linear line, the better your model is. This is how ROC-AUC can help us judge the performance of our classification models as well as provide us a means to select one model from many classification models.
  
    If AUC = 0 then its Terrible and AUC = 1 for Best model.
  
    Main drawback of this is that **for imbalanced data AUC can be very high and it will not depends on  label predicted probability scores** , it just functions based on Ordering of the scores. So because of that two models with **same score order** will give same AUC but But one model can be better than other with respect to scores.
  
  - ##### PR Curve：不平衡数据集
  
    当测试集上存在大量的负样本时，ROC-AUC并不能真实地反映模型的性能，此时可以选择使用 Precision-Recall curve，做法仍然是移动分类阈值得到一系列坐标点，**横坐标表示Recall，纵坐标表示Precision**.
  
  - 情景：
  
    两个模型给出预测结果的同时，还附带给出概率得分，如果两个模型的准确率是相同的，且其中一个的概率得分普遍比另一个高，则这个模型是比较好的。
  
    如果一个模型的性能指标高于另一个模型，可以说该模型更好，而如果一个模型的性能指标与另一个模型相同，比如具有相同的AUC值，或是F1 Score ，如果模型可以输出概率得分的话，此时概率得分较高的那个模型更好。

- #### Coding

  - [实现讲解](https://blog.floydhub.com/a-pirates-guide-to-accuracy-precision-recall-and-other-scores/)

  - 混淆矩阵

    ```python
    from sklearn.metrics import confusion_matrix
    import numpy as np
    
    y_true = [0, 1, 2, 0, 1, 2, 2, 2, 1, 1]
    y_pred = [0, 2, 1, 0, 0, 1, 2, 1, 2, 0]
    
    
    def get_num(y_true, y_pred, row_num, col_num):
        return len([t for t, p in zip(y_true, y_pred) if p == col_num and t == row_num])
    
    
    def get_confusion_matrix(y_true, y_pred, n_labels=None):
        if n_labels is None:
            n_labels = len(set(y_true))
        confusion_matrix = np.zeros((n_labels, n_labels), dtype=np.int32)
        for i in range(n_labels):
            for j in range(n_labels):
                confusion_matrix[i, j] = get_num(y_true, y_pred, i, j)
        return confusion_matrix
    
    
    print(get_confusion_matrix(y_true, y_pred))
    print(confusion_matrix(y_true, y_pred))
    
    """
    [[2 0 0]
     [2 0 2]
     [0 3 1]]
    [[2 0 0]
     [2 0 2]
     [0 3 1]]
    """
    ```

    

  - 准确率

    ```python
    def my_accuracy_score(y_true, y_pred):
        cm = get_confusion_matrix(y_true, y_pred)
        return np.sum(np.diag(cm)) / np.sum(cm, axis=(0, 1))
    
    print(accuracy_score(y_true, y_pred))
    print(my_accuracy_score(y_true, y_pred))
    
    """
    输出：
    0.3
    0.3
    """
    ```

    

  - Precision、Recall、f1 score

    ```python
    def my_precision_score(y_true, y_pred):
        cm = get_confusion_matrix(y_true, y_pred)
        denominator = np.sum(cm, axis=0)
        denominator = np.where(denominator == 0, 1, denominator)
        return np.diag(cm) / denominator # 预测结果作为分母，对列求和
    
    
    print(precision_score(y_true, y_pred, average=None))
    print(my_precision_score(y_true, y_pred))
    
    
    def my_recall_score(y_true, y_pred):
        cm = get_confusion_matrix(y_true, y_pred)
        denominator = np.sum(cm, axis=1)
        denominator = np.where(denominator == 0, 1, denominator)
        return np.diag(cm) / denominator  # 真实结果作为分母，对行求和
    
    
    print(recall_score(y_true, y_pred, average=None))
    print(my_recall_score(y_true, y_pred))
    
    
    def my_f1_score(y_true, y_pred):
        cm = get_confusion_matrix(y_true, y_pred)
        recall_scores = np.diag(cm) / np.sum(cm, axis=1)
        precision_scores = np.diag(cm) / np.sum(cm, axis=0)
        denominator = recall_scores + precision_scores
        # 除法中分母不可取0，如果某项元素的P+R=0，则P=R=0，因此既需要将分母中的所有o替换为1。
        denominator = np.where(denominator == 0, 1, denominator)
        return (2 * recall_scores * precision_scores) / denominator
    
    
    print(f1_score(y_true, y_pred, average=None))
    print(my_f1_score(y_true, y_pred))
    
    
    """
    输出：
    Precision：
    [0.5        0.         0.33333333]
    [0.5        0.         0.33333333]
    Recall:
    [1.   0.   0.25]
    [1.   0.   0.25]
    f1 score
    [0.66666667 0.         0.28571429]
    [0.66666667 0.         0.28571429]
    """
    
    ```

    



