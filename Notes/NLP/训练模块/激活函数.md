激活函数作用：网络只是线性模型，只能把输入**线性组合**再输出，不能学到复杂的映射关系，所以用激活函数做非线性的转换。

#### 阶跃函数

- 公式
  $$
  sgn(x) =
  \begin{cases}
  1, & if\ x\geq0; \\\\\
  0, & if\ x<0.
  \end{cases}
  $$
  

#### Sigmoid activation function

- 计算公式
  $$
  f(x) =\sigma(x)= \frac{1}{1+e^{-x}}
  $$

- 存在的问题
  - 梯度消失问题：
    - The saturated neurons can kill gradients if we’re too positive or too negative of an input.
    
      较大的正数或较小的负数会导致梯度值为0，导致梯度无法进行传播。
    
       **sigmoid neurons stop learning when they saturate, i.e., when their output is near either 0 or 1**
    
  - 低效梯度更新：The second problem is that the sigmoid **output is not zero-centered**.
    
    神经元激活值不是以零为中心的，也就是不是有正有负的，就会出现所有权重同升同降的情况。
    
    - They’re also not zero-centered and so we get these, this inefficient kind of gradient update.（它们也不是零中心的，所以我们得到了这些，这种低效的梯度更新。）
    
    令 $δ^{l+1}_j=∂C/∂Z^{l+1}_j$，对于第$l+1$层第 $j$ 个神经元的其中一个权重$w^{l+1}_{jk}$，$Z^{l+1}_j=\sum_{k}w^{l+1}_{jk}·a^l_k + b^{l+1}_j$则有
    $$
    \frac{∂C}{∂w^{l+1}_{jk}} = \frac{∂C}{∂Z^{l+1}_j} ·\frac{∂Z^{l+1}_j}{∂w^{l+1}_{jk}}=δ^{l+1}_j·a^l_k
    $$
    由于 $a^l_k\geq0$，则梯度值${∂C}/{∂w^{l+1}_{jk}}$的符号都与 $δ^{l+1}_j$ 保持一致（不论k=1,2,3,…），若其大于0，则权重的梯度值为正，这意味着该神经元的所有权重都会在权重更新时下降，若其小于0，则权重的梯度值为负，这意味着该神经元的所有权重都会在权重更新时上升，则**同⼀的神经元的所有权重都会⼀起增加或者⼀起减少。**正是由于sigmoid函数大于等于0的性质，导致了这一现象的出现。如果是tanh函数，就不会出现这种情况，即便 $δ^{l+1}_j>0$, $a^l_k$ 是正是负尚未确定，则梯度值也是不确定的，因此不太可能出现同一所有神经元的所有权重同时上升或下降的现象。
    
    

#### Softmax activation function

- 计算公式
  $$
  softmax(z_i)= \frac{e^{z_i}}{\sum^K_{k=1}e^{z_k}},for \   i=1,2,…,K
  $$
  

#### TanH activation function

- 计算公式
  $$
  f(x)= tanh(x)=\frac {e^x-e^{-x}}{e^x+e^{-x}} = \frac {2e^x}{e^x+e^{-x}} -1=\frac {2}{1+e^{-2x}} -1= 2\sigma(2x)-1
  $$
  the tanh output is zero-centered.解决了sigmoid函数存在的第二个问题。

- 存在的问题

  - 梯度消失问题（函数值饱和区域，学习缓慢的问题）

    

#### ReLU activation function

- 计算公式
  $$
  f(x) = relu(x)= max(0,x)
  $$

- 存在问题
  - the relu output is not zero-centered（一个神经元的权重同升同降的问题）.
  - 虽然不存在像sigmoid、tanh函数饱和时学习缓慢的问题，但是当relu函数输入为负值时，神经元的权重停止了学习。

#### Leaky ReLU

- 公式
  $$
  LeakyReLU(x) = max(0, x) + γ min(0, x)=\begin{cases}
  x, & if\ x>0; \\\\\
  γ x, & if\ x\leq0.
  \end{cases}
  $$
  

#### PRelu（Parametric ReLU，PReLU）

- 公式：

  引入一个可学习的参数，不同神经元可以有不同的参数。对于第 *i* 个神经元，其 PReLU 的定义为
  $$
  PReLU(x) = max(0, x) + γ_i min(0, x)=\begin{cases}
  x, & if\ x>0; \\\\\
  γ_i x, & if\ x\leq0.
  \end{cases}
  $$
  其中 $γ_i $ 为*x* *≤* 0时函数的斜率。因此，PReLU是非饱和函数。如果 $γ_i $  = 0，那么PReLU就退化为ReLU。如果为$γ_i $ 一个很小的常数，则PReLU可以看作带泄露的ReLU。PReLU可以允许不同神经元具有不同的参数，也可以一组神经元共享一个参数。

#### ELU（Exponential Linear Unit，指数线性单元）

- 公式
  $$
  ELU(x)=max(0, x) + min(0, γ( e^x - 1)) =\begin{cases}
  x, & if\ x>0; \\\\\
  γ( e^x - 1), & if\ x\leq0.
  \end{cases}
  $$
  其中 $γ $ *≥* 0是一个超参数，决定*x* *≤* 0时的饱和曲线，并调整输出均值在0附近。 

#### **Softplus** 函数

- 公式
  $$
  Softplus(x) = log(1 + exp(x))
  $$
  

#### Swish 函数

- 计算公式
  $$
  swish(x) = x\odot \sigma(\beta x)
  $$
  其中*σ*(*·*)为Logistic函数，*β* 为可学习的参数或一个固定超参数。*σ*(*·*) *∈* (0*,* 1)可以看作是一种软性的门控机制。当*σ*(*βx*)接近于1时，门处于“开”状态，激活函数的输出近似于*x*本身；当*σ*(*βx*)接近于0时，门的状态为“关”，激活函数的输出近似于0。 

#### GELU函数

- 名称：Gaussian Error Linear Unit，高斯误差线性单元

- 公式
  $$
  \rm GELU(x) = x\odot \Phi(x) = xP(X ≤ x),
  $$
  和Swish函数比较类似，也是一种通过门控机制来调整其输出值的激活函数。其中*P*(*X* *≤* *x*)是高斯分布*N* (*µ, σ*2)的累积分布函数，其中*µ, σ* 为超参数，一般设*µ* = 0*, σ* = 1即可。

#### GLU

Gated Linear Unit
$$
\rm GLU(x) =\sigma( Wx+b) \odot (Vx+c)
$$
以下激活函数都是在将其他激活函数（GeLU、Swish）与GLU组合得到的。[公式来源](https://arxiv.org/pdf/2002.05202.pdf)

#### GEGLU

$$
\rm GEGLU = \rm GELU(Wx+b) \odot (Vx+c)
$$

#### SwiGLU

SwiGLU (Swish-Gated Linear Unit) is a novel activation function that combines the advantages of the Swish activation function and the GLU（Gated Linear Unit）.
$$
\rm SwiGLU(x) = Swish(Wx+b) \odot (Vx+c)
$$

