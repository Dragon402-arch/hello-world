### 1. Cross-entropy

The **cross-entropy** as the **Log Loss function** (not the same but they measure the same thing) **computes the difference between two probability distribution functions.**

- Binary cross-entropy: for binary classification problem
- Categorical cross-entropy: binary and multiclass problem, the label needs to be encoded as categorical, one-hot encoding representation (for 3 classes: [0, 1, 0], [1,0,0]…)
- Sparse cross-entropy: binary and multiclass problem (the label is an integer — 0 or 1 or … n, depends on the number of labels)

$$
L_{CE} = -\frac{1}{N}\sum_{j=1}^N\sum_{i=1}^Cq_ilog(p_i)
$$

交叉熵损失(cross-entropy Loss) 又称为对数似然损失(Log-likelihood Loss)、对数损失；二分类时还可称之为逻辑斯谛回归损失(Logistic Loss)。
$$
loss(x,class) = -weight[class]·log \bigg(\frac{exp(x[class]}{\sum_iexp(x_i)}) \bigg) =weight[class]·\bigg ( -x[class]+-log \bigg(\sum_iexp(x_i) \bigg)\bigg ) 
$$

### 2. Log-Loss

实现一个负对数似然到交叉熵损失的等价推导。

-  negative log-likelihood Loss，NLL Loss
-  cross-entropy
- maximum likelihood estimation （MLE），极大似然估计

logistic regression 算法使用了对数损失函数 。

> Negative log-likelihood for binary classification problems is often shortened to simply “log loss” as the loss function derived for logistic regression.
>
> \- log loss = negative log-likelihood, under a Bernoulli probability distribution
>
> For classification problems, “*log loss*“, “*cross-entropy*” and “*negative log-likelihood*” are used interchangeably.
>
> More generally, the terms “*cross-entropy*” and “*negative log-likelihood*” are used interchangeably in the context of loss functions for classification models.

Logistic Loss：
$$
L(y_i,\hat y_i) = y_i ln(1+e^{-\hat y_i}) + (1-\hat y_i) ln(1+e^{\hat y_i})
$$


### 3. Exponential Loss

AdaBoost 算法使用了指数损失函数，是一种二分类损失函数。
$$
L(y,f(x)) = exp^{\large -y\ ·f(x)}
$$
指数损失函数对异常值比较敏感，因此需要注意数据的预处理和模型的选择。

```python
import numpy as np
 
def exponential_loss(y_true, y_pred):
    """
    计算指数损失函数
    :param y_true: 真实标签，形状为(n_samples, )
    :param y_pred: 模型预测概率，形状为(n_samples, )
    :return: 指数损失值，标量
    """
    loss = np.exp(-y_true * y_pred)
    #  array([1.        , 0.54881164, 1.        , 0.54881164])
    return np.mean(loss)

```



### 4. Hinge Loss

SVM 算法使用了Hinge 损失函数
$$
L(y,f(x)) = max(0,1-y·f(x))
$$

```python
import numpy as np
 
def hinge_loss(y_true, y_pred):
    # 计算每个样本的Hinge损失
    loss = np.maximum(0, 1 - y_true * y_pred)
    # 计算所有样本的平均损失
    avg_loss = np.mean(loss)
    return avg_loss
```

**正是因为HingeLoss的零区域对应的正是非支持向量的普通样本，从而所有的普通样本都不参与最终超平面的决定，这才是支持向量机最大的优势所在，对训练样本数目的依赖大大减少，而且提高了训练效率。**

### 5. KL Divergence Loss

KL 散度损失函数用于表达两个概率分布之间的差异。
$$
KL(p||q) = \sum_{i=1}^np(x_i)log\frac{p(x_i)}{q(x_i)} = \int_{-\infty}^{+\infty}p(x)log\bigg(\frac{p(x)}{q(x)}\bigg)dx
$$
其中 $||$ 意味着 p 和 q 不能互换位置，也就是  $KL(p||q)\ !=KL(q||p)$ ，不具有对称性。p 表示真实分布，q表示拟合分布，$KL(p||q)$ 表示当用概率分布 q 来拟合真实分布 p 时，产生的信息损失。损失越低，拟合分布越接近真实分布。

当 p 和 q 是离散随机变量的概率密度函数时，相对熵可以定义为求和形式。当 p 和 q 是连续随机变量的概率密度函数时，相对熵可以定义为积分形式。

```python
def kl_divergence(y_true, y_pred):
    return y_true * np.log(y_true / y_pred)
```

KL 散度通常被称为相对熵（Relative Entropy）。从信息论角度观察三者，其关系为**信息熵 = 交叉熵 - 相对熵**。

$$
KL(p||q) = \sum_{i=1}^np(x_i)log\frac{p(x_i)}{q(x_i)}  =  -\sum_{i=1}^np(x_i)log\ q(x_i)  -\sum_{i=1}^np(x_i)log\ p(x_i) = E(p,q) - E(p)
$$
其中 $E(p,q)$  表示 p 和 q 的交叉熵，$E(p)$ 表示 p 的信息熵。

信息熵（Information Entropy）的定义为：
$$
Ent(p) = -\sum_{i=1}^np(x_i)log \ p(x_i)
$$
交叉熵、相对熵、信息熵（经验熵）中的对数的底数通常取2、e、10，上面的式子表示以 e 为底。

### 6. MSE

Mean Squared Error，均方误差
$$
L(y,\hat y) = \frac{1}{n}\sum_{i=1}^n(y_i-\hat y)^2
$$

### 7. MAE

MAE（Mean Absolute Error），称为平均绝对误差，也即是L1 Loss，用来目标变量和预测变量之间绝对差值之和，因此它衡量的是一组预测值中的平均误差大小，而不考虑它们的方向。
$$
L(y,\hat y) = \frac{1}{n}\sum_{i=1}^n|y_i-\hat y|
$$
### 8.Smooth L1 Loss

原始形式：
$$
SmoothL_1(x) =
\begin{cases}
0.5x^2, & if \ |x| <1 \\\\\
|x|-0.5, & otherwise
\end{cases}
$$
计算形式：Torch使用
$$
SmoothL_1(x) =
\begin{cases}
\frac{1}{\large 2}(y-\hat y)^2/\beta, & if \ |y-\hat y| < \beta \\\\\
|y-\hat y|-\large \frac{\beta}{2}, & otherwise
\end{cases}
$$

### 9. Huber Loss

Huber Loss is **a combination of MAE and MSE** (L1-L2) but it depends on an additional parameter call delta that influences the shape of the loss function. This parameter needs to be fine-tuned by the algorithm. 
$$
HuberLoss(y,\hat y) =
\begin{cases}
\frac{1}{2}(y-\hat y)^2, & if \ |y_i-\hat y| < \delta \\\\\
 \delta·(|y-\hat y|-\frac{\delta}{2}), & otherwise
\end{cases}
$$
What this equation essentially says is: **for loss values less than delta, use the MSE; for loss values greater than delta, use the MAE.** 



Huber Loss 和 Smooth L1 Loss 比较

当 $\beta = 1 $ 时，两者完全等价。 

> [来源](https://detectron2.readthedocs.io/en/latest/_modules/fvcore/nn/smooth_l1_loss.html)
>
> **Smooth L1 loss is equal to huber(x) / beta.** This leads to the following differences: 
>
> - As beta -> 0, Smooth L1 loss converges to L1 loss, while Huber loss  converges to a constant 0 loss.
> - As beta -> +inf, Smooth L1 converges to a constant 0 loss, while Huber loss converges to L2 loss.    
> - For Smooth L1 loss, as beta varies, the L1 segment of the loss has a constant slope（斜率） of 1. For Huber loss, the slope of the L1 segment is beta.

Smooth L1 loss可以看作是L1 loss的一种变形，在$|y-\hat y|=\beta$ 时，损失函数在此处的导数为1，只有当 $ |y-\hat y|<\beta$ 时，L1 Loss 用一个二次函数来替换.这个二次函数段使得L1 loss 在$|y-\hat y|=0$附近更加平滑。

### 10. Quantile Loss

Quantile Loss，称为分位数损失函数
$$
L(y,\hat y) =
\begin{cases}
(1-\alpha)|y-\hat y| , & if \ |y-\hat y| \le 0 \\\\\
 \alpha|y-\hat y|, & if\ |y_i-\hat y| > 0
\end{cases}
$$
 其中 $\alpha$ 为分位数，需要在进行回归前指定，该损失函数对异常值具有较好的鲁棒性。

### 11. Focal Loss

类别不平衡问题可以使用 Focal Loss（焦点损失）。Huber loss是借由针对outlier (hard example)进行 down-weighting，因此对于outlier，损失函数具有鲁棒性。而Focal loss是希望针对 inliers(easy example进行 down-weighting，因为 Focal loss希望的是在训练过程中能尽量去训练hard example，忽略那些 easy example。

**Focal loss is used to address the issue of the class imbalance problem. A modulation term applied to the Cross-Entropy loss function, make it efficient and easy to focus learning on hard misclassified examples.** It is a dynamically scaled cross entropy loss, where the scaling factor decays to zero as confidence in the correct class increases.


负对数似然的交叉熵形式：
$$
CE(p_t) = -log(p_t)
$$
样本加权交叉熵损失形式：
$$
CE(p_t) = -\alpha_t·log(p_t)
$$
焦点损失形式：
$$
FL(p_t) =\large -\alpha_t·(1-p_t)^{\gamma}·log(p_t)
$$
其中 $\gamma \ge 0$，$(1-p_t)^{\gamma}$  是一个调整因子，

When an example is misclassified and pt is small, the modulating factor is near 1 and the loss is unaffected. As pt → 1, the factor goes to 0 and the loss for well-classified examples is down-weighted.



