总结

[代码实现](https://github.com/bentrevett/a-tour-of-pytorch-optimizers/tree/main)

| Method                 | Formula                                                      | Definitions                                                  |
| ---------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| Learning Rate          | $w^{(t+1)}=w^{(t)} -\eta·∇_{\Large w^{(t)}}$                 | $∇_{\Large w^{(t)}}=≡ ∇ℓ(w^{(t )} , z)$                      |
| Adaptive Learning Rate | $w^{(t+1)}=w^{(t)} - η_t·∇_{\Large w^{(t)}}$                 |                                                              |
| Momentum               | $w^{(t+1)}=w^{(t)} +\mu·(w^{(t)}-w^{(t-1)})- η·∇_{\Large w^{(t)}}$ |                                                              |
| Nesterov Momentum      | $w^{(t+1)}=w^{(t)} + v^{(t)}$                                | $v_{t+1} = \mu · v_t − η · ∇ℓ(w^{(t )} − \mu· v_t , z)$      |
| AdaGrad                | $w^{(t+1)}_i=w^{(t)}_i - \Large\frac{\large \eta \ · ∇w^{(t)}_{i}} {\sqrt{\Large A_{i,t}}+ \large \varepsilon}$ | $A_{\large i,t} = \sum_{τ =0}^{t} (∇_{\Large w_i}^{(τ)})^2$  |
| RMSProp                | $w^{(t+1)}_i=w^{(t)}_i - \Large\frac{\large \eta \ · ∇w^{(t)}_{i}} {\sqrt{\Large A_{\large i,t}^`}+ \large \varepsilon}$ | $A_{\large i,t}^` = \beta · A_{\large t-1 }^` + (1-\beta)(∇_{\Large w_i}^{(t)})^2$ |
| Adam                   | $w^{(t+1)}_i=w^{(t)}_i - \Large\frac{\large \eta \ · M^{(\large 1)}_{\Large i,t}}{\sqrt{\Large M^{(2)}_{i,t}}+ \large \varepsilon}$ | $M_{\large i,t}^{(m)} = \Large \frac{\beta_m · M_{\large i, t-1 }^{(m)} + (1-\beta_m)(∇_{\Large w_i}^{(t)})^m}{1-\beta_m}$ |



#### SGD

- 缺点
  - 对于初始化参数比较敏感
  - 容易陷入局部极小值
  - 数据较多时，训练时间较长

#### 动量梯度下降

##### Momentum

Momentum method is a technique that can accelerate gradient descent by taking accounts of **previous gradients** in the update rule at each iteration.
$$
\theta_{t+1} = \theta_{t} + v_{t+1} \\
v_{t+1} = \alpha v_t - \eta \ · g_t \\
g_t = \frac{1}{n}\sum_{i=1}^n ∇_ \theta L(x^{(i)},y^{(i)},\theta_t)
$$
也即是
$$
\begin{aligned}  \theta_{t+1} &= \theta_{t} - \eta \ · g_t + \alpha·v_{t} \\
&=\theta_{t} - \eta \ · g_t -\alpha ·\eta \ · g_{t-1}  + \alpha^2 v_{t-1} \\
 &=  \theta_{t}- \eta \ · g_t - \alpha ·\eta \ · g_{t-1} - \alpha^2 \eta \ · g_{t-2}-…-\alpha^{t-1} \eta \ · g_{1}  \\
 &=\theta_{t}- \eta \ · g_t + \alpha · (\theta_{t}-\theta_{t-1})
 \end {aligned}
$$


##### Nesterov Accelerated Gradient (NAG)

对Momentum的一种改进：先对参数进行估计，然后使用估计后的参数来计算误差。

[讲解](https://medium.com/konvergen/momentum-method-and-nesterov-accelerated-gradient-487ba776c987)
$$
\tilde \theta_t = \theta_t + \alpha·v_{t}  \\
g_t = \frac{1}{n}\sum_{i=1}^n ∇_ \theta L(x^{(i)},y^{(i)},\tilde \theta_t)
$$ { }
![NAG](D:\Typora\Notes\NLP\概念与理论\NAG.png)

Nesterov Accelerated Gradient 相比于  Momentum 仅在迭代的梯度计算上存在差别，其余部分两者相同。

Both methods are distinct only when the learning rate *η* is reasonably large. When the learning rate *η* is relatively large, Nesterov Accelerated Gradients allows larger decay rate *α* than Momentum method, while preventing oscillations. The theorem also shows that both Momentum method and Nesterov Accelerated Gradient become equivalent when *η* is small.

仅当学习率 η 相当大时，两种方法才有区别。当学习速率 η 相对较大时，Nesterov Accelerated Gradients比 Momentum 方法允许更大的衰减率α，同时预防振荡。该定理还表明，当η很小时，两种方法等价。

![nesterov](D:\Typora\Notes\NLP\概念与理论\nesterov.jpeg)

NAG 在梯度计算时，提前对参数进行了更新，因此显示在图上的gradient step发生了变化，[图片来源](https://cs231n.github.io/neural-networks-3/#sgd)。

#### AdaGrad

[讲解](https://medium.com/konvergen/an-introduction-to-adagrad-f130ae871827)

Adaptive gradient
$$
\theta_{t+1} = \theta_{t} -  \frac {\eta}{\sqrt{\epsilon I+diag(G_t)}}\ · g_t \\
g_t = \frac{1}{n}\sum_{i=1}^n ∇_ \theta L(x^{(i)},y^{(i)},\theta_t) \\
G_t = \sum_{k=1}^t g_k·g_k^T \\
\large G_t^{(i,i)} = \large \sum_{k=1}^t (g_k^{(i)})^2
$$
![优化器](D:\Typora\Notes\NLP\概念与理论\优化器.png)

公式注解：$g_t$ 向量表示在第 $t$ 次迭代时损失值关于所有参数 $\theta$  （一个向量）的梯度，$ g_k\ ·g_k^T$ 则表示某次迭代的梯度向量与其自身的外积，其结果是一个矩阵，求和 $ \sum_{k=1}^t $ 则表示将所有迭代历史的梯度值平方后累计求和，其结果仍然是一个矩阵，理解了这一点，也就理解了第四个公式的由来。于是有更新规则：
$$
\begin{aligned}  \theta_{t+1}^{(i)}  &= \theta_{t}^{(i)} -  \frac {\eta}{\sqrt{\epsilon +G_t^{(i,i)}}}\ · g_t^{(i)} \\ 
 &= \theta_{t}^{(i)} -  \frac {\eta}{\sqrt{\epsilon +\sum_{k=1}^t (g_k^{(i)})^2}}\ · g_t^{(i)}
 \end {aligned}
$$

其中  $ g_t^{(i)}$ 表示参数 $\theta_{t}^{(i)}$ 在第  $t$ 次迭代的梯度值。 

将每一维各自的历史梯度的平方叠加起来，然后更新的时候除以叠加的结果，这样每个参数的学习率就与其梯度有关系了，那么每个参数的学习率也就不一样了，也就是所谓的自适应学习率。（the effective learning rate decreased very fast because of the accumulation of squared gradients from the very beginning of training）

- 缺点：
  - 容易受到过去梯度的影响，导致学习率下降很快，能学到更多知识的能力也越来越弱，就会提前停止学习。
  - 如果初始梯度值较大，有效学习率就会很小，这可以通过设置学习率超参数来缓解，使初始梯度不会对剩余训练的有效学习率产生太大的影响。这使得Adagrad算法对学习率的选择很敏感，违背了算法的目的。


#### Adadelta

[讲解](https://medium.com/konvergen/continuing-on-adaptive-method-adadelta-and-rmsprop-1ff2c6029133)

Adadelta 在AdaGrad算法的基础上进行了优化。不需要初始学习率，并且对超参数不敏感。	

This algorithm was built by following two main ideas, i.e., accumulating the squared gradients over moving windows, and correcting the unit of parameter updates with hessian approximations. 

the accumulation as an **exponentially decaying average of the squared gradients**

在AdaGrad中，梯度平方累计为：
$$
\large G_t^{(i,i)} = \large \sum_{k=1}^t (g_k^{(i)})^2
$$
在Adadelata中，梯度平方累计为：
$$
\begin{aligned}  \large G_t^{(i,i)} &= \rho G_{t-1}^{(i,i)} +(1-\rho)(g_t^{(i)})^2 \\
 \large &= (1-\rho)(g_t^{(i)})^2 +  \rho(1-\rho)(g_{t-1}^{(i)})^2 + \rho^2G_{t-2}^{(i,i)} \\
 \large &=(1-\rho)(g_t^{(i)})^2 +  \rho(1-\rho)(g_{t-1}^{(i)})^2 +  \rho^2(1-\rho)(g_{t-2}^{(i)})^2 +…+\rho^{t-1}(1-\rho)(g_{1}^{(i)})^2
\end {aligned}
$$
其中 $\rho$ 为衰减常数，衰减常数也可以看作是控制移动窗口大小的超参数。

算法原始公式：
$$
E[g^2]_t =\rho E[g^2]_{t-1} + (1- \rho) g_t^2 \\
RMS[g]_t = \sqrt {E[g^2]_t +\epsilon }  \\
\theta_{t+1}^{(i)}  = \theta_{t}^{(i)} -  \frac {\eta}{RMS[g^{(i)}]_t}\ · g_t^{(i)} =\theta_{t}^{(i)} -  \frac {\eta}{\sqrt{\epsilon +G_t^{(i,i)}}}\ · g_t^{(i)}
$$
如果使用以上更新规则，那么对应的就是 RMSProp算法。然而，使用该算法，仍然需要选择初始学习率，并存在单位不匹配（the unit mismatch）问题，由此提出了使用Hessian近似值校正参数更新单元的解决方案。

Correcting the unit of parameter updates ：
$$
E[ (\Delta\theta)^2]_t =\rho E[(\Delta\theta)^2]_{t-1} + (1- \rho) (\Delta\theta_t)^2 \\
RMS[\Delta\theta]_t = \sqrt {E[{(\Delta\theta)}^2]_t +\epsilon }  \\
\theta_{t+1}^{(i)}  = \large \theta_{t}^{(i)} -  \frac {RMS[\Delta\theta^{(i)}]_{t-1}}{RMS[g^{(i)}]_t}\ · g_t^{(i)}
$$
其中：
$$
\begin{aligned}   E[ (\Delta\theta)^2]_t &=\rho E[(\Delta\theta)^2]_{t-1} + (1- \rho) (\Delta\theta_t)^2 \\
&= (1- \rho) (\Delta\theta_{t})^2 + \rho(1- \rho)(\Delta\theta_{t-1})^2+…+\rho^{t-1}(1- \rho)(\Delta\theta_{1})^2

\end {aligned}
$$


#### RMSProp

Root Mean square prop，在AdaGrad算法的基础上进行了优化。

在自适应梯度的基础上引入了衰减因子，在梯度累计的时候会对过去与现在做一个平衡，通过超参数调节衰减量，适合处理非平稳目标（也就是与时间相关的），对于RNN效果很好。

核心公式：
$$
\begin{aligned}  \large G_t^{(i,i)} &= \rho G_{t-1}^{(i,i)} +(1-\rho)(g_t^{(i)})^2 \\
 \large &= (1-\rho)(g_t^{(i)})^2 +  \rho(1-\rho)(g_{t-1}^{(i)})^2 + \rho^2G_{t-2}^{(i,i)} \\
 \large &=(1-\rho)(g_t^{(i)})^2 +  \rho(1-\rho)(g_{t-1}^{(i)})^2 +  \rho^2(1-\rho)(g_{t-2}^{(i)})^2 +…+\rho^{t-1}(1-\rho)(g_{1}^{(i)})^2
\end {aligned}
$$
其中 $\rho$ 为衰减常数，衰减常数也可以看作是控制移动窗口大小的超参数。

参数更新规则为：
$$
\theta_{t+1}^{(i)}  = \theta_{t}^{(i)} -  \frac {\eta}{\sqrt{\epsilon +G_t^{(i,i)}}}\ · g_t^{(i)}
$$


#### Adam

- 相关链接
  - [公式推导1](https://towardsdatascience.com/adam-latest-trends-in-deep-learning-optimization-6be9a291375c)
  - [公式推导2](https://blog.csdn.net/BeiErGeLaiDe/article/details/126059488)
  - [原论文](https://arxiv.org/pdf/1412.6980.pdf)
  - [讲解](https://medium.com/konvergen/accelerating-the-adaptive-methods-rmsprop-momentum-and-adam-aa5249ef1e78)

Adaptive Moment Estimation，自适应矩估计，结合了 RMSProp 和 Momentum 的优点，适用于大数据集和高维空间。

Adam was designed to combine the advantages of Adagrad, which works well with sparse gradients, and RMSprop, which works well in on-line settings. Having both of these enables us to use Adam for broader range of tasks. **Adam can also be looked at as the combination of RMSprop and SGD with momentum.**

To estimate the first-order moment, i.e., the mean, we use the moving averages of gradients. In the other hand, to estimate the second-order raw moments, i.e., the uncentered variance, we use the moving averages of squared gradients.

为了估计一阶矩，即均值，我们使用梯度的移动平均。另一方面，为了估计二阶原始矩，即无中心方差，我们使用梯度平方的移动平均。
$$
m_t = \beta_1 · m_{t-1} + (1-\beta_1) · g_t \\
v_t = \beta_2 · v_{t-1} + (1-\beta_2) · g_t^2 \\

\hat m_t = \frac{m_t}{1-\beta_1^t} \\
\hat v_t = \frac{v_t}{1-\beta_2^t} \\

\theta_t = \theta_{t-1}- \frac{\alpha · \hat m_t}{\sqrt{\hat v_t +\epsilon}}
$$
其中 $m_t$ 是梯度在动量形式下的一阶矩估计，$v_t$ 是梯度在动量形式下的二阶矩估计， $\hat m_t$ 是偏差纠正后的一阶矩估计，$\hat v_t$ 是偏差纠正后的二阶矩估计。

公式变形：
$$
\large m_t = \beta_1 · m_{t-1} + (1-\beta_1) · g_t =  (1-\beta_1) \sum_{i=1}^t\beta_1^{\ t-i}·g_i  \\
\large v_t = \beta_2 · v_{t-1} + (1-\beta_2) · g_t^2  = (1-\beta_2) \sum_{i=1}^t\beta_2^{\ t-i}·g_i^2 \\
$$
公式推导注解：
$$
\begin{aligned}  E\big[v_t\big] &= E\bigg[(1-\beta_2) \sum_{i=1}^t\beta_2^{\ t-i}·g_i^2\bigg]\\
&=E\big[g_t^2\big]·(1-\beta_2) \sum_{i=1}^t\beta_2^{\ t-i} + \xi  \\
&= E\big[g_t^2\big]·(1-\beta_2)\frac{(1-\beta_2^{\ t})}{(1-\beta_2)} + \xi \\
&= E\big[g_t^2\big]·(1-\beta_2^{\ t}) + \xi
\end {aligned}
$$
补充：由于用 $g_t$ 近似表示 $g_i$，因此可以将其从求和中取出。同时由于使用了近似表示，误差项 $\xi$ 就出现在了公式中。（来自公式推导1）

从而使得：（无偏估计）
$$
E\big[\hat v_t\big] =\frac{E\big[v_t\big]}{1-\beta_2^t} = E\big[g_t^2\big]
$$
where $\xi=0$  if the true second moment $E[g^2_i ]$ is stationary; otherwise  $\xi$ can be kept small since the exponential decay rate $\beta_1$ can (and should) be chosen such that the exponential moving average assigns small weights to gradients too far in the past.（出自原论文）

如果 $E[g^2_i ]$  是一个常数，则  $\xi=0$ ；反之，由于选择的指数衰减率 $\beta_1$ 将会为指数移动平均中过去太远的梯度分配较小的权重，从而使得误差项   $\xi$  是个较小的值。



#### AdamW

Adam + Weight Decay 

![AdamW](D:\Typora\Notes\NLP\概念与理论\AdamW.png)

粉色部分表示 Adam + L2 正则化执行的权重更新，而 绿色部分表示 Adam + 权重衰减的执行的权重更新。

