- 交叉熵

  - [讲解](https://towardsdatascience.com/cross-entropy-loss-function-f38c4ec8643e)

  - 定义：

    For `p(x)` — probability distribution and a random variable X, entropy is defined as follows
    $$
    H(X) =
    \begin{cases}
    -\int_{x}p(x)logp(x), & if\ X\ is\ continous \\\\\
    -\sum_{x}p(x)logp(x), & if\ X\ is\ discrete
    \end{cases}
    $$
    The greater the value of entropy,H(x) , the greater the uncertainty for probability distribution and the smaller the value the less the uncertainty.

  - **Cross-Entropy Loss**

    - Cross-Entropy Loss
      $$
      L_{CE} = -\frac{1}{N}\sum_{j=1}^N\sum_{i=1}^Cq_ilog(p_i)
      $$

    - Binary Cross-Entropy Loss
      $$
      L_{CE} = -\frac{1}{N}\Bigg [\sum_{j=1}^N\Big[t_jlog(p_j)+(1-t_j)log(1-p_j)\Big]\Bigg ]
      $$
      For N data points  where $t_j$ is the truth value taking a value 0 or 1 and $p_j$ is the Softmax probablity for the `jth` data point.
      
      

- 交叉熵代码实现

  - Cross-Entropy Loss

    注意：PyTorch中nn.CrossEntropyLoss()的input（也就是模型的输出，是没有经softmax之前的输出 **the network’s output is assumed to be the vector just prior to applying the softmax function.**）；**而在nn.BCELoss()的input则是模型经过sigmoid之后的输出。**

    ```python
    import torch
    import torch.nn as nn
    # import torch.functional as F
    import torch.nn.functional as F
    
    input = torch.tensor(
        [[2.8883, 0.1760, 1.0774], 
        [1.1216, -0.0562, 0.0660], 
        [-1.3939, -0.0967, 0.5853]]
    )
    # the network’s output is assumed to be the vector just prior to applying the softmax function. 
    
    target = torch.tensor([1, 2, 0]) # 交叉熵损失内部会实现真实标签到独热编码表示。
    
    loss_func = nn.CrossEntropyLoss()
    
    loss1 = loss_func(input,target)
    loss2 = F.cross_entropy(input, target)
    print(loss1,loss2)
    
    """
    tensor(2.3185) tensor(2.3185)
    """
    
    ```

    - 负对数似然损失函数 Negative Log-Likelihood Loss（NLL）

      [讲解](https://medium.com/deeplearningmadeeasy/negative-log-likelihood-6bd79b55d8b6)

      最大似然估计（MLE）转化为 最小化负对数似然损失。

      ***Likelihood refers to the chances of some calculated parameters producing some known data.***

      The negative log likelihood loss is mostly used in classification problems, here Likelihood refers to the chances of some calculated parameters producing some known data.

      ```python
      import torch
      import torch.nn as nn
      
      
      input = torch.tensor(
          [[2.8883, 0.1760, 1.0774],
           [1.1216, -0.0562, 0.0660],
           [-1.3939, -0.0967, 0.5853]]
      )
      target = torch.tensor([1, 2, 0])
      
      log_softmax = nn.LogSoftmax(dim=1)
      
      
      def get_nll_loss(input, target):
          """基本实现"""
          total_loss = torch.tensor(0.0)
          for i, j in zip(input, target.long()):
              total_loss += i[j]
          return -total_loss / target.size(0)
      
      
      def get_nll_loss(input, targets):
          """优化实现"""
          # log_input[:, targets] 表示按照targets进行二维矩阵中列次序的重排。
          out = torch.diag(input[:, targets])  # 取对角线元素
          return -torch.mean(out)
      
      
      nll_loss_func = nn.NLLLoss()
      ce_loss_func = nn.CrossEntropyLoss()
      
      """计算交叉熵损失"""
      print(get_nll_loss(input, target))
      print(nll_loss_func(log_softmax(input), target))
      print(ce_loss_func(log_softmax(input), target))
      
      """
      tensor(2.3185)
      tensor(2.3185)
      tensor(2.3185)
      """
      ```
      
      $$
      \text{loss}(x, class) = -\log\left(\frac{\exp(x[class])}{\sum_j \exp(x[j])}\right)
                             = -x[class] + \log\left(\sum_j \exp(x[j])\right)
      $$
      
      $$
      \ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad
              l_n = - w_{y_n} x_{n,y_n}, \quad
              w_{c} = \text{weight}[c] \cdot \mathbb{1}\{c \not= \text{ignore\_index}\}
      $$
      
      
      
    -  CrossEntropyLoss = LogSoftmax + NLLLoss.
    
      ```python
      import torch
      import torch.nn as nn
      import torch.nn.functional as F
      
      logits = torch.tensor(
          [[2.8883, 0.1760, 1.0774], 
          [1.1216, -0.0562, 0.0660], 
          [-1.3939, -0.0967, 0.5853]]
      ) # 模型输出，没有经过激活函数之前的输出。
      target = torch.tensor([1, 2, 0])
      
      def log_softmax(input):
          return torch.log(torch.softmax(input, dim=1))
      
      """
      等价操作：
      # 操作一：
      F.log_softmax()
      
      # 操作二
      log_softmax = nn.LogSoftmax(dim=1)
      """
      
      nll_loss_func = nn.NLLLoss()
      ce_loss_func = nn.CrossEntropyLoss()
      
      print(nll_loss_func(log_softmax(logits),target)) # 多分类老版本损失函数
      print(ce_loss_func(logits,target)) # 多分类新版本损失函数
      
      """
      tensor(2.3185)
      tensor(2.3185)
      """
      ```
    
      
    
  - Binary Cross-Entropy Loss
  
    ```python
    import torch
    import torch.nn as nn
    # import torch.functional as F
    import torch.nn.functional as F
    from math import log
    
    
    input = torch.tensor([[-0.8728], [0.3632], [-0.0547]])
    target = torch.tensor([0, 1, 0])
    
    # 方案一
    loss_func1 = nn.BCELoss()
    loss1 = loss_func1(torch.sigmoid(input).squeeze(-1), target.float())
    
    # 方案二
    loss_func2 = nn.BCEWithLogitsLoss()
    loss2 = loss_func2(input.squeeze(-1), target.float())
    
    # 方案三
    loss3 = F.binary_cross_entropy(torch.sigmoid(input).squeeze(-1), target.float())
    
    # 方案四
    def get_loss(input, target):
        loss = torch.tensor(0.0)
        input = torch.sigmoid(input).squeeze(-1)
        target = target.float()
        for true, pred in zip(target, input):  # 默认以e为底的对数
            temp = (true * log(pred) + (1 - true) * log(1 - pred))
            loss += temp
        loss /= -input.size(0)
        return loss
    
    # 方案五
    def get_loss(input, target):
        input = torch.sigmoid(input).squeeze(-1)
        target = target.float()
        loss = torch.sum(target*torch.log(input)+(1-target)*torch.log(1-input))
        loss /= -input.size(0)
        return loss
    
    
    
    loss4 = get_loss(input, target)
    print(loss1, loss2,loss3,loss4)
    
    """
    输出
    tensor(0.5144) tensor(0.5144) tensor(0.5144) tensor(0.5144)
    """
    ```
  
    注意二分类和多分类时 input输入维度上的差别。
  
  - 
  
- 

