### Parallel Training: 

------

[讲解1：分布式并行训练——数据并行与模型并行](https://towardsdatascience.com/distributed-parallel-training-data-parallelism-and-model-parallelism-ec2d234e3214)

[讲解2：分布式并行训练——模型并行](https://towardsdatascience.com/distributed-parallel-training-model-parallel-training-a768058aa02a)

There are two primary types of distributed parallel training: **data parallelism and model parallelism**. We further divide the latter into two subtypes: **pipeline parallelism and tensor parallelism**. It’s common to mix both model and data parallelism for large-scale models. For instance, the largest [T5](https://arxiv.org/abs/1910.10683) models and [GPT-3](https://arxiv.org/abs/2005.14165) employ a model and data combined parallelism.

**Distributed parallel training** has two high-level concepts: parallelism and distribution.

> Parallelism is a framework strategy to tackle the size of large models or improve training efficiency, and distribution is an infrastructure architecture to scale out. 并行是解决大型模型或提高训练效率的框架策略，分布式是一种基础设施架构，可以向外扩展。
>
>  **Parallelism is a framework strategy, and distribution is an infrastructure architecture.**

### data parallelism

Data parallelism shards data across all cores with the same model. A data parallelism framework like **PyTorch Distributed Data Parallel**, SageMaker Distributed, and Horovod mainly accomplishes the following three tasks:

1. First, it creates and dispatches copies of the model, one copy per each accelerator. 创建和分发模型的副本，每个设备上分发一个模型的副本。

2. It shards the data and then distributes it to the corresponding devices. 拆分数据并分发到对应的设备上。

3. It finally aggregates all results together in the backpropagation step. 在反向传播步骤将所有结果聚合到一起。

   示例代码

   ```python
   import torch
   import torch.nn as nn
   import torch.distributed as dist
   import torch.multiprocessing as mp
   from torch.nn.parallel import DistributedDataParallel as DDP
   
   ### Step 1: setup and cleanup setups
   def setup(rank, world_size):
       ...
       # initialize the process group
       dist.init_process_group("tst", rank=rank, world_size=world_size)
   
   def cleanup():
       dist.destroy_process_group()
       
   ### Step 2: define DDP modeling
   def dummy_init(rank, world_size):
       setup(rank, world_size)
       model = DummyModel().to(rank)
       ddp_model = DDP(model, device_ids=[rank])
       ...
       cleanup()
   ### Step 3: Spawn to run
   def run_dummy(dummy_fn, world_size):
       mp.spawn(dummy_fn,
                args=(world_size,),
                nprocs=world_size,
                join=True)
   ```

   - 缺陷：如果模型较大，而一个显卡GPU的显存无法放得下一个模型，此时该方法就行不通了。
   - 显存占用：Parameter + Gradient + Optimizer + Intermediate，模型计算的中间结果，在使用数据并行方法训练时，Intermediate部分占用的显存变为原来的几分之一了，而其余部分的显存占用不变。

### model parallelism

- 显存占用：Parameter + Gradient + Optimizer + Intermediate，每张显卡上只需保存模型 1/N 的参数，此时Parameter、Gradient、Optimizer 在每张显卡上占用的显存都变为原来的 1/N，而此时每张显卡上模型计算的中间结果占用的显存是不变的。

Model parallelism shards a model (i.e., its layers or tensors) across multiple cores, unlike data parallelism, replicating the same model for all training cores. PyTorch alleviates the parallel implementation and wraps it with minimal changes.

模型并行方案是指按照层或张量拆分整个模型在多个设备（GPU）上，与数据并行不同（在每个设备上都复制一份模型），PyTorch 减轻了并行实现，并以最小的更改封装它。

**Model parallelism shards a model across multiple GPUs, unlike data parallelism replicating the same model for all training GPUs.**

There are three critical points for model parallel training: 

1. how to shard the layers of a model effectively;  如何有效地拆分模型的层。
2.  how to train sharded layers in parallel other than sequentially; 以并行而非串行的方式训练拆分的层。
3. how to design excellent throughputs between nodes. 在节点间设计优秀的吞吐量

#### pipeline parallelism 

- Module Parallel

  - [教程](https://pytorch.org/tutorials/intermediate/model_parallel_tutorial.html)：方便使用多个GPU加载一个大模型。

    - Apply Model Parallel to Existing Modules

    - Speed Up by Pipelining Inputs

  - 示例代码

    ```python
    import torch
    import torch.nn as nn
    import torch
    import torch.nn as nn
    import torch.optim as optim
    
    
    class DummyModel(nn.Module):
        def __init__(self):
            super(DummyModel, self).__init__()
            self.net0 = nn.Linear(20, 10).to('cuda:0')
            self.relu = nn.ReLU()
            self.net1 = nn.Linear(10, 10).to('cuda:1')
    
        def forward(self, x):
            x = self.relu(self.net0(x.to('cuda:0')))
            return self.net1(x.to('cuda:1'))
        
    
    model = DummyModel()
    loss_fn = nn.MSELoss()
    optimizer = optim.SGD(model.parameters(), lr=0.001)
    
    optimizer.zero_grad()
    outputs = model(torch.randn(30, 10))
    labels = torch.randn(30, 10).to('cuda:1')
    loss_fn(outputs, labels).backward()
    optimizer.step()
    ```

    

#### tensor parallelism

In essence, tensor parallelism consists in traversing the model and replacing specific submodules of the model with their distributed implementations.

- [Mesh-TensorFlow](https://arxiv.org/abs/1811.02084) ：TensorFlow
- [Megatron-LM](https://arxiv.org/abs/1909.08053) ：PyTorch
- Amazon SageMaker：PyTorch （ supporting **pipeline and tensor parallelism** with memory-saving features.）

- Deepspeed （微软）
- Megatron（NVIDIA）

### Zero Redundancy Optimizer（ZeRO）

对数据并行方法进行了修改，由于每个显卡上使用的是相同的模型参数与获得的梯度也是一样的，又会进行相同的参数更新，同样的事情每个显卡都去做，就会带来计算上的重复与冗余，为了避免重复计算，在每张显卡上只获取一部分参数的梯度，只进行一部分模型参数的更新，然后将模型参数更新结果在多张显卡之间进行共享，多张显卡分工合作完成模型参数更新。

- ZeRO-Stage 1

  分工合作，成果共享，避免了重复计算，加快了计算速度。

- ZeRO-Stage 2（反向传播，倒着来，用完一层删一层）

  假设有4张显卡，每个显卡上batch_size为32，模型为32层transformer，正常进行完前向传递，每个GPU上计算得到的梯度信息进行汇总后，汇总后的梯度信息可以用完即删，也就是说，第32层transformer的模型参数分成四份到每个GPU上，进行模型参数的更新，更新完之后，这部分的梯度信息就可以删除了。重复以上步骤，就可以降低显存占用情况。

  只在汇总每张显卡上的梯度信息时，使用了一次All Gather操作，

- ZeRO-Stage 3（模型参数，临时汇总，用完一层删一层）

  可以看到前两个阶段，模型在每张显卡上都保存完整的模型参数，而实际会出现每个GPU没有足够显存保存完整模型参数的情况，为此需要对模型的参数进行拆分，每张显卡上只保留了一部分的模型参数，结果也就会只保留这一部分模型参数的梯度，也就会只对这一部分模型参数进行更新，此时每张显卡上就只有一部分的数据，一部分的模型参数，一部分的梯度，一部分的优化器，一部分的模型参数更新。

  在进行前向传播的过程中，比如说在计算第一层transformer时，可以临时性地将每张显卡上保留的模型参数进行一个 All Gather操作，此时每张显卡上都有了第一层transformer的所有参数，用完就把不是自己保留的参数进行删除。在进行方向传播的过程中，如果需要用到某一层的所有参数也可以进行上述操作。

  相比于 ZeRO-Stage 2，该阶段在梯度更新时没有使用All Gather操作，但在前向传播和反向传播的过程分别使用了一次All Gather操作，增加一次进程间的通信。

### Pipeline Parallel 流水线并行

 流水线并行方法与模型的并行方法与类似之处，模型并行的方法是通过将线性层分成很多的小矩阵，将小的矩阵运算分到不同的显卡上，而流水线的并行方法则是将模型不同的层分给不同的显卡，比如说，对于一个3层transformer模型，可以将第一层分给第一张显卡，第二层分给第二张显卡，第三层分给第三张显卡，不同显卡间按照次序将各自的结果传给下一张显卡。此时Parameter、Gradient、Optimizer、Intermediate在每张显卡上占用的显存都变为原来的 1/N。该方法的弊端是某个显卡计算时，其余显卡都是闲着的，存在一个计算资源的浪费。



### 优化训练的方法

- Mixed Precision

- Offloading

  为每张显卡GPU绑定多个CPU，将部分Optimizer 状态转移到CPU上（Optimzer远比模型参数占用更多的显存。）

  - 将梯度从GPU转移到CPU
  - 在CPU上更新Optimizer的状态
  - 将更新后的参数从CPU传回GPU

- Overlapping

  在GPU中，memory操作一般是异步实现的，因此可以在两个memory操作之间穿插一个计算。比如在all gather获得第一层的所有参数之后，在第一层前向传播计算过程中，可以提前去获取第二层的所有参数，当第一层计算完成后，同时也获得了第二层的所有参数。

- Checkpointing

  检查点：主要思想是在前向传播的过程中不保留所有的中间结果，而只保留一定的检查点（some hidden states （checkpoint）are reserved），其他所有中间结果立即释放。而在反向传播过程中，被释放的中间结果，可以使用hidden states 重新进行计算，在获取到梯度信息后，再次进行释放。时间换空间。

  - Forward
    - some hidden states （checkpoint）are reserved
    - All other intermediate results are immediately freed（释放）
  - Backward
    - freed intermediate results  are recomputed
    - And release again after obtaining gradient states.

#### 加速训练框架

- Accelerate
- DeepSpeed
- FSDP（Fully-sharded data-parallel ）
- 链接
  - https://towardsdatascience.com/pytorch-lightning-vs-deepspeed-vs-fsdp-vs-ffcv-vs-e0d6b2a95719
  - 
