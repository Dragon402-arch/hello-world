#### 预训练模型

- **BERT**

  ```python
  import torch
  from transformers import BertTokenizer,BertModel
  
  BERT_MODEL_PATH = "../pretrained_model/chinese_wwm_ext_pytorch"
  tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_PATH)
  bert = BertModel.from_pretrained(BERT_MODEL_PATH)
  
  
  def get_model_output(text):
  	inputs = tokenizer.encode(text)
      # 模型输入必须是批量化输入，如果只输入一条数据，也必须是二维张量
  	mask = torch.tensor([[1] * len(inputs)], dtype=torch.long)
  	input_ids = torch.tensor([inputs], dtype=torch.long)
      out = bert(input_ids,mask)
      seq_out, pool_out = out[0], out[1]
      return seq_out, pool_out
  
  text = "我们都有一个家"
  seq_out, pool_out = get_model_output(text)
  print(pool_out.size(), seq_out.size())
  
  ```

  模型输出包含两部分内容：其一是序列输出，其二是`cls token`向量表示，且`cls token`表示并不是直接等于 `seq_out[:,0,:]`，而是使用该向量通过了一个线性层和 `tanh` 激活函数输出的结果，线性层的权重在下一句预测（分类任务）目标中进行训练。

  Last layer hidden-state of the first token of the sequence (classification token) **further processed by a Linear layer and a Tanh activation function**. The Linear layer weights are trained from the next sentence prediction (classification) objective during pre-training.

  This output is usually *not* a good summary of the semantic content of the input, you're often better with averaging or pooling the sequence of hidden-states for the whole input sequence.

- 权重与梯度

  ```python
  pretrained_model_path = "/home/lis/algProjects/pretrained_models/chinese_wwm_ext_pytorch/"
  model = BertForSequenceClassification.from_pretrained(pretrained_model_path)
  
  # 包含参数值与参数名称
  for name,param in model.named_parameters():
       print(name,param.size(),param.requires_grad)
          
  # 只包含参数值        
  for param in model.parameters():
       print(param.size(),param.requires_grad)
  ```

- BERT使用

  - 训练时使用

    ```shell
    model = BertModel.from_pretrained(pretrained_model_path)
    ```

  - 推理时使用

    ```python
    bert_config = BertConfig.from_pretrained(pretrained_model_path)
    model = BertModel(config=bert_config)
    ```

