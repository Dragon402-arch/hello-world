### 采样策略

#### 1、Temperature

```python
class TemperatureLogitsWarper(LogitsWarper):
    r"""
    [`LogitsWarper`] for temperature (exponential scaling output probability distribution).

    Args:
        temperature (`float`):
            The value used to module the logits distribution.
    """

    def __init__(self, temperature: float):
        if not isinstance(temperature, float) or not (temperature > 0):
            raise ValueError(f"`temperature` has to be a strictly positive float, but is {temperature}")

        self.temperature = temperature

    def __call__(self, input_ids: torch.Tensor, scores: torch.Tensor) -> torch.FloatTensor:
        scores = scores / self.temperature
        return scores
```

#### 2、TopK

```python
class TopKLogitsWarper(LogitsWarper):
    r"""
    [`LogitsWarper`] that performs top-k, i.e. restricting to the k highest probability elements.

    Args:
        top_k (`int`):
            The number of highest probability vocabulary tokens to keep for top-k-filtering.
        filter_value (`float`, *optional*, defaults to `-float("Inf")`):
            All filtered values will be set to this float value.
        min_tokens_to_keep (`int`, *optional*, defaults to 1):
            Minimum number of tokens that cannot be filtered.
    """

    def __init__(self, top_k: int, filter_value: float = -float("Inf"), min_tokens_to_keep: int = 1):
        if not isinstance(top_k, int) or top_k <= 0:
            raise ValueError(f"`top_k` has to be a strictly positive integer, but is {top_k}")

        self.top_k = max(top_k, min_tokens_to_keep)
        self.filter_value = filter_value

    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:
        top_k = min(self.top_k, scores.size(-1))  # Safety check
        # Remove all tokens with a probability less than the last token of the top-k
        indices_to_remove = scores < torch.topk(scores, top_k)[0][..., -1, None]
        # 对于 torch.topk(scores, top_k)[0][..., -1, None]，其中[0]表示取返回值的第一个元素也就是values,
        # 由于返回的values默认是将前topk个最大值降序排列的，则-1就是取最小的那个值（概率），
        # None的作用是进行升维，方便进行广播运算。
       # indices_to_remove中为True的位置被替换为 -inf
        scores = scores.masked_fill(indices_to_remove, self.filter_value)
        return scores

```

注解：

`torch.topk() `方法使用示例以及返回结果：

    >>> x = torch.arange(1., 6.)
    >>> x
    tensor([ 1.,  2.,  3.,  4.,  5.])
    >>> torch.topk(x, 3)
    torch.return_types.topk(values=tensor([5., 4., 3.]), indices=tensor([4, 3, 2]))

#### 3、TopP

```python
class TopPLogitsWarper:
    """
    [`LogitsWarper`] that performs top-p, i.e. restricting to top tokens summing to prob_cut_off <= prob_cut_off.

    Args:
        top_p (`float`):
            If set to < 1, only the smallest set of most probable tokens with probabilities that add up to `top_p` or
            higher are kept for generation.
        filter_value (`float`, *optional*, defaults to `-float("Inf")`):
            All filtered values will be set to this float value.
        min_tokens_to_keep (`int`, *optional*, defaults to 1):
            Minimum number of tokens that cannot be filtered.
    """

    def __init__(self, top_p: float, filter_value: float = -float("Inf"), min_tokens_to_keep: int = 1):
        top_p = float(top_p)
        if top_p < 0 or top_p > 1.0:
            raise ValueError(f"`top_p` has to be a float > 0 and < 1, but is {top_p}")

        self.top_p = top_p
        self.filter_value = filter_value
        self.min_tokens_to_keep = min_tokens_to_keep

    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:
        # 进行升序排列，返回排列后的值以及对应的索引值
        sorted_logits, sorted_indices = torch.sort(scores, descending=False)
        print(sorted_logits, sorted_indices)
        # 将该值进行概率化处理，然后进行累加求和
        cumulative_probs = sorted_logits.softmax(dim=-1).cumsum(dim=-1)
        print(cumulative_probs)
		# 低于阈值的位置变为True
        # Remove tokens with cumulative top_p above the threshold (token with 0 are kept)
        sorted_indices_to_remove = cumulative_probs <= (1 - self.top_p)
        print(sorted_indices_to_remove)
        if self.min_tokens_to_keep > 1:
            # Keep at least min_tokens_to_keep
            sorted_indices_to_remove[..., -self.min_tokens_to_keep:] = 0 # False
		# 将sorted_indices_to_remove的值按照原索引进行重排
        # scatter sorted tensors to original indexing
        indices_to_remove = sorted_indices_to_remove.scatter(dim=1, index=sorted_indices, src=sorted_indices_to_remove)

        # scatter()方法的等价实现，实现将sorted_indices_to_remove按照最初scores中的索引顺序进行排列
        # sorted_indices = sorted_indices[0].tolist()
        # sorted_indices_to_remove = sorted_indices_to_remove[0].tolist()
        # map_dict = dict(zip(sorted_indices, sorted_indices_to_remove))
        # result = [map_dict[i] for i in range(scores.shape[1])]
        # print(torch.as_tensor(result)[None, :])

        print(indices_to_remove)
        # indices_to_remove中为True的位置被替换为 -inf
        scores = scores.masked_fill(indices_to_remove, self.filter_value)
        return scores
```

测试用例：

```python
scores = torch.rand(1, 32).float()
print(scores)
input_ids = torch.randn(1, 32).long()
logits_wrapper = TopPLogitsWarper(top_p=0.85)

print(logits_wrapper(input_ids=input_ids, scores=scores))
```

#### 4、repetition_penalty

```python
class RepetitionPenaltyLogitsProcessor(LogitsProcessor):
    r"""
    [`LogitsProcessor`] enforcing an exponential penalty on repeated sequences.

    Args:
        repetition_penalty (`float`):
            The parameter for repetition penalty. 1.0 means no penalty. See [this
            paper](https://arxiv.org/pdf/1909.05858.pdf) for more details.
    """

    def __init__(self, penalty: float):
        if not isinstance(penalty, float) or not (penalty > 0):
            raise ValueError(f"`penalty` has to be a strictly positive float, but is {penalty}")

        self.penalty = penalty

    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:
        score = torch.gather(scores, 1, input_ids)

        # if score < 0 then repetition penalty has to be multiplied to reduce the previous token probability
        score = torch.where(score < 0, score * self.penalty, score / self.penalty)

        scores.scatter_(1, input_ids, score)
        return scores
```



### 生成过程

1、第一步：

```python
model.generate()
```

第二步：

```python
elif is_sample_gen_mode:
    # 11. prepare logits warper
    logits_warper = self._get_logits_warper(generation_config)

    # 12. expand input_ids with `num_return_sequences` additional sequences per batch
    input_ids, model_kwargs = self._expand_inputs_for_generation(
        input_ids=input_ids,
        expand_size=generation_config.num_return_sequences,
        is_encoder_decoder=self.config.is_encoder_decoder,
        **model_kwargs,
    )

    # 13. run sample
    return self.sample(
        input_ids,
        logits_processor=logits_processor,
        logits_warper=logits_warper,
        stopping_criteria=stopping_criteria,
        pad_token_id=generation_config.pad_token_id,
        eos_token_id=generation_config.eos_token_id,
        output_scores=generation_config.output_scores,
        return_dict_in_generate=generation_config.return_dict_in_generate,
        synced_gpus=synced_gpus,
        streamer=streamer,
        **model_kwargs,
    )
```

- `self.sample()` 使用示例：

  ```python
  Examples:
  
  
      >>> from transformers import (
          ...     AutoTokenizer,
          ...     AutoModelForCausalLM,
          ...     LogitsProcessorList,
          ...     MinLengthLogitsProcessor,
          ...     TopKLogitsWarper,
          ...     TemperatureLogitsWarper,
          ...     StoppingCriteriaList,
          ...     MaxLengthCriteria,
          ... )
      >>> import torch
  
      >>> tokenizer = AutoTokenizer.from_pretrained("gpt2")
      >>> model = AutoModelForCausalLM.from_pretrained("gpt2")
  
      >>> # set pad_token_id to eos_token_id because GPT2 does not have a EOS token
      >>> model.config.pad_token_id = model.config.eos_token_id
      >>> model.generation_config.pad_token_id = model.config.eos_token_id
  
      >>> input_prompt = "Today is a beautiful day, and"
      >>> input_ids = tokenizer(input_prompt, return_tensors="pt").input_ids
  
      >>> # instantiate logits processors
      >>> logits_processor = LogitsProcessorList(
          ...     [ 
              MinLengthLogitsProcessor(15,eos_token_id=model.generation_config.eos_token_id)
                         ]
          ... )
      >>> # instantiate logits processors
      >>> logits_warper = LogitsProcessorList(
          ...     [
              ...         TopKLogitsWarper(50),
              ...         TemperatureLogitsWarper(0.7),
              ...     ]
          ... )
  
      >>> stopping_criteria = StoppingCriteriaList([MaxLengthCriteria(max_length=20)])
  
      >>> torch.manual_seed(0)  # doctest: +IGNORE_RESULT
      >>> outputs = model.sample(
          ...     input_ids,
          ...     logits_processor=logits_processor,
          ...     logits_warper=logits_warper,
          ...     stopping_criteria=stopping_criteria,
          ... )
  
      >>> tokenizer.batch_decode(outputs, skip_special_tokens=True)
      ['Today is a beautiful day, and we must do everything possible to make it a day of celebration.']
  ```

第四步：

```python
def _get_logits_warper(
    self,
    generation_config: GenerationConfig,
) -> LogitsProcessorList:
    """
    This class returns a [`LogitsProcessorList`] list object that contains all relevant [`LogitsWarper`] instances
    used for multinomial sampling.
    """

    # instantiate warpers list
    warpers = LogitsProcessorList()

    # the following idea is largely copied from this PR: https://github.com/huggingface/transformers/pull/5420/files
    # all samplers can be found in `generation_utils_samplers.py`
    if (
        generation_config.temperature is not None
        and generation_config.temperature != 1.0
    ):
        warpers.append(TemperatureLogitsWarper(generation_config.temperature))
    min_tokens_to_keep = 2 if generation_config.num_beams > 1 else 1
    if generation_config.top_k is not None and generation_config.top_k != 0:
        warpers.append(
            TopKLogitsWarper(
                top_k=generation_config.top_k, min_tokens_to_keep=min_tokens_to_keep
            )
        )
    if generation_config.top_p is not None and generation_config.top_p < 1.0:
        warpers.append(
            TopPLogitsWarper(
                top_p=generation_config.top_p, min_tokens_to_keep=min_tokens_to_keep
            )
        )
    return warpers

```

内部数据处理过程：

```python
class LogitsProcessorList(list):
        @add_start_docstrings(LOGITS_PROCESSOR_INPUTS_DOCSTRING)
    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> torch.FloatTensor:
        for processor in self:
            function_args = inspect.signature(processor.__call__).parameters
            if len(function_args) > 2:
                if not all(arg in kwargs for arg in list(function_args.keys())[2:]):
                    raise ValueError(
                        f"Make sure that all the required parameters: {list(function_args.keys())} for "
                        f"{processor.__class__} are passed to the logits processor."
                    )
                scores = processor(input_ids, scores, **kwargs)
            else:
                scores = processor(input_ids, scores)
        return scores
```



第五步：

```python
 # prepare model inputs
model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)

# forward pass to get next token
outputs = self(
    **model_inputs,
    return_dict=True,
    output_attentions=output_attentions,
    output_hidden_states=output_hidden_states,
)
next_token_logits = outputs.logits[:, -1, :]

# pre-process distribution
next_token_scores = logits_processor(input_ids, next_token_logits)
# 在这里发挥作用，调用LogitsProcessorList的 __call__()方法

next_token_scores = logits_warper(input_ids, next_token_scores)

# sample
probs = nn.functional.softmax(next_token_scores, dim=-1)
next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)

# update generated ids, model inputs, and length for next step
input_ids = torch.cat([input_ids, next_tokens[:, None]], dim=-1)
```







### 结束条件

在 `self.sample() `方法内部：

```python
eos_token_id = (
    eos_token_id if eos_token_id is not None else self.generation_config.eos_token_id
)
if isinstance(eos_token_id, int):
    eos_token_id = [eos_token_id]
eos_token_id_tensor = (
    torch.tensor(eos_token_id).to(input_ids.device)
    if eos_token_id is not None
    else None
)
```

追踪多个序列的生成情况

```python
# keep track of which sequences are already finished

# 1 就是True,表示该序列还没有生成结束
unfinished_sequences = torch.ones(input_ids.shape[0], dtype=torch.long, device=input_ids.device)
```

批量生成情况下序列补齐问题

```python
# finished sentences should have their next token be a padding token
if eos_token_id is not None:
    if pad_token_id is None:
        raise ValueError(
            "If `eos_token_id` is defined, make sure that `pad_token_id` is defined."
        )
    next_tokens = next_tokens * unfinished_sequences + pad_token_id * (
        1 - unfinished_sequences
    )

```

是否结束生成条件判断

```python
output_scores = output_scores if output_scores is not None else self.generation_config.output_scores
return_dict_in_generate = (
    return_dict_in_generate
    if return_dict_in_generate is not None
    else self.generation_config.return_dict_in_generate
)
scores = () if (return_dict_in_generate and output_scores) else None
if return_dict_in_generate:
    if output_scores:
        scores += (next_token_scores,)
```



```python
# if eos_token was found in one sentence, set sentence to finished
if eos_token_id_tensor is not None:
    unfinished_sequences = unfinished_sequences.mul(
        next_tokens.tile(eos_token_id_tensor.shape[0], 1) # shape (bath_size,1)
        .ne(eos_token_id_tensor.unsqueeze(1)) # eos_token_id_tensor.unsqueeze(1) shape (1,1)
        .prod(dim=0) # (1)
    )

    # stop when each sentence is finished
    if unfinished_sequences.max() == 0:
        this_peer_finished = True

# stop if we exceed the maximum length
if stopping_criteria(input_ids, scores):
    this_peer_finished = True

if this_peer_finished:
    break

```

- 工具方法

  - `torch.mul()`

    ```python
    Args:
        input (Tensor): the input tensor.
        other (Number): the number to be multiplied to each element of :attr:`input`
    ```

  - `torch.tile()`

    ```python
    Args:
        input (Tensor): the tensor whose elements to repeat.
        reps (tuple): the number of repetitions per dimension.
        
    Example::
        
        >>> x = torch.tensor([1, 2, 3])
        >>> x.tile((2,))
        tensor([1, 2, 3, 1, 2, 3])
    ```

    

  - `torch.ne()`

    ```python
    Args:
        input (Tensor): the tensor to compare
        other (Tensor or float): the tensor or value to compare
    Returns:
        A boolean tensor that is True where :attr:`input` is not equal to :attr:`other` and False elsewhere
    Example::
        >>> torch.ne(
            torch.tensor([[1, 2], [3, 4]]), 
            torch.tensor([[1, 1], [4, 4]])
        )
        tensor([
            [False, True], 
            [True, False]]
        )
    ```

  - `torch.prod()`

    ```python
    Args:
        input (Tensor): the input tensor.
        dim (int): the dimension to reduce.
        keepdim (bool): whether the output tensor has :attr:`dim` retained or not.
    Returns the product of each row of the :attr:`input` tensor in the given dimension :attr:`dim`.
    Example::
        
        >>> a = torch.randn(4, 2)
        >>> a
        tensor([[ 0.5261, -0.3837],
                [ 1.1857, -0.2498],
                [-1.1646,  0.0705],
                [ 1.1131, -1.0629]])
        >>> torch.prod(a, 1)
        tensor([-0.2018, -0.2962, -0.0821, -1.1831])
    ```

    

  - 

先判断是否所有序列都已经生成eos没有，然后再去判断是否都达到了最大长度。

停止标准相关代码：

```python
stopping_criteria = (
    stopping_criteria
    if stopping_criteria is not None
    else StoppingCriteriaList()
)

stopping_criteria = self._get_stopping_criteria(
    generation_config=generation_config, stopping_criteria=stopping_criteria
)

# [<transformers.generation.stopping_criteria.MaxLengthCriteria object at 0x7f0253633450>]
```

上述代码主要涉及 `StoppingCriteriaList()`、`MaxLengthCriteria`、`_get_stopping_criteria` 三个类或方法，其细节如下：

```python
class StoppingCriteriaList(list):
    @add_start_docstrings(STOPPING_CRITERIA_INPUTS_DOCSTRING)
    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:
        return any(criteria(input_ids, scores) for criteria in self)  # self是一个列表对象，其继承了列表list  
    
class MaxLengthCriteria(StoppingCriteria):
    """
    This class can be used to stop generation whenever the full generated number of tokens exceeds `max_length`. Keep
    in mind for decoder-only type of transformers, this will include the initial prompted tokens.

    Args:
        max_length (`int`):
            The maximum length that the output sequence can have in number of tokens.
    """

    def __init__(self, max_length: int):
        self.max_length = max_length

    @add_start_docstrings(STOPPING_CRITERIA_INPUTS_DOCSTRING)
    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:
        return input_ids.shape[-1] >= self.max_length

    
def _get_stopping_criteria(
        self,
        generation_config: GenerationConfig,
        stopping_criteria: Optional[StoppingCriteriaList],
) -> StoppingCriteriaList:
    criteria = StoppingCriteriaList()
    if generation_config.max_length is not None:
        criteria.append(MaxLengthCriteria(max_length=generation_config.max_length))
    if generation_config.max_time is not None:
        criteria.append(MaxTimeCriteria(max_time=generation_config.max_time))
     # 内部只是进行验证操作
    criteria = self._merge_criteria_processor_list(criteria, stopping_criteria)
    return criteria
```



