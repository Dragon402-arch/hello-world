### 1. 下载模型

#### 1.1 下载仓库

代码示例

HuggingFace

```python
from huggingface_hub import snapshot_download

save_model_path = "/home/lis/algProjects/pretrained_models/tigerbot-7b"
res = snapshot_download(
"TigerResearch/tigerbot-7b-sft",
local_dir=save_model_path,
local_dir_use_symlinks=False,
)


git lfs install
git clone https://huggingface.co/THUDM/chatglm2-6b
```

ModelSocpe

```python
from modelscope.hub.snapshot_download import snapshot_download

model_dir = snapshot_download(
    'ZhipuAI/chatglm2-6b',
    cache_dir='/home/lis/algProjects/pretrained_models/chatglm2-6b',
    revision="v1.0.2")
```



#### 1.2 下载权重

代码示例

```python
from huggingface_hub import hf_hub_download

checkpoint = "bert-base-uncased"
hf_hub_download(
    checkpoint, 
    "pytorch_model.bin",
     local_dir="./bert-base-uncased",
	 local_dir_use_symlinks=False,
)


/THUDM/chatglm-6b/blob/main/pytorch_model-00002-of-00008.bin

https://huggingface.co/THUDM/chatglm-6b/blob/main/pytorch_model-00002-of-00008.bin
```



### 2. 加载模型

#### 2.1 使用`from_pretrained`方法

代码示例

```python
from transformers import BloomForCausalLM, BloomTokenizerFast
import os

DEVICE_ID = 0
os.environ["CUDA_VISIBLE_DEVICES"] = f"{DEVICE_ID}"

pretrained_model_path = "/home/lis/algProjects/pretrained_models/tigerbot-7b/"
model = BloomForCausalLM.from_pretrained(
    "/home/lis/algProjects/pretrained_models/tigerbot-7b/",
    device_map="auto",
    torch_dtype="auto",
)
```

#### 2.2 使用 `load_checkpoint_and_dispatch` 方法

- [官方参考示例](https://huggingface.co/docs/accelerate/usage_guides/big_modeling)

代码示例1

```python
from accelerate import init_empty_weights, load_checkpoint_and_dispatch
from transformers import AutoConfig, AutoModel

checkpoint = "./src_chatglm"
config = AutoConfig.from_pretrained(checkpoint, revision="fp32", trust_remote_code=True)

# 不消耗内存初始化模型,可用于初始化超大模型
with init_empty_weights():
    model = AutoModel.from_config(config, trust_remote_code=True)

# 模型分片
model = load_checkpoint_and_dispatch(  # 加载权重并将权重自动分发到指定设备上
    model,
    checkpoint,
    device_map="auto",
    no_split_module_classes=["GLMBlock"],
    max_memory=None,
).float()

```

代码示例2

```python
from accelerate import init_empty_weights, load_checkpoint_and_dispatch
from accelerate import infer_auto_device_map
from transformers import AutoConfig, AutoModel

checkpoint = "./src_chatglm"
config = AutoConfig.from_pretrained(checkpoint, revision="fp32", trust_remote_code=True)

# 不消耗内存初始化模型,可用于初始化超大模型
with init_empty_weights():
    model = AutoModel.from_config(config, trust_remote_code=True)

device_map = infer_auto_device_map(
    model,
    max_memory={0: "10GiB", 1: "10GiB", "cpu": "30GiB"},
    no_split_module_classes=["GLMBlock"],
)

# 模型分片
model = load_checkpoint_and_dispatch(  # 加载权重并将权重自动分发到指定设备上
    model,
    checkpoint,
    device_map=device_map,
    no_split_module_classes=["GLMBlock"],
    max_memory=None,
).float()


   
```

#### 2.3 指定GPU

```python
# 终端指定
CUDA_VISIBLE_DEVICES=0 python my_script.py


# 代码指定
import os
os.environ["CUDA_VISIBLE_DEVICES"] = "0"


# 代码指定
import torch
torch.cuda.set_device(id)

# 代码指定
import torch
x = torch.rand(3,3)
x.cuda(device=0)
```

注意事项：

`CUDA_VISIBLE_DEVICES` 使用

- 错误写法

  ```python
  import os
  import torch
  
  os.environ["CUDA_VISIBLE_DEVICES"] = "1"
  
  device = torch.device("cuda:1" if torch.cuda.is_available() else "cpu")
  # device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
  x = torch.as_tensor([1,2],device=device)
  print(x)
  ```

  - 报错信息

    ```shell
    Traceback (most recent call last):
      File "/home/lis/algProjects/sequenceProject/main.py", line 9, in <module>
        x = torch.as_tensor([1,2],device=device)
    RuntimeError: CUDA error: invalid device ordinal
    CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
    For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
    ```

    由于设置了只能看到第二块GPU，因此此时GPU数量只是1，设置的DEVICE_ID的索引只能是0，而不能是1。

- 正确写法

  ```python
  import os
  import torch
  
  os.environ["CUDA_VISIBLE_DEVICES"] = "1"
  
  device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
  x = torch.as_tensor([1,2],device=device)
  print(x)
  ```



官方

ChatGLMTokenizer(name_or_path='/date/pretrained_models/ChatGLM/', vocab_size=150344, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<sop>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)

本地

ChatGLMTokenizer(name_or_path='', vocab_size=150344, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<sop>', 'eos_token': '<eop>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)





