#### 单机多卡启动

- torchrun

  ```shell
  CUDA_VISIBLE_DEVICES=2,1,0 OMP_NUM_THREADS=10 torchrun --nproc_per_node=3 --master_port=6996 finetune_model.py
  ```

  若不传入OMP_NUM_THREADS 参数，就会出现如下警告：

  > WARNING:torch.distributed.run:
  > *****************************************
  > Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
  >
  > *****************************************

- deepspeed

  ```shell
  deepspeed --include localhost:0,1,2 finetune_model.py --deepspeed ds_config.json
  
  
  # 单机多卡时，传入master_port 和 master_addr 两个参数没有实际作用，多机多卡时需要进行通信
  deepspeed --include localhost:0,1,2 finetune_model.py --master_port=6996 --master_addr="172.16.100.31" --deepspeed ds_config.json
  ```

  注意：CUDA_VISIBLE_DEVICES 参数对于deepspeed启动无效

- accelerate

  ```shell
  accelerate launch --config_file ./zero_stage3_acce_config.yaml acce_ds_run.py
  ```

  若服务器安装了多个CUDA版本，且在 config_file 文件中，有配置这两个参数：

  ```yaml
  offload_optimizer_device: 'cpu'
  offload_param_device: 'cpu'
  ```

  如果出现如下报错BUG信息：

  >   torch/include/ATen/cuda/CUDAContext.h:10:10: fatal error: cusolverDn.h: No such file or directory

  对应的解决办法为

  ```shell
  export CUDA_HOME=/usr/local/cuda-11.6
  
  accelerate launch --config_file ./zero_stage3_acce_config.yaml acce_ds_run.py
  
  ```

#### 多机多卡启动

```shell
Parameter at index 55 has been marked as ready twice. This means that multiple autograd engine  hooks have fired for this particular parameter during this iteration. You can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print parameter names for further debugging.
```

