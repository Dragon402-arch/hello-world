## 模型训练

- 指定GPU进行训练

  ```python
  import torch
  
  # 方法一：
  torch.cuda.set_device(1)
  
  # 方法二：
  device = torch.device("cuda:1")
  
  # 方法三：（官方推荐使用）
  os.environ["CUDA_VISIBLE_DEVICES"] = '1'
  os.environ["CUDA_VISIBLE_DEVICES"] = '1,2'
  
  # 若使用以上代码，仍使用默认的GPU进行训练，则需要将指定GPU的代码放在执行文件最开始的部位。
  
  ```

  

- learning rate warmup：针对模型训练过程中学习率优化的一种策略。

  > The idea behind learning rate warmup is simple. In the earliest stages of training - weights are far from their ideal states. This means large updates all across the board, which can be seen as "overcorrections" for each weight - where the drastic update of another may negate the update of some other weight, making initial stages of training more unstable.
  >
  > 这意味着整个模型权重的大量更新，可以看作是每个权重的“过度纠正”，一个权重的巨大更新可能会抵消某些其他权重的更新，从而使训练的初始阶段更加不稳定。
  >
  > These changes iron out, but can be avoided by having a small learning rate to begin with, reaching a more stable suboptimal state, and then applying a larger learning rate. You can sort of ease the network into updates, rather than hit it with them.
  >
  > 这些变化会被消除，可以通过从开始就采用较小的学习率，达到更稳定的次优状态，接着使用较大的学习率来避免。您可以将网络缓慢地更新，而不是快速更新。
  >
  > That's learning rate warmup! Starting with a low (or 0) learning rate and increasing to a starting learning rate (what you'd start with anyway). This increase can follow any function really, but is commonly linear.
  >
  > 这就是learning rate warmup！从0或低学习率开始，逐渐增加到起始学习率（您本来就在开始训练时设定的）。这种增加实际上可以遵循任何函数，但通常是线性的。
  >
  > After reaching the initial rate, other schedules such as cosine decay, linear reduction, etc. can be applied to progressively lower the rate down until the end of training. **Learning rate warmup is usually part of a two-schedule schedule, where LR warmup is the first, while another schedule takes over after the rate has reached a starting point.**
  >
  > 在达到开始设定的学习率后，可以应用其他schedule，例如余弦衰减、线性减少等，逐渐降低速率，直到训练结束。学习率预热通常是two-schedule schedule的一部分，其中学习率预热是第一个，而另一个schedule在学习率达到训练开始预定的学习率后发挥作用（第一个schedule负责把学习率升上去，第二个schedule 负责把学习率降下来 ）。
  
  刚开始训练时，模型的权重是随机初始化得到的，若选择一个较大的学习率，可能会使模型不稳定（振荡），选择warm up的方式，使得模型在最初的epoch或step内学习率较小，模型慢慢趋于稳定。模型相对稳定后再选择预先设置的学习率进行训练，使模型收敛速度更快，模型效果更佳。
  
  ```python
  from transformers import AdamW, get_linear_schedule_with_warmup
  
  
  optimizer = AdamW(model.parameters(),
                    lr = 2e-5, # default is 5e-5, our notebook had 2e-5
                    eps = 1e-8 # default is 1e-8.
                    )
  
  
  # Total number of training steps is number of batches * number of epochs.`train_dataloader` contains batched data so `len(train_dataloader)` gives us the number of batches.
  
  total_steps = len(train_dataloader) * epochs
  
  # Create the learning rate scheduler.
  scheduler = get_linear_schedule_with_warmup(optimizer, 
                                              num_warmup_steps = 0, # Default value in run_glue.py
                                              num_training_steps = total_steps)
  ```
  
  - 当`num_warmup_steps=0`时，`learning rate`没有warmup的过程，只有从初始设定的`learning rate`衰减到0的过程。
  
  paddle中使用
  
  ```python
  # 训练过程中的最大学习率
  learning_rate = 3e-5 
  
  # 训练轮次
  epochs = 2
  
  # 学习率预热比例
  warmup_proportion = 0.1
  
  # 权重衰减系数，类似模型正则项策略，避免模型过拟合
  weight_decay = 0.01
  
  num_training_steps = len(train_data_loader) * epochs
  
  # 学习率衰减策略
  lr_scheduler = paddlenlp.transformers.LinearDecayWithWarmup(learning_rate, num_training_steps, warmup_proportion)
  
  decay_params = [
      p.name for n, p in model.named_parameters()
      if not any(nd in n for nd in ["bias", "norm"])
  ]
  
  optimizer = paddle.optimizer.AdamW(
      learning_rate=lr_scheduler,
      parameters=model.parameters(),
      weight_decay=weight_decay,
      apply_decay_param_fun=lambda x: x in decay_params)
  ```
  
  其中 warmup_proportion 学习预热比例表示，预热学习的步数比例。比如warmup_proportion=0.1，总步数=100，那么warmup步数就为10，在第10步时就会达到设定的目标学习率。从第1步到第10步，学习率最终增加到设定的学习率，随后将学习率从优化器的预定的学习率（线性\非线性）降低到0.



