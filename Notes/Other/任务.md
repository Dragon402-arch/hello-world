- 交叉熵损失内部实现，非公式版本

- 比较两种bert模型分类任务训练的差异

- 模型蒸馏模板中可以借鉴的地方，梯度积累、fp16、函数和类的封装

- chatbot stackbuse

- B站聊天机器人视频

- [源码理解](https://pytorch-nlp.com/8/)

- [PyTorch源码](https://xenonpy.readthedocs.io/en/stable/xenonpy.model.training.html#xenonpy.model.training.loss.L1Loss)

- 自然语言生成：聊天 机器人，问答，摘要，对话，，

- 最大似然估计，交叉熵损失
  - https://towardsdatascience.com/log-loss-function-math-explained-5b83cd8d9c83
  - https://glassboxmedicine.com/2019/12/07/connections-log-likelihood-cross-entropy-kl-divergence-logistic-regression-and-neural-networks/
  - https://zhuanlan.zhihu.com/p/159477597
  - https://randool.github.io/2018/06/22/Loss%20Functions/
  - 
  
- 按照任务收集公开数据集上传GitHub，分开存储。

- 目前所有用到的损失函数计算方法，以及适用场景；

- 手动编写HMM、CRF、

- Prompt learning，PET（Pattern-Exploiting Training）

- Git 撤销undo操作，GIt，GitHub熟练操作

- 生成

  - 通过训练模型预测下一个词，从而在给定第一个词的情况下生成一个指定长度的句子。

  - 通过ngram统计得到下一个词的分布情况，从中随机选取一个当作预测的下一个词，从而在给定第一个词的情况下生成一个指定长度的句子。

  - 序列分类获取一个序列，实现细节有待完善

    - 训练过程：

      ​		输入：sos A B C 

      ​		输出：A B C eos

    - 评估过程：同上

    - 预测过程：分条分布预测，指定最大序列长度，根据sos、eos进行解码获取生成的结果。

  - seq2seq模型：encoder-decoder模型是一种实现方法，中，encoder可有可无，而decoder则必不可少，encoder可以使用静态向量提供上下文表示，也可以使用注意力机制生成动态上下文表示，在机器翻译任务中encoder必不可以，而在姓氏生成国籍的任务中则可以不要encoder。

    



