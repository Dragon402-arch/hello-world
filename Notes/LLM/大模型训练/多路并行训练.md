## 多路并行 Multi-way Parallelism

We showcase this approach by training an 8.3 billion parameter transformer language model with **8-way model parallelism** and **64-way data parallelism** on 512 GPUs, making it the **largest transformer based language model ever trained at 24x the size of BERT and 5.6x the size of GPT-2**. 

CodeGeex并行训练：192路数据并行+8路模型并行（流水线模型并行），全局批大小为3072，使用了1536张昇腾910处理器。

每个数据并行使用了$1536/192 =8$ 张GPU，每个数据并行的 mini-batch size 为 $3072/192=16$，则每路流水线模型并行的micro-batch size 为 $16/8=2$。相当于 $192 * 8 = 1536$ 路数据并行，每个batch_size为2。 

代码注解：

```python
# helper argument to set deepspeed pipeline parallel or not
args.ds_pipeline_enabled = not args.no_pipeline_parallel

assert args.world_size % args.tensor_model_parallel_size == 0

# Pipeline model parallel size.
args.pipeline_model_parallel_size = min(
    args.pipeline_model_parallel_size,
    (args.world_size // args.tensor_model_parallel_size),
)


# Checks.
if args.no_pipeline_parallel:
    assert (
        args.pipeline_model_parallel_size == 1
    ), "pipeline_model_parallel_size must be 1 if pipeline parallel is disabled"
    
model_parallel_size = (
   args.pipeline_model_parallel_size * args.tensor_model_parallel_size
)

assert args.world_size % model_parallel_size == 0


# 192 = 1536 // 8

args.data_parallel_size = args.world_size // model_parallel_size


# 3072  = 16 * 192
args.global_batch_size = args.micro_batch_size * args.data_parallel_size

```



多路并行示意图：

![多路并行](D:\Typora\Notes\LLM\大模型训练\多路并行.png)



以NVIDIA 多路并训练 GPT-3 模型为例进行说明，首先是使用了6 路数据并行，其次切分模型为64个阶段进行流水线模型并行，最后是针对每个阶段进行张量模型并行。模型训练总共使用了 $6 * 64 * 8 = 384 * 8 = 3072$ 块 A100 GPU。同一台机器不同GPU之间使用 NVLink进行通信，不同机器之间使用 IB （InfiniBand）进行通信。每路并行训练时使用了64台机器，共计 512 块显卡。

多种并行方式总结：

1. DataParallel (DP) - the same setup is replicated multiple times, and each being fed a slice of the data. The processing is done in parallel and all setups are synchronized at the end of each training step.

2. TensorParallel (TP) - each tensor is split up into multiple chunks, so instead of having the whole tensor reside on a single gpu, each shard of the tensor resides on its designated gpu. During processing each shard gets processed separately and in parallel on different GPUs and the results are synced at the end of the step. This is what one may call **horizontal parallelism**, as the splitting happens on horizontal level.

3. PipelineParallel (PP) - the model is split up **vertically (layer-level)** across multiple GPUs, so that only one or several layers of the model are places on a single gpu. Each gpu processes in parallel different stages of the pipeline and working on a small chunk of the batch.

4. Zero Redundancy Optimizer (ZeRO) - Also performs sharding of the tensors somewhat similar to TP, except the whole tensor gets reconstructed in time for a forward or backward computation, therefore the model doesn’t need to be modified. It also supports various offloading techniques to compensate for limited GPU memory.

   ![3D数据并行](D:\Typora\Notes\LLM\大模型训练\3D数据并行.png)

上图显示了8个节点，每个节点包含4个GPU（相同颜色的GPU表示同一个节点）。在使用Zero3数据并行和流水线并行的情况下，所有GPU一起只保留了一份完整的模型权重。在3D并行中，GPU4同时应用在数据并行、流水线并行、张量并行的一个组中。

**When ZeRO-DP is combined with PP (and optionally TP) it typically enables only ZeRO stage 1 (optimizer sharding).**

模型切分粒度：

- PP（垂直切分）：流水线并行（模型分层）会对模型进去切分，将一层或几层切分到不同的GPU上。
- DP：分片数据并行（权重间切分）会对模型权重进行切分，如将模型权重A和B切分到不同的GPU上。
- TP（水平切分）：张量并行（权重内分块）也会对模型权重进行切分，如将模型权重A切分为A1、A2、A3、A4四个权重到不同的GPU上。

并行分组：

```markdown
Let's say we have a total of 16 GPUs denoted by g0 ... g15 and we use 2 GPUs to parallelize the model tensor, and 4 GPUs to parallelize the model pipeline. The present function will create 8 tensor model-parallel groups, 4 pipeline model-parallel groups and 8 data-parallel groups as:
    8 data_parallel groups:
        [g0, g2], [g1, g3], [g4, g6], [g5, g7], [g8, g10], [g9, g11], [g12, g14], [g13, g15]
    8 tensor model-parallel groups:
        [g0, g1], [g2, g3], [g4, g5], [g6, g7], [g8, g9], [g10, g11], [g12, g13], [g14, g15]
    4 pipeline model-parallel groups:
        [g0, g4, g8, g12], [g1, g5, g9, g13], [g2, g6, g10, g14], [g3, g7, g11, g15]
Note that for efficiency, the caller should make sure adjacent ranks are on the same DGX box. For example if we are using 2 DGX-1 boxes with a total of 16 GPUs, rank 0 to 7 belong to the first box and ranks 8 to 15 belong to the second box.
```

- 假定目前有16个GPU，属于两个node，rank 0 ～7 属于第一个节点，rank 8 ～ 15 属于第二个节点。

- create 8 tensor model-parallel groups, 4 pipeline model-parallel groups，这说明将一个完整模型切分如下：

  - 沿着行横向切了一刀：tensor_model_parallel_size = 16 / 8 = 2，就是2个 GPUs 来进行模型张量并行。
  - 沿着列纵向切了三刀：pipeline_model_parallel_size = 16 /4 = 4，就是4个GPUs 进行流水线并行。
  - 因此，一个模型分为8块，每一块放在一个GPU之上，就是8个GPU。而通过如下计算可以知 16 GPUs / 8 GPUs = 2 models。即，16张卡可以放置两个完整模型。

- 因为张量模型并行组大小是2，即16个GPU被分成8组，则这8组内容是 [g0, g1], [g2, g3], [g4, g5], [g6, g7], [g8, g9], [g10, g11], [g12, g13], [g14, g15]。

- 因为流水线并行组大小是4，即16个GPU被分成4组，则这4组内容是[g0, g4, g8, g12], [g1, g5, g9, g13], [g2, g6, g10, g14], [g3, g7, g11, g15]。

- 因为数据并行组大小是2，16个GPU被分成8组，则这8组内容是[g0, g2], [g1, g3], [g4, g6], [g5, g7], [g8, g10], [g9, g11], [g12, g14], [g13, g15]。

- 以上这些进程组都是通过 torch.distributed.new_group 来完成，这样组内进程之间就知道哪些进程是在同一个组内，是在一起训练的，也知道怎么通信。

  当张量并行大小为1时，流水线并行大小与数据并行大小相等；当张量并行大小不等于1时，流水线并行大小与数据并行大小不相等。

  张量并行将从同一个张量拆分出来的分为一组，流水线并行将组成一条流水线的分为一组，数据并行将保存有相同模型权重的部分分为一组。

**理解难点**

一般情况下，一个数据并行对应一个流水线并行，而在使用张量并行的情况下，会出现一个数据并行对应多个流水线并行的情况。如上面的示例中，数据并行路数为2，流水线并行路数为4， 张量并行路数为2，可知：
$$
数据并行路数 *  张量并行路数 = 流水线并行路数
$$

sftp -P 29880 henglian@iftp.iluvatar.com.cn
123465
cd /test/lpf/chatglm

iluvatar_mr
密码：1qazXDR%
sftp -P 29880 iluvatar_mr@iftp.iluvatar.com.cn
智铠软件栈路径： /MR_3.2.0/x86


### 1 数据并行（Data Parallelism）

#### 1.1 DP

1. First, it creates and dispatches copies of the model, one copy per each accelerator.
2. It shards the data and then distributes it to the corresponding devices.
3. It finally aggregates all results together in the backpropagation step.

#### 1.2 DDP

In standard DDP training, every worker processes a separate batch and the gradients are summed across workers using an **all-reduce operation**. While DDP has become very popular, it takes more GPU memory than it needs because the model weights and optimizer states are replicated across all DDP workers.

DDP：the model weights and optimizer states are replicated across all workers

DeepSpeed and FSDP：**sharding a model’s parameters, gradients and optimizer states across data parallel workers** 

#### 1.3 FSDP 

- FSDP：Fully Sharded Data Parallel ，完全分片数据并行
- 学习资源：[讲解1](https://engineering.fb.com/2021/07/15/open-source/fsdp/)、[讲解2](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/)、[Accelerate中使用FSDP](https://huggingface.co/docs/accelerate/usage_guides/fsdp)
- 主要思想：**sharding a model’s parameters, gradients and optimizer states across data parallel workers** 

##### 1.3.1 对比DDP和FSDP

One method to reduce replications is to apply a process called full parameter sharding, where only a subset of the model parameters, gradients, and optimizers needed for a local computation is made available.

减少冗余复制的一种方法是使用一个称为全参数分片的处理，该方法只需要提供局部计算所需的模型参数、梯度和优化器的子集（三者的子集）。

*In standard data parallel training methods, a copy of the model is present on each GPU and a sequence of forward and backward passes are evaluated on only a shard of the data. After these local computations, the parameters and optimizers for each local process are shared with the other GPUs in order to calculate the global weight update.* ***In FSDP, only a shard of the model is present on a GPU.** Then, locally, all weights are gathered from the other GPUs — by means of an all-gather step — to calculate the forward pass. To maximize memory efficiency, we can discard the full weights after each layer s forward pass, saving memory for subsequent layers.This gathering of weights is then performed again before the backward pass. After that backward pass, the local gradients are averaged and sharded across the GPUs by means of a reduce-scatter step, which allows each GPU to update its local weight shard.*

在FSDP中，每个GPU上只保存了模型的一块权重。那么，所有的模型权重通过all-gather操作从其他GPU上收集起来进行当前一层或几层的前向传播计算。由于只有在完成所有层的前向传递计算后，才会开始进行反向传播。因此，在完成当前一层或几层的前向传递之后，为了给后续层计算节省显存，当前一层或几层的模型权重将会被释放（free full weights）。

而在进行反向传播之前，当前一层或几层的模型权重会再次被收集起来，从而进行反向传播。在完成反向传播之后，收集起来的权重会再次被释放（free full weights），然后计算梯度的平均值，最后使用reduce-scatter操作将梯度切分到多个GPU上，这允许每个GPU只更新自己的局部模型权重。

![fsdp_workflow](D:\Typora\Notes\LLM\大模型训练\fsdp_workflow.png)

To further maximize memory efficiency, **FSDP can offload the parameters, gradients and optimizer states to CPUs** when the FSDP instance is not active in the computation.

- FSDP过程伪代码

  ```python
  FSDP forward pass:
      for layer_i in layers:
          all-gather full weights for layer_i
          forward pass for layer_i
          discard full weights for layer_i
  
  FSDP backward pass:
      for layer_i in layers:
          all-gather full weights for layer_i
          backward pass for layer_i
          discard full weights for layer_i
          reduce-scatter gradients for layer_i
  ```

##### 1.3.2 通信操作

we can decompose the **all-reduce** operations in DDP into separate **reduce-scatter** and **all-gather**  operations.

![通信操作](D:\Typora\Notes\LLM\大模型训练\通信操作.png)

*All-reduce as a combination of reduce-scatter and all-gather. The standard all-reduce operation to aggregate gradients can be decomposed into two separate phases: reduce-scatter and all-gather. During the reduce-scatter phase, the gradients are summed in equal blocks among ranks on each GPU based on their rank index. During the all-gather phase, the sharded portion of aggregated gradients available on each GPU are made available to all GPUs* 

- 代码实现：https://zhuanlan.zhihu.com/p/564063463



#### 1.4 Sharded DDP / Zero DP

##### 1.4.1 optimizer states

##### 1.4.2 optimizer states + gradients

##### 1.4.3 optimizer states + gradients + parameters



### 2  流水线模型并行（Pipeline Model Parallelism）

Tensor parallelism can be seen as **intra-layer parallelism** and Pipeline parallelism can be seen as **inter-layer parallelism**.

流水线模型并行：垂直方向的并行，层间并行

- 学习资源：[讲解](https://oneflow2020.medium.com/oneflow-made-training-gpt-3-easier-part-1-5b6b65d70d3c)、[NVIDIA讲解](https://developer.nvidia.com/blog/scaling-language-model-training-to-a-trillion-parameters-using-megatron/)

**Pipeline model parallelism is to partition the entire network into stages**, where each device runs a certain amount of stages, thus enabling the large model to be executed in a way of relay（接力）.

#### 2.1 GPipe

Gpipe splits a **mini-batch** of data into **micro-batches** and feeds it to the device hosting the first shard. The layers on each device process the micro-batches and send the output to the following shard/device. In the meantime it is ready to process the micro batch from the previous shard/device.

GPipe Pipeline parallelism **divides the input mini-batch into smaller micro-batches**, enabling different accelerators to work on different micro-batches simultaneously.

![GPipe](D:\Typora\Notes\LLM\大模型训练\GPipe.png)

其中，第一个下标表示GPU编号，第二个下标表示micro-batch编号。竖着看，可以看到某一时刻有哪些GPU在运行和空置。

![GPipe1](D:\Typora\Notes\LLM\大模型训练\GPipe1.png)

其中数字表示micro-batch编号，权重参数在pipeline flush时更新。

存在的问题：该流水线并行方式会导致显存消耗，因为每个 micro-batch在前向传播计算产生的中间激活被反向传播计算所需要，因此对于每个阶段都需要保存所有micro-batch的前向传递的中间激活值。该问题的解决可以通过引入 Re-computation 技术来缓解。

- checkpointing (recompute，重计算)

  用额外的计算换取显存。即在前向传播时，删除一些暂时用不到的中间激活值以降低内存，在反向传播时，再根据需要临时进行前向计算恢复；

  Generally, tensors calculated in the forward direction will be used in backward pass. Consequently, for specific layers, the forward tensors need to be retained in memory until the end of their corresponding backward computation. This tensor retaining process occupies a significant amount of memory, especially for the layers that are early in the order. Fortunately, the checkpointing method can significantly reduce the time of retaining these forward tensors, thus saving the required memory.

  Re-computation was mainly used in single-device or data parallelism previously. Indeed, it was not specifically designed for pipeline parallelism originally. **But re-computation is critical in pipeline parallelism, because it is unnecessary to cache all forward activations, but only to cache a small number of tensors that are checkpointed.** Therefore, the memory cost can be greatly reduced in pipeline parallelism.

- CPU offload

  CPU offload is a method similar to virtual memory in the computer operating system (swapping the infrequently used memory to disk temporarily thus increasing the capacity of memory). As we know, GPU memory is expensive, with high speed and small capacity. By contrast, host (CPU) memory is cheap, with low speed and large capacity. **The process of Swapping out some activations that are not used in the forward computation temporarily to the CPU host memory, and then swapping them into the GPU device memory when needed in the backward computation can save memory.**

These two methods support memory optimization in different ways: **Checkpointing is a trade off between computation overhead and memory, and CPU offload is a trade off between transmission overhead and memory.**

#### 2.2 PipeDream：1F1B

1F1B（One Forward Pass Followed by One Backward Pass）

The principle of 1F1B is concise: **since the activation in the forward computation can only be released after the completion of corresponding backward computation** (regardless of whether the technique of Checkpointing is used or not), in pipeline parallelism, to reduce the number of buffers of activations as much as possible, the storage time of each activation must be shortened as much as possible. That is, freeing the buffers as early as possible.

![1F1B](D:\Typora\Notes\LLM\大模型训练\1F1B.png)

其中数字表示micro-batch编号，反向传播花费的时间是前向传播的两倍，因此在反向传播时每个数字出现了两次。

1F1B limits the number of in-flight microbatches (the number of microbatches for which the backward pass is outstanding and activations need to be maintained) to the depth of the pipeline, **instead of the number of microbatches in a batch**. After the warm-up phase, each worker then enters steady state, where workers perform **one forward pass followed by one backward pass** (1F1B for short). Finally, at the end of a batch, workers complete backward passes for all remaining in-flight microbatches.

预热阶段：限制 micro-batch 的个数，而非一个mini-batch中所有的micro-bath，显示在图上就只有4个micro-bath，而非8个micro-bath。

稳定阶段：每个worker 执行一个前向传播，然后再执行一个反向传播。

结束阶段：每个worker完成所有剩余micro-batch的反向传播。

So, to complete the backward computation of each micro-batch as early as possible, the priority of backward computation needs to be increased: the micro-batch with a smaller serial number conducts backward computation before the forward computation of micro-batch with larger serial number.

也就是 先执行具有较小编号的micro-batch的反向传播，然后再执行具有较大编号的 micro-batch 的前向传播。

Therefore, if the backward computation of one micro-batch in the last stage starts immediately after its forward computation finished, other stages will start backward computation as early as possible, which exactly follows the principle of 1F1B strategy. 

因此，如果在一个micro-batch完成前向传播计算后就立即在最后一个阶段开始反向传播计算，其他阶段也将尽可能早地开始反向传播计算，这恰好遵循1F1B策略的原理。

From the comparison of the pipeline of 1F1B and GPipe, we can find that 8 pieces of activations need to be cached for backward computation using the method of GPipe, while only 4 activations are needed using 1F1B.

Although **the proportion of idle time of the two is the same**, more layers can be executed and a larger size of micro-batch can be supported with more saved memory, thereby improving the performance.

两种方法具有同样的闲置时长，但是1F1B方法可以更节省显存的方式在一个阶段运行更多的层和更大的micro-batch 。

#### 2.3  Interleaved 1F1B Pipeline



This schedule requires **the number of microbatches in a batch to be an integer multiple of the degree of pipeline parallelism (number of devices in the pipeline)**.For example, with four devices, the number of microbatches in a batch must be a multiple of 4. 比如 mini-batch-size = 256，micro-batch-size=8，则有 256 / 8 = 32，假如一个pipeline使用了4张显卡，则microbatches的个数必须是4的整数倍。

In this schedule, each device can perform computation for multiple subsets of layers(called a model chunk) instead of a single contiguous set of layers. i.e. Before device 1 had layer 1-4; device 2 had layer 5-8; and so on. But now device 1 has layer 1,2,9,10; device 2 has layer 3,4,11,12; and so on. With this scheme, each device in the pipeline is assigned multiple pipeline stages and each pipeline stage has less computation. [来源](https://colossalai.org/docs/features/pipeline_parallel/)

其运行过程如下：先在Device 1 上面运行1,2层，Device 2上面运行3,4层，Device 3 上面运行5,6层，Device 4上面运行7,8层，再次回到Device 1 上面运行9,10层，Device 2上面运行11,12层，Device 3 上面运行13,14层，Device 4上面运行15,16层。在第一个microbatches完成前向传播后，就可以立即进行反向传播，也就是绿色 1 所在的位置。由于相比于原来stage翻倍了，增加了额外的通信，但是 pipeline bubble 的大小却下降了。

In an interleaved pipeline, backward execution of the microbatches is prioritized whenever possible. This allows quicker release of the memory used for activations, using memory more efficiently. It also allows for scaling the number of microbatches higher, reducing the idle time of the GPUs. At steady-state, each device alternates between running forward and backward passes. This means that the backward pass of one microbatch may run before the forward pass of another microbatch finishes.

在交错流水线中，一个microbatch反向传播的执行在任何情况下都是优先的。这使其可以快速释放激活值占用的显存，更加高效地使用显存。它还允许缩放较高的microbatch数量，从而减少GPU的空闲时间。在稳定阶段，每个device交替运行前向和反向传播，这意味一个microbatch的反向传播可能运行在另外一个microbatch的前向传播完成之前。

![交错1F1B](D:\Typora\Notes\LLM\大模型训练\交错1F1B.png)

图中深颜色表示第一阶段，浅颜色表示第二阶段，前向和反向传播中都有深浅两种颜色，图中1，2，3，……分别表示第 1，2，3，……个microbatch。

**对比：**

- GPipe：all-forward, all-backward，has a high memory footprint.
- 1F1B：one-forward，one-backward，has a more memory-efficient footprint.
-  Interleaved 1F1B：one-forward，one-backward， This mode is both memory-efficient and time-efficient.

### 3 张量模型并行 （Tensor Model Parallelism）

张量并行：水平方向的并行

Tensor parallel training is to split a tensor into `N` chunks along a specific dimension and each device only holds `1/N` of the whole tensor while not affecting the correctness of the computation graph. This requires additional communication to make sure that the result is correct.

Tensor model parallelism is to divide a large model tensor into multiple sub-tensors, so that the large model can be computed in parallel.  Bloom模型代码中涉及到了张量模型并行的处理。

![tensor-parallel](D:\Typora\Notes\LLM\经典大模型\tensor-parallel.png)

#### 2.2.1 行并行

#### 2.2.2 列并行































































Slurm Workload Manager：分布式作业调度器（distributed job scheduler）

节省显存的方法：

- FSDP,
- CPU offloading 
- gradient checkpointing

分布式启动方式

- Horovod
- MPI
- Slurm，submitit

模型：

- GPT-J
- GPT-NeoX
- 

代码结构：

```shell
TOKENIZER_PATH="$MAIN_DIR/codegeex/tokenizer/"

CMD="python $MAIN_DIR/codegeex/data/process_pretrain_dataset.py \
        --dataset_path $DATASET_PATH \
        --tokenizer_path $TOKENIZER_PATH \
        --output_prefix $OUTPUT_PATH \
        --language $LANGUAGE \
        --mode pretrain \
        --seq_len 2048"
  
  
# 预训练  
$MAIN_DIR/codegeex/megatron/tools/pretrain_codegeex.py \  
https://github.com/THUDM/CodeGeeX/blob/main/scripts/pretrain_codegeex.sh

# 微调
$MAIN_DIR/codegeex/megatron/tools/pretrain_codegeex.py \
        
        
```

