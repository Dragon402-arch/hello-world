### 大模型微调

- 模型类型
  - Causal Language Modeling (CLM): CausalLM，如GPT
  - Masked Language Modeling (MLM): MaskedLM，如BERT
  - Seq2SeqLM: 如 BART、T5

[介绍](https://www.leewayhertz.com/parameter-efficient-fine-tuning/)

![大模型微调](D:\Typora\Notes\PyTorch\大模型使用\大模型微调.png)

#### 1、Adapter

##### 1.1 主要思想

Adapter组件主要被添加不同模块之间，如在Attention模块和Feed Forward模块之间添加了Adapter组件，从而创建了可供学习的参数。而LoRA则是模块内部的组件中添加了LoRA组件，从而实现对原模型权重的更新。

##### 1.2 示意图

![image-20230704150723770](C:\Users\千江映月\AppData\Roaming\Typora\typora-user-images\image-20230704150723770.png)

#### 2、LoRA（Low-Rank Adaptation ）

[讲解](https://medium.com/serpdotai/lora-low-rank-adaptation-of-large-language-models-82869419b7b8)

使用了低秩矩阵分解技术，

https://sebastianraschka.com/blog/2023/llm-finetuning-lora.html

##### 2.1 示意图



##### 2.2 代码示例

```python
pass
```

##### 2.3 LoRA实现细节

模型代码

```python
class LoraModel(torch.nn.Module):
    def __init__(self, config, model):
        super().__init__()
        self.peft_config = config
        self.model = model
        self._find_and_replace()
        mark_only_lora_as_trainable(self.model, self.peft_config.bias)
        self.forward = self.model.forward
```

其中 :

- `mark_only_lora_as_trainable(self.model, self.peft_config.bias)` 

  作用是将所有非LoRA部分的权重都进行冻结，然后再根据 bias 的配置情况来确定是否对参数中的bias进行学习更新。

- ` self._find_and_replace()`

  实现了根据 LoRA 配置对模型进行修改并添加LoRA层，代码详情如下：

```python
    def _find_and_replace(self):
        is_target_modules_in_base_model = False
        kwargs = {
            "r": self.peft_config.r,
            "lora_alpha": self.peft_config.lora_alpha,
            "lora_dropout": self.peft_config.lora_dropout,
            "fan_in_fan_out": self.peft_config.fan_in_fan_out,
            "merge_weights": self.peft_config.merge_weights or self.peft_config.inference_mode,
        }
        key_list = [key for key, _ in self.model.named_modules()]
        for key in key_list:
            if isinstance(self.peft_config.target_modules, str):
                target_module_found = re.fullmatch(self.peft_config.target_modules, key)
            else:
                target_module_found = any(key.endswith(target_key) for target_key in self.peft_config.target_modules)
            if target_module_found:
                if not is_target_modules_in_base_model:
                    is_target_modules_in_base_model = True
                parent, target, target_name = self._get_submodules(key)
                bias = target.bias is not None
                if isinstance(target, torch.nn.Linear) and self.peft_config.enable_lora is None:
                    # 执行这里，核心模块
                    new_module = Linear(target.in_features, target.out_features, bias=bias, **kwargs)
                # 原始模块替换为添加LoRA后的新模块
                self._replace_module(parent, target_name, new_module, target)
        if not is_target_modules_in_base_model:
            raise ValueError(
                f"Target modules {self.peft_config.target_modules} not found in the base model. "
                f"Please check the target modules and try again."
            )

```

核心代码：` new_module = Linear(target.in_features, target.out_features, bias=bias, **kwargs)`

```python
class Linear(nn.Linear, LoraLayer):
    # Lora implemented in a dense layer
    def __init__(
            self,
            in_features: int,
            out_features: int,
            r: int = 0,
            lora_alpha: int = 1,
            lora_dropout: float = 0.0,
            fan_in_fan_out: bool = False,
            # Set this to True if the layer to replace stores weight like (fan_in, fan_out)
            merge_weights: bool = True,
            **kwargs,
    ):
        # 继承了两个类的初始化方法
        nn.Linear.__init__(self, in_features, out_features, **kwargs)
        LoraLayer.__init__(self, r=r, lora_alpha=lora_alpha, lora_dropout=lora_dropout, merge_weights=merge_weights)

        self.fan_in_fan_out = fan_in_fan_out
        # Actual trainable parameters
        if r > 0:
            self.lora_A = nn.Linear(in_features, r, bias=False)
            self.lora_B = nn.Linear(r, out_features, bias=False)
            self.scaling = self.lora_alpha / self.r
            # Freezing the pre-trained weight matrix
            self.weight.requires_grad = False

    def train(self, mode: bool = True):
        nn.Linear.train(self, mode)
        self.lora_A.train(mode)
        self.lora_B.train(mode)
        if not mode and self.merge_weights and not self.merged:# 执行权重合并操作，self.merged为权重合并状态的记录，此时为False
            # Merge the weights and mark it
            # 推理时这行这里
            if self.r > 0:
                self.weight.data += (
                        transpose(self.lora_B.weight @ self.lora_A.weight, self.fan_in_fan_out) * self.scaling
                )
            self.merged = True
        elif self.merge_weights and self.merged: # 已经添加了lora权重，此时想要去除
            # Make sure that the weights are not merged
            if self.r > 0:
                self.weight.data -= (
                        transpose(self.lora_B.weight @ self.lora_A.weight, self.fan_in_fan_out) * self.scaling
                )
            self.merged = False

    def eval(self):
        nn.Linear.eval(self)
        self.lora_A.eval()
        self.lora_B.eval()

    def forward(self, x: torch.Tensor):
        if self.disable_adapters:# 添加了lora权重，此时想要去除
            if self.r > 0 and self.merged:
                self.weight.data -= (
                        transpose(self.lora_B.weight @ self.lora_A.weight, self.fan_in_fan_out) * self.scaling
                )
                self.merged = False
            return F.linear(x, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)
        elif self.r > 0 and not self.merged: # 需要添加lora权重，此时尚未添加，执行添加操作
            # 模型训练时执行这里
            result = F.linear(x, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)
            # y = xA^T + b，其中self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
            if self.r > 0:
                result += self.lora_B(self.lora_A(self.lora_dropout(x))) * self.scaling
            return result
        else:# 已经添加了lora权重，推理时执行
            return F.linear(x, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)

```

###### 2.3.1 使用transformers权重情形

​	即使用 `model = PeftModel.from_pretrained(model, lora_model_path).half().cuda()` 加载权重

- 训练时

  执行

  ```python
  def forward(self, x: torch.Tensor):
      elif self.r > 0 and not self.merged: # 需要添加lora权重，此时尚未添加，执行添加操作
          # 模型训练时执行这里
          result = F.linear(x, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)
         # y = xA^T + b，其中self.weight = Parameter(torch.empty((out_features, in_features))
          if self.r > 0:
              result += self.lora_B(self.lora_A(self.lora_dropout(x))) * self.scaling
              return result
  ```

- 推理时

  首先执行

  ```python
  def _find_and_replace(self):
      kwargs = {
          "r": self.peft_config.r,
          "lora_alpha": self.peft_config.lora_alpha,
          "lora_dropout": self.peft_config.lora_dropout,
          "fan_in_fan_out": self.peft_config.fan_in_fan_out,
          "merge_weights": self.peft_config.merge_weights or self.peft_config.inference_mode,
      }
      # merge_weights在推理时由False变为了True
      new_module = Linear(target.in_features, target.out_features, bias=bias, **kwargs)
      
  ```

  其次执行

  ```python
  
  def train(self, mode: bool = True):
      if not mode and self.merge_weights and not self.merged:# self.merged 默认为False
          # Merge the weights and mark it
          # 推理时这行这里
          if self.r > 0:
              self.weight.data += (
               transpose(self.lora_B.weight @ self.lora_A.weight, self.fan_in_fan_out) * self.scaling
              )
          self.merged = True
  ```

  然后执行

  ```python
  def forward(self, x: torch.Tensor):
      if self.disable_adapters:
          pass
      # 由于在执行过上一步的操作之后，self.merged由False变为True了。
      elif self.r > 0 and not self.merged:
          pass
      else:
          # 执行这里，
          return F.linear(x, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)
  ```

###### 2.3.2 使用torch权重情形

```python
model = get_peft_model(model, lora_config)
model.load_state_dict(torch.load(lora_model_path), strict=False)


"""
"CAUSAL_LM": PeftModelForCausalLM
"""

def get_peft_model(model, peft_config):
    if not isinstance(peft_config, PromptLearningConfig):
        peft_config = _prepare_lora_config(peft_config, model_config)
        
def _prepare_lora_config(peft_config, model_config):
    if peft_config.inference_mode:
        peft_config.merge_weights = True
        
        
class PeftModelForCausalLM(PeftModel):
	pass

class PeftModel(PushToHubMixin, torch.nn.Module):
   def __init__(self, model, peft_config: PeftConfig):
        super().__init__()
        self.peft_config = peft_config
        self.base_model = model
        self.config = self.base_model.config
        self.modules_to_save = None
        if isinstance(self.peft_config, PromptLearningConfig):
            self._setup_prompt_encoder()
        else:# 执行这里
            self.base_model = LoraModel(peft_config, model)
        if getattr(self.peft_config, "modules_to_save", None) is not None:
            self.modules_to_save = self.peft_config.modules_to_save
            _set_trainable(self)
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# 剩下的部分与model = PeftModel.from_pretrained(model, lora_model_path).half().cuda()情形基本相同
```

##### 2.4 AdaLoRA 

AdaLoRA，参数高效微调的自适应预算分配，是对LoRA的一种改进。

#### 3、Prefix tuning

##### 3.1 主要思想

Prefix-tuning keeps the language model parameters frozen and optimizes a small continuous task-specific vector called the prefix. In prefix-tuning, the prefix is a set of free parameters that are trained along with the language model. The goal of prefix-tuning is to find a context that steers the language model toward generating text that solves a particular task.

**When using prefix tuning, only the prefixes are updated, while the rest of the layers are fixed and not updated.**

##### 3.2 示意图

![image-20230704132337265](C:\Users\千江映月\AppData\Roaming\Typora\typora-user-images\image-20230704132337265.png)

##### 3.3 代码示例

[官方代码示例](https://huggingface.co/docs/peft/task_guides/seq2seq-prefix-tuning)

```python
from peft import get_peft_model, TaskType, PrefixTuningConfig

from transformers import AutoModel

peft_config = PrefixTuningConfig(
    task_type=TaskType.SEQ_2_SEQ_LM, 
    inference_mode=False, 
    num_virtual_tokens=32,
    prefix_projection=False
)

model = AutoModel.from_pretrained(pretrained_model_path, trust_remote_code=True).half()
model = get_peft_model(model, peft_config).cuda()

"""
模型可调参数：

PrefixEncoder(
  (embedding): Embedding(32, 229376) num_virtual_tokens, num_layers * 2 * hidden_size
)

past_key_values:(num_layers, 2, pre_seq_len, hidden_size)
"""
```

#### 4、Prompt tuning

##### 4.1 主要思想

**Prompt tuning is a simpler variant of prefix tuning.** In it, some vectors are prepended at the beginning of a sequence at the input layer. **When presented with an input sentence, the embedding layer converts each token into its corresponding word embedding, and the prefix embeddings are prepended to the sequence of token embeddings.** Next, the pre-trained transformer layers will process the embedding sequence like a transformer model does to a normal sequence. **Only the prefix embeddings are adjusted during the fine-tuning process, while the rest of the transformer model is kept frozen and unchanged.**

##### 4.2 示意图

![image-20230704133850532](C:\Users\千江映月\AppData\Roaming\Typora\typora-user-images\image-20230704133850532.png)

##### 4.3 代码示例

[官方示例](https://huggingface.co/docs/peft/task_guides/clm-prompt-tuning)

**训练**

```python
from peft import get_peft_config, get_peft_model, PromptTuningInit, PromptTuningConfig, TaskType, PeftType
from transformers import AutoModel

peft_config = PromptTuningConfig(
    task_type=TaskType.CAUSAL_LM,
    prompt_tuning_init=PromptTuningInit.TEXT,
    num_virtual_tokens=8,
    prompt_tuning_init_text="Classify if the tweet is a complaint or not:",
    tokenizer_name_or_path=model_name_or_path,
)
model = AutoModel.from_pretrained(model_name_or_path).half()
model = get_peft_model(model, peft_config).cuda()

```

**推理**

```python
from peft import PeftModel, PeftConfig

peft_model_id = "stevhliu/bloomz-560m_PROMPT_TUNING_CAUSAL_LM"

config = PeftConfig.from_pretrained(peft_model_id)
model = AutoModel.from_pretrained(config.base_model_name_or_path)
model = PeftModel.from_pretrained(model, peft_model_id)
```



#### 5、P-Tuning v1

##### 5.1 主要思想

不再人工设计硬模板，而是在输入端直接插入若干可被优化的 Pseudo Prompt Tokens，通过学习对应的嵌入表示来寻找效果较好的prompt；直接优化prompt对应的token表示，而非prompt tokens。

主要用于NLU任务上；使用prompt encoder（由一个双向的LSTM+两层MLP组成）建模 pseudo-token 直接的依赖关系。

##### 5.2 核心代码

使用示例

```python
# https://huggingface.co/docs/peft/task_guides/ptuning-seq-classification
from peft import PromptEncoderConfig,get_peft_model
from transformers import BertForSequenceClassification

peft_config = PromptEncoderConfig(task_type="SEQ_CLS", num_virtual_tokens=32, encoder_hidden_size=128)
model = BertForSequenceClassification.from_pretrained("/home/lis/algProjects/pretrained_models/chinese_wwm_ext_pytorch/")
model = get_peft_model(model, peft_config).cuda()
print(model)
```

**不添加 anchor tokens 情况**

**代码实现细节**

给定模板template （模板长度为 m，也就是包含 m 个 token）
$$
T= {\{[P_0: P_i ],x, [P_{i+1}:P_m],y\}}
$$
P-tuning regards the [$P_i$] as pseudo tokens and map the template to
$$
\{{h_0,…,h_i,e(x),h_{i+1},…,h_m,e(y)}\}
$$


[代码来源：苏剑林](https://github.com/bojone/P-tuning/blob/main/gpt.py#L46)

[理论讲解](https://spaces.ac.cn/archives/8295/comment-page-1)

```python
# P_0:P_i,e(x),P_i+1:P_m,e(y)

# 
if model_name = "gpt":
	# 对应的任务描述
    desc = ['[unused%s]' % i for i in range(1, 9)]
    desc_ids = [tokenizer.token_to_id(t) for t in desc]
    
	token ids, segment ids = tokenizer.encode(text, maxlen=maxlen)
    token_ids = token_ids[:1] + desc_ids[:4] + token_ids[:-1]
    token_ids = token_ids + desc_ids[4:]
    if label == 0:
        token_ids = token_ids + [neg_id]
    elif label == 1:
        token_ids = token_ids + [pos_id]
        
# [CLS] P_0:P_i,[MASK],P_i+1:P_m,e(x),[SEP]
elif model_name = "bert":
    # 对应的任务描述
    mask_idx = 5
    desc = ['[unused%s]' % i for i in range(1, 9)]
    desc.insert(mask_idx - 1, '[MASK]')
    desc_ids = [tokenizer.token_to_id(t) for t in desc]
    token_ids = token_ids[:1] + desc_ids + token_ids[1:]

```

[官方源码](https://github.com/THUDM/P-tuning/blob/main/LAMA/p_tuning/modeling.py)

```python
class PTuneForLAMA(torch.nn.Module):    
    self.pseudo_token_id = self.tokenizer.get_vocab()[self.args.pseudo_token]
    self.prompt_encoder = PromptEncoder(self.template, self.hidden_size, self.tokenizer, self.device, args)
    # template 默认(3,3,3)
    if 'gpt' in self.args.model_name or 'megatron' in self.args.model_name:
            template = (template[0], template[1], 0)
    self.template = template # 默认(3,3,3)
    self.spell_length = sum(self.template)

    def embed_input(self, queries):
        batch_size = queries.shape[0]
        queries_for_embedding = queries.clone()
        # 将 pseudo_token_id 替换为 unk_token_id
        queries_for_embedding[(queries == self.pseudo_token_id)] = self.tokenizer.unk_token_id
        raw_embeds = self.embeddings(queries_for_embedding)
		
        # 以下部分主要完成将P_0:P_m 的embeddings替换为prompt_encoder的返回值
        
        blocked_indices = (queries == self.pseudo_token_id).nonzero().reshape((bz, self.spell_length, 2))[:, :, 1]  # bz
        replace_embeds = self.prompt_encoder()
        for batch_idx in range(batch_size):
            for i in range(self.prompt_encoder.spell_length):
                raw_embeds[batch_idx, blocked_indices[batch_idx, i], :] = replace_embeds[i, :]
        return raw_embeds

   def get_query(self, x_h, prompt_tokens, x_t=None):
        
        # [CLS] P_0:P_i,[MASK],P_i+1:P_m,e(x),[SEP]
        if 'gpt' not in self.args.model_name and 'megatron' not in self.args.model_name:
            # BERT-style model
            return [[self.tokenizer.cls_token_id]  # [CLS]
                    + prompt_tokens * self.template[0]
                    + [self.tokenizer.mask_token_id]  # head entity
                    + prompt_tokens * self.template[1]
                    + self.tokenizer.convert_tokens_to_ids(self.tokenizer.tokenize(' ' + x_h))  # [MASK] (tail entity)
                    + (prompt_tokens * self.template[2] if self.template[
                                                               2] > 0 else self.tokenizer.convert_tokens_to_ids(['.']))
                    + [self.tokenizer.sep_token_id]
                    ]
        # P_0:P_i,e(x),P_i+1:P_m,e(y)
        elif 'gpt' in self.args.model_name or 'megatron' in self.args.model_name:
            # GPT-style models
            return [prompt_tokens * self.template[0]
                    + self.tokenizer.convert_tokens_to_ids(self.tokenizer.tokenize(' ' + x_h))  # head entity
                    + prompt_tokens * self.template[1]
                    + (self.tokenizer.convert_tokens_to_ids(
                self.tokenizer.tokenize(' ' + x_t)) if x_t is not None else [])
                    ]


    def forward(self, x_hs, x_ts, return_candidates=False):
        bz = len(x_hs)

        # construct query ids
        prompt_tokens = [self.pseudo_token_id]
        
        queries = [torch.LongTensor(self.get_query(x_hs[i], prompt_tokens)).squeeze(0) for i in range(bz)]
        # 填充到batch内部的最大长度
        queries = pad_sequence(queries, True, padding_value=self.pad_token_id).long().to(self.device)

        # construct label ids
        label_ids = torch.LongTensor(self.tokenizer.convert_tokens_to_ids(x_ts)).reshape(
            (bz, -1)).to(self.device)
        
        # 生成 mask 
        attention_mask = queries != self.pad_token_id

        # get embedded input
        inputs_embeds = self.embed_input(queries)
        return self.base_model(inputs_embeds=inputs_embeds, **kwargs)


```

**peft包实现**

```python
class PeftModelForSequenceClassification(PeftModel):
   	def forward(
        self,
        input_ids=None,
        attention_mask=None,
        inputs_embeds=None,
        labels=None,
        output_attentions=None,
        output_hidden_states=None,
        return_dict=None,
        **kwargs,
    ):
        batch_size = input_ids.shape[0]
        if attention_mask is not None:
            # concat prompt attention mask
            prefix_attention_mask = torch.ones(batch_size, self.peft_config.num_virtual_tokens).to(self.device)
            attention_mask = torch.cat((prefix_attention_mask, attention_mask), dim=1)
        if kwargs.get("position_ids", None) is not None:
            warnings.warn("Position ids are not supported for parameter efficient tuning. Ignoring position ids.")
            kwargs["position_ids"] = None
        kwargs.update(
            {
                "attention_mask": attention_mask,
                "labels": labels,
                "output_attentions": output_attentions,
                "output_hidden_states": output_hidden_states,
                "return_dict": return_dict,
            }
        )
        if kwargs.get("token_type_ids", None) is not None:
                kwargs["token_type_ids"] = torch.cat(
                    (
                        torch.zeros(batch_size, self.peft_config.num_virtual_tokens).to(self.device),
                        kwargs["token_type_ids"],
                    ),
                    dim=1,
                ).long()
        if inputs_embeds is None:
            inputs_embeds = self.word_embeddings(input_ids)
        prompts = self.get_prompt(batch_size=batch_size)
        prompts = prompts.to(inputs_embeds.dtype)
        # 取P-Tuning的一种特殊情况，将所有的Prompt表示都添加在了序列的左边了
        inputs_embeds = torch.cat((prompts, inputs_embeds), dim=1)
        return self.base_model(inputs_embeds=inputs_embeds, **kwargs)

```

PromptEncoder：

```python
import torch
import torch.nn as nn


class PromptEncoder(torch.nn.Module):
    def __init__(self, template, hidden_size, tokenizer, device, args):
        super().__init__()
        self.device = device
        self.spell_length = sum(template)
        self.hidden_size = hidden_size
        self.tokenizer = tokenizer
        self.args = args
        # ent embedding
        self.cloze_length = template
        self.cloze_mask = [
            [1] * self.cloze_length[0]  # first cloze
            + [1] * self.cloze_length[1]  # second cloze
            + [1] * self.cloze_length[2]  # third cloze
        ]
        self.cloze_mask = torch.LongTensor(self.cloze_mask).bool().to(self.device)

        self.seq_indices = torch.LongTensor(list(range(len(self.cloze_mask[0])))).to(self.device)
        # embedding
        self.embedding = torch.nn.Embedding(len(self.cloze_mask[0]), self.hidden_size).to(self.device)
        # LSTM
        self.lstm_head = torch.nn.LSTM(input_size=self.hidden_size,
                                       hidden_size=self.hidden_size // 2,
                                       num_layers=2,
                                       dropout=self.args.lstm_dropout,
                                       bidirectional=True,
                                       batch_first=True)
        self.mlp_head = nn.Sequential(nn.Linear(self.hidden_size, self.hidden_size),
                                      nn.ReLU(),
                                      nn.Linear(self.hidden_size, self.hidden_size))
        print("init prompt encoder...")

    def forward(self):
        input_embeds = self.embedding(self.seq_indices).unsqueeze(0)
        output_embeds = self.mlp_head(self.lstm_head(input_embeds)[0]).squeeze()
        return output_embeds
```





**总结**

- P-Tuning v1 与Prompt Tuning 实现比较相似
- P-Tuning v2 与Prefix Tuning 实现比较相似

#### 6、P-Tuning v2

[介绍](https://mp.weixin.qq.com/s/jFIEZ8f029x1HYzdD3v_Mg)

##### 6.1 主要思想

主要思想以及代码实现与Prefix Tuning方法几乎完全一致。

##### 6.2 核心代码

```python
class PrefixEncoder(torch.nn.Module):
    """
    The torch.nn model to encode the prefix
    Input shape: (batch-size, prefix-length)
    Output shape: (batch-size, prefix-length, 2*layers*hidden)
    """
    def __init__(self, config):
        super().__init__()
        self.prefix_projection = config.prefix_projection
        if self.prefix_projection:
            # 这里！！
            self.embedding = torch.nn.Embedding(config.pre_seq_len, config.hidden_size)
            self.trans = torch.nn.Sequential(
                torch.nn.Linear(config.hidden_size, config.hidden_size),
                torch.nn.Tanh(),
                torch.nn.Linear(config.hidden_size, config.num_layers * config.hidden_size * 2)
            )
        else:
            self.embedding = torch.nn.Embedding(config.pre_seq_len, config.num_layers * config.hidden_size * 2)

    def forward(self, prefix: torch.Tensor):
        if self.prefix_projection:
            prefix_tokens = self.embedding(prefix)
            past_key_values = self.trans(prefix_tokens)
        else:
            past_key_values = self.embedding(prefix)
        return past_key_values
```



[官方代码示例](https://github.com/THUDM/ChatGLM-6B/blob/main/ptuning/README.md)

[代码示例](https://github.com/liucongg/ChatGLM-Finetuning/blob/master/finetuning_pt.py)

```python
config = ChatGLMConfig.from_pretrained(args.model_dir)
config.pre_seq_len = args.pre_seq_len
config.prefix_projection = args.prefix_projection

model = ChatGLMForConditionalGeneration.from_pretrained(args.model_dir, config=config)

for name, param in model.named_parameters():
    if not any(nd in name for nd in ["prefix_encoder"]):
        param.requires_grad = False
```



#### 7、Freeze

##### 7.1 主要思想

Freeze方法，即参数冻结，对原始模型部分参数进行冻结操作，仅训练部分参数，从而实现对大模型的训练。

##### 7.2 核心代码

[代码示例](https://github.com/liucongg/ChatGLM-Finetuning/blob/master/finetuning_freeze.py)

```python
for name, param in model.named_parameters():
    if not any(nd in name for nd in ["layers.27", "layers.26", "layers.25", "layers.24", "layers.23"]):
        param.requires_grad = False
```



#### 8、待办事项

- LoRA 实现代码细节

- P-Tuning v1实现细节

- ChatGLM模型生成输出实现代码细节

  
