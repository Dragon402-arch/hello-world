#### Half Precision：半精度

- 半精度介绍与使用意义

  在神经网络中，所有的计算通常（默认）以单精度（**single precision**）完成，这意味着网络中所有输入、激活、权重的数组中的所有浮点数都是以32位浮点数（**FP32**）表示的。一个减少内存使用的想法是尝试以半精度（**half precision**）做同样的事情，此时可以使用16位浮点数（**FP16**）。根据定义，它们占用RAM中一半的空间，理论上可以让您将模型大小和批处理大小增加一倍。

  - FP16 vs FP32

    |                  | sign | exponent | digits |
    | ---------------- | ---- | -------- | ------ |
    | FP16（float16）  | 1    | 5        | 10     |
    | BF16（bfloat16） | 1    | 8        | 7      |
    | FP32（float32）  | 1    | 8        | 23     |

  - 好处：
    - 加速计算，加快训练速度
    - 降低存储空间占用
  - 坏处：
    - 数值表示的精度有所下降

- 训练时可能出现的情形

  - 权重更新规则：`w = w - lr * w.grad ` ，在以半精度数值表示的情况下进行上面的运算，由于 ` w.grad`  的数值比权重 w 低几个数量级，同时学习率也很小，可能会造成权重没有更新，比如w=1 和  lr*w.grad =0.0001（甚至更小），此时权重 w 将不会进行任何更新（原因是计算后的值超出了FP16的表示范围）。
  - 梯度下溢（gradient underflow）In FP16, your gradients can easily be replaced by 0 because they are too low.
  - 激活值或损失值上溢（ loss overflow）：使用半精度表示，梯度很容易达到 nan 或无穷大，训练很容易发散（diverge）

  上述问题的解决方案就是使用 **混合精度训练 （mixed precision training）**。

#### Automatic Mixed Precision (Amp) training with PyTorch：自动混合精度训练

- 针对半精度训练时出现的第一个问题，大部分运算仍然使用FP16完成，少部分运算使用FP32完成。其主要思想是 前向传递和梯度计算以FP16表示进行（计算速度更快），而权重更新部分则使用FP32表示（更加准确）。训练过程转换如下：
  1. compute the output with the FP16 model, then the loss  计算FP16模型的FP32输出并计算FP32的损失值
  2. back-propagate the gradients in half-precision. 以FP16表示反向传播梯度，得到每个w 的梯度值
  3. copy the gradients in FP32 precision  将计算得到的FP16表示的梯度值转换为 FP32表示的，此时没有精度损失。
  4. do the update on the master model (in FP32 precision)  以FP32表示更新模型权重w，也就是进行 `w = w - lr * w.grad` 该运算
  5. copy the master model in the FP16 model.  将更新后的模型权值从FP32表示转换为FP16表示。 注意这一步转换有精度损失。

- 针对半精度训练时出现的第二个问题，可以梯度缩放（**gradient scaling**）的方法来解决。 为了避免使用FP16表示将梯度归零，可以先将 loss  乘上一个 loss_scale （比如 128 ）值，缩放后计算得到的梯度值使用FP16表示不会再被归零。由于不希望 loss_scale 参与到权重更新计算中，执行完上面的第三步之后，再除以 loss_scale 得到真实的梯度值，此时的梯度值以FP32表示，就可以进行权重更新了。整个变换过程如下：
  1. compute the output with the FP16 model, then the loss. 以FP16表示 计算模型的输出以及损失值
  2. multiply the loss by scale then back-propagate the gradients in half-precision. 损失值（FP16精度）乘上 loss_scale 进行反向传播，得到每个w 的梯度值
  3. copy the gradients in FP32 precision then divide them by scale. 先将梯度值转换为 FP32 精度，再除以 loss_scale
  4. do the update on the master model (in FP32 precision). 更新模型的每个权重值（FP32精度）
  5. copy the master model in the FP16 model. 从FP32 精度转换为 FP16 精度

- 针对半精度训练时出现的第三个问题，可以将部分层如 batchnorm layers 使用 FP32精度进行计算，同时将模型最后一层的输出值转换为FP32精度表示，从而以FP32精度计算损失值。compute the loss in FP32  (which means converting the last output of the model in FP32 before passing it to the loss).

  上面的所有解决方案叠加在一起后，整个转换过程如下：

  ![image-20230413153942247](C:\Users\千江映月\AppData\Roaming\Typora\typora-user-images\image-20230413153942247.png)

- 转换过程：

  1. Forward Pass：**模型最后一层输出值FP32精度表示**
  2. Loss Scaling ： loss 值乘以 loss_scale
  3. Backprop：损失值求导得到 FP16精度表示的梯度值（放大），因为模型权重以FP16精度表示
  4. Copy：FP16精度表示的梯度值转换FP32精度表示的梯度值
  5. Remove scale ; gradient clipping  FP32精度表示的梯度值除以 loss_scale（恢复），如果梯度值过大还可以对其进行剪切。
  6. Apply：update weights  使用P32精度表示的梯度值更新模型的每个权重
  7. Copy：FP32精度表示的权重值转换FP16精度表示的权重值

- Dynamic loss scaling

  首先选取一个较大的 loss_scale 值如128，如果造成梯度或是损失值 上溢，则将loss_scale 值 减半变为 64，如果仍然出现上溢，继续减半，直到不再上溢。

  随着训练的进行，loss_scale 值还需要随之动态调整，如果还是太高，每次溢出就减半。然而，一段时间后，训练将会收敛，梯度将开始变小，所以我们需要一种机制来动态扩大loss_scale。每次我们有给定数量的迭代而不会溢出时，将loss_scale 乘以2，也就是loss_scale值进行翻倍。

  [**以上内容来源**](https://docs.fast.ai/callback.fp16.html)

  [Medium讲解](https://towardsdatascience.com/understanding-mixed-precision-training-4b246679c7c4)

- Mixed precision training using Pytorch’s `autocast` and `GradScaler`

  ```python
  scaler = GradScaler()
  
  for epoch in epochs:
      for i, (input, target) in enumerate(data):
          with autocast("cuda"):
              output = model(input)
              loss = loss_fn(output, target)
              # normalize the loss 
              loss = loss / gradient_accumulation_steps
  
          # Accumulates scaled gradients.
          scaler.scale(loss).backward()
            # weights update
          if (i + 1) % gradient_accumulation_steps == 0:
              # may unscale_ here if desired 
               # Scales loss.  Calls backward() on scaled loss to create scaled gradients.
              scaler.unscale_(optimizer)
              torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)
              
           # scaler.step() first unscales gradients of the optimizer's params.
          # If gradients don't contain infs/NaNs, optimizer.step() is then called,
          # otherwise, optimizer.step() is skipped.
              scaler.step(optimizer)
               # Updates the scale for next iteration.
              scaler.update()
              optimizer.zero_grad()
  ```
  
  [代码示例](https://pytorch.org/docs/stable/notes/amp_examples.html#working-with-unscaled-gradients)

