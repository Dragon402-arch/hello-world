### Model Inference & Deploy 模型推理与部署

学习资源：

模型部署入门教程（一）：模型部署简介 https://zhuanlan.zhihu.com/p/477743341

模型部署入门教程（二）：解决模型部署中的难题 https://zhuanlan.zhihu.com/p/479290520

模型部署入门教程（三）：PyTorch 转 ONNX 详解 https://zhuanlan.zhihu.com/p/498425043

#### 1. PyTorch to TorchScript Module

讲解文章：[讲解1](https://towardsdatascience.com/pytorch-jit-and-torchscript-c2a77bac0fff)、[torch官方讲解](https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html#mixing-scripting-and-tracing)、

PyTorch生态系统支持2种独立的模型来应对研究（research）和生产（production）环境。

-  **Eager mode**：用于训练和实验，方便进行研究

- **Script mode**：用于生产环境，包含两个组件：PyTorch JIT 与 TorchScript。

  - **PyTorch JIT**：JIT（just-in-time）

    PyTorch JIT is an optimized compiler for PyTorch programs.

    1. It is a lightweight threadsafe interpreter
    2. Supports easy to write custom transformations
    3. It’s not just for inference as it has auto diff support

  - **TorchScript**

    TorchScript is a **static** high-performance subset of Python language, specialized for ML applications.
    
    The script mode works by utilizing **PyTorch JIT** and **TorchScript**.Script mode is invoked by either `torch.jit.trace` or `torch.jit.script`.
    
  - **JIT Trace**

    `torch.jit.trace` take a data instance and your trained module as input. The tracer runs the supplied module and records the tensor operations performed. This recording is turned into a **TorchScript module**.

    相关代码：返回一个 ScriptModule（torch.jit.ScriptModule ）

    ```python
    from transformers import BertForSequenceClassification
    
    model = BertForSequenceClassification.from_pretrained("./bert_model")
    model.eval()
    
    input_ids = torch.randint(1,10000,(2,128))
    traced_model_gpu = torch.jit.trace(model.cuda(), [input_ids.cuda()])
    ```
    
  - **JIT Script**
  
    `torch.jit.script` allows you to write your code directly into TorchScript. It's more verbose but it more versatile and with a little tweaking can support the majority of the PyTorch models.
  
    ```python
    from transformers import BertForSequenceClassification
    
    model = BertForSequenceClassification.from_pretrained("./bert_model")
    model.eval()
    
    input_ids = torch.randint(1,10000,(2,128))
    script_model_gpu = torch.jit.script(model.cuda(), (input_ids.cuda()))
    ```
  
  - save & load TorchScript modules
  
    ```python
    # Saving a module 
    torch.jit.save(traced_model,’traced_bert.pt’)
    # Loading a module 
    model = torch.jit.load('traced_bert.pt')
    
    # https://developer.nvidia.com/blog/accelerating-inference-up-to-6x-faster-in-pytorch-with-torch-tensorrt/
    ```
  
    

#### 2. PyTorch to ONNX 

ONNX（Open Neural Network Exchange），用于标准描述计算图的一种格式。

[Netron](https://netron.app/) ：查看ONNX 模型算子图

Exporting a model in PyTorch works via **tracing or scripting**. This tutorial will use as an example a model exported by tracing. To export a model, we call the `torch.onnx.export()` function. This will execute the model, recording a trace of what operators are used to compute the outputs. 

B站教程

- [视频](https://www.bilibili.com/video/BV1Lh411u7WL/?spm_id_from=333.337.search-card.all.click)
- [文档](https://www.aiexplorer.blog/article/pyday10)

#### 3. ONNX Model Inference

基于 ONNX 模型进行推理加速的推理框架或推理引擎有：ONNX Runtime （MicroSoft）、TensorRT（Nvidia）、OpenVINO （Intel），

##### 3.1 ONNX Runtime

ONNX Runtime，微软维护，跨平台推理加速器，支持不同操作系统、CPU架构、编程语言（API）

[参考](https://www.cnblogs.com/silence-gtx/p/15509545.html)

[官方教程](https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html)

ONNX依赖要求：https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html#requirements

- 使用推理引擎ONNX Runtime （运行环境）对ONNX模型进行推理

- ONNX 版本与PyTorch版本的适配关系，ONNX Runtime 与 CUDA版本的适配关系

- 核心概念

  - operator 算子

  - operator set 算子集

核心代码：

```python
dynamic_axes_dict = {
    "input_ids": {0: "batch_size", 1: "seq_length"},  # 0, 1分别代表axis 0和axis 1
    "attention_mask": {0: "batch_size", 1: "seq_length"},  # [0,1]
    "token_type_ids": {0: "batch_size", 1: "seq_length"},
    "output": {0: "batch_size"},
}  # 输出并不涉及长度维度

"""
dynamic_axes_dict = {
    "input_ids": [0,1],  # 0, 1分别代表axis 0和axis 1
    "attention_mask": [0,1],  # [0,1]
    "token_type_ids": [0,1],
    "output": [0],
}  # 输出并不涉及长度维度
"""


# Export the model
torch.onnx.export(
    model,  # model being run
    (
        input_ids,
        attention_mask,
        token_type_ids,
    ),  # model input (or a tuple for multiple inputs)
    "bert_model.onnx",  # where to save the model (can be a file or file-like object)
    export_params=True,  # store the trained parameter weights inside the model file
    opset_version=10,  # the ONNX Operator Set version to export the model to # 此处有坑，必须指定≥10，否则会报错
    do_constant_folding=True,  # whether to execute constant folding for optimization
    input_names=[
        "input_ids",
        "attention_mask",
        "token_type_ids",
    ],  # the model's input names
    output_names=["output"],  # the model's output names
    dynamic_axes=dynamic_axes_dict,
)
```

##### 3.2 TensorRT

[讲解视频](https://www.bilibili.com/video/BV1Tx4y1F768/?spm_id_from=autoNext&vd_source=390d16c51af19211c9d56a848f7c3fee)，[讲解文章](https://deci.ai/blog/tensorrt-framework-overview/)

TensorRT 内含推理优化器（TensorRT Optimizer）和运行时环境（Optimized Inference Engine），使模型能以更高吞吐量和更低延迟运行。使用推理引擎TensorRT对ONNX模型进行推理，分为两个阶段：

###### 3.2.1 构建阶段（build phrase）

构建阶段（build phrase）,对ONNX模型进行转换和优化，输出优化后的模型

构建基本命令：

```shell
trtexec --onnx=bert_model.onnx --saveEngine=bert_model.plan# (后缀随意)
```

批处理命令：

```shell
trtexec --onnx=bert_model.onnx \
		--saveEngine=bert_model.plan \
		--minShapes=input:1x3x224x224 \
		--optShapes=input:8x3x224x224 \
		--maxShapes=input:32x3x224x224 \
		--workspace=1024
```

量化命令：

```shell
# FP16
trtexec --onnx=bert_model.onnx \
		--saveEngine=bert_model.plan \
		--minShapes=input:1x3x224x224 \
		--optShapes=input:8x3x224x224 \
		--maxShapes=input:32x3x224x224 \
		--workspace=1024 \
		--fp16

# 8-bit
trtexec --onnx=bert_model.onnx \
		--saveEngine=bert_model.plan \
		--minShapes=input:1x3x224x224 \
		--optShapes=input:8x3x224x224 \
		--maxShapes=input:32x3x224x224 \
		--workspace=1024 \
		--int8 \
		--calib=calibration.cache
```

- 量化类型：
  - Post-training quantization (PTQ)，训练后量化
  - Quantization-aware training (QAT)，训练时量化

###### 3.2.2 运行阶段（runtime phrase）

运行阶段（runtime phrase），加载优化后模型，执行推理

##### 3.3 OpenVINO































