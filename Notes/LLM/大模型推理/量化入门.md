### 量化基础

Quantization refers to the process of reducing the number of bits that represent a number. 

**二进制表示基础**：以INT8表示为例，int8占1个字节(byte) 也就是8个二进制位(bit)，每个二进制位可以存储 0 和 1 两个数 ，8个二进制位就有2^8 = 256种组合(可以存储256个数)，int8最高位是符号位，所以正数和负数都有128个数，也就是正数最大为+127，负数最小为-127，但是这样的话就会出现两个0，也就是 +0和-0，所以才将-0（二进制表示：1000，0000）规定为-128，所以范围为-128到+127（其中+0表示0）。

假定需要进行量化的数据取值范围在 $[\beta,\alpha]$ 内，且 $b$ 表示符号整数表示（signed integer representation）的位宽（bit-width），均匀量化的目标是将 $[\beta,\alpha]$ 范围内的数据映射到$[-2^{b-1},2^{b-1}-1]$范围内，超出此范围的数据被裁剪到最近的边界。

Calibration Set：校准集，少量的训练数据，用于在量化过程中校准评估量化误差

[affine-quantization-vs-scale-quantization](https://iq.opengenus.org/affine-quantization-vs-scale-quantization/) 

### Uniform transformations

[参考](https://docs.nvidia.com/deeplearning/tensorrt/tensorflow-quantization-toolkit/docs/docs/intro_to_quantization.html)

#### 1 Affine Quantization

Affine Quantization，也就是仿射量化，属于非对称量化，量化取值范围为：$[ -2^{b-1}, 2^{b-1} − 1]$，如进行INT8量化时范围为：$[ -128,127]$。

量化公式：
$$
x_q=clip(round(x * scale)+zeroPt,min = -2^{b-1}, max = 2^{b-1} − 1) \\
scale = \frac{2^b-1}{α-β} \\
zeroPt = -round(\beta * scale) - 2^{b-1}
$$
当 $x = 0$ 时，$x_q = zeroPt$，当 $x = \beta $ 时，$x_q = -2^{b-1}$，当 $x = \alpha $ 时，$x_q = 2^{b-1}-1$。

Affine DeQuantization（仿射反量化）
$$
\hat x = dequantize(x_q,zeroPt,scale)=\frac{x_q−zeroPt}{scale}
$$

#### 2 Scale Quantization

Scale Quantization，也就是缩放量化，属于对称量化，量化取值范围为：$[ -2^{b-1}+1, 2^{b-1} − 1]$，进行INT8量化时范围为：$[ -127,127]$，$-128$ 被舍弃了。

量化公式：
$$
x_q=clip(round(x * scale)+zeroPt,min = -2^{b-1}+1, max = 2^{b-1} − 1) \\
scale = \frac{2^{b}-2}{α-(-\alpha)}=\frac{2^{b-1}-1}{\alpha}\\
zeroPt = 0
$$
其中 $2^{b}-2=(2^{b}-1)-1$ ，第一项表示当前数据类型可表示的数据总数，后面减1是因为 $-128$ 被舍弃了。
Scale DeQuantization（缩放反量化）
$$
\hat x = dequantize(x_q,scale) = \frac{x_q}{scale}
$$


由量化计算公式可知，仿射量化对应公式的一般情况，而缩放量化则对应公式的特殊情况（zeroPt=0；且$\alpha = \beta$）

The only drawback in Scale Quantization is that is uses one less value as its range compared to Affine Quantization but this symmetric nature makes the calculations easy and gives better accuracy for most models as well.

#### 3 量化比较

[参考](https://tech.retrieva.jp/entry/20220128)

对于 $Y=XW$ ，$X=(x_{ik})∈R^{\large m×p}$，$W=(w_{kj})∈R^{\large p×n}$, $Y=(y_{kj})∈R^{\large m×n}$

缩放量化：
$$
\begin{eqnarray}
y_{i j}&=&\sum_{k=1}^{p} x_{i k} \cdot w_{k j} \\
&\approx& \sum_{k=1}^{p} \mathrm{dequantize}\left(x_{q, i k}, s_{x}\right) \cdot \mathrm{dequantize}\left(w_{q, k j}, s_{w}\right) \\
&=&\sum_{k=1}^{p} \frac{1}{s_{x}} x_{q, i k} \cdot \frac{1}{s_{w}} w_{q, k j} \\
&=&\frac{1}{s_{x} \cdot s_{w}} \sum_{k=1}^{p} x_{q, ik} \cdot w_{q, kj}
\end{eqnarray}
$$
仿射量化：
$$
\begin{eqnarray}
y_{i j} &\approx& \sum_{k=1}^{p} \frac{1}{s_{x}}\left(x_{q, i k}-z_{x}\right) \frac{1}{s_{w}}\left(w_{q, k j}-z_{w, j}\right) \\
       &=&\frac{1}{s_{x} s_{w}}\left(\sum_{k=1}^{p} x_{q, i k} w_{q, k j}-\sum_{k=1}^{p}\left(w_{q, k j} z_{x}+z_{x} z_{w}\right)-\sum_{k=1}^{p} x_{q, i k} z_{w}\right) \tag{11}
\end{eqnarray}
$$
第一项是缩放量化中也存在的部分，第二三项可以预先计算并重复使用，可以将其添加到当前层的bias中，而第四项是依赖输入数据的，无法提前计算，每次提供输入时都需要进行计算。因此与缩放量化相比，仿射量化具有较大的计算开销。因此，在注重加速计算时，对称量化似乎被认为优于仿射量化。

#### 4  Calibration

Calibration is the process of choosing *α* and *β* for model weights and activations.There are three calibration methods：

1.  Max: Use the maximum absolute value seen during calibration.
2. Entropy: Use KL divergence to minimize information loss between the original flfloating-point values and values that could be represented by the quantized format. **This is the default method used by TensorRT**.
3. Percentile: Set the range to a percentile of the distribution of absolute values seen during calibration. For example, 99% calibration would clip 1% of the largest magnitude values.设置绝对值分布分位数的范围。

#### 5 Post Training Quantization (PTQ)

Quantization parameters are calibrated offline by processing the trained model weights and activations generated by running inference on a sample dataset, no further training is involved.

- **Weight Quantization**
-  **Activation Quantization**

#### 6 Quantization-Aware Training (QAT)

Quantization Aware Training (QAT) describes the technique of inserting quantization operations in to the neural network before training or fine-tuning, to allow the network to adapt to the quantized weights and activations.

量化范围：fixed quantization ranges and learned quantization ranges；固定量化范围和可学习的量化范围。

**Simulated Quantization:**

A common approach to implementing QAT is to insert fake quantization, also called simulated quantization, operations into a flfloating-point network. Equation 12 defines fake quantization as a quantize and dequantize operation

that produces an approximate version of the input, $\hat x ≈  x$ , where *x* and $\hat x$ are both floating-point values.
$$
\hat x = dequantize(quantize(x, z, s), z, s)
$$
We add fake quantization operations at the inputs of the operation we wish to quantize to simulate the effects of quantization.After training, we transform the network to enable a quantized integer matrix multiply.

训练时，使用量化和反量化运算来产生权重和激活值的近似值，从而获得一个训练好的模型。训练后，通过对模型权重进行量化过程，也就是 $w_q=quantize(w, z, s)$，相当于只保留一份量化后的权重即可。再调用反量化方法 $\hat w = dequantize(w_q, z, s)$恢复原数据参与计算进行推理。

This scale factor needs to be calculated per-layer per-tensor. The simplest way is to map the min/max values of the float tensor to the min/max of the integer format. For weights and biases this is easy, as they are set once training is complete. For activations, the min/max float values can be obtained "online" during inference, or "offline".

- **Offline** means gathering activations statistics before deploying the model, either during training or by running a few "calibration" batches on the trained FP32 model. Based on these gathered statistics, the scaled factors are calculated and are fixed once the model is deployed. This method has the risk of encountering values outside the previously observed ranges at runtime. These values will be clipped, which might lead to accuracy degradation.
- **Online** means calculating the min/max values for each tensor dynamically during runtime. In this method clipping cannot occur, however the added computation resources required to calculate the min/max values at runtime might be prohibitive.

