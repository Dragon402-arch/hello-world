## Model Quantization 模型量化

> Quantization is a popular model compression technique that reduces the neural network model’s computation load and memory usage by converting the real number values into values with lower precision. 

量化就是把高位宽（Float32）表示的权重或是激活值用较低位宽来近似表示（INT8、INT4，……），在数值上的体现就是将连续的值离散化。

模型量化好处：压缩参数，降低模型大小、降低显存占用、提升推理速度；模型量化坏处：会有精度损失

量化类型：

- 量化的均匀性
  - 均匀量化（线性量化）
  - 非均匀量化（非线性量化）
- 量化的范围
  - 对称量化
  - 非对称量化
- Bit范围：1-bit、2-bit、4-bit、8-bit
- 量化位宽：
  - 全部采用统一位宽
  - 混合精度
- 量化的参数粒度
  - per-channel / per-axis :权重的每个通道使用单独的量化参数。Per-channel means that for a given dimension, it is usually the channel dimension of the tensor, and **each slice of the tensor in this dimension will use different scaling and offset** (so that scaling and offset can be represented by vectors), this ensures that there are fewer errors in the quantization process.
  - per-tensor / per-layer: 整个张量使用相同的量化参数。Per-tensor means that all values in the tensor are scaled in the same way.
- 量化方式：
  - Post-training static quantization (PTQ)，训练后量化，将已经训练完成的模型直接量化，然后用于推理。
  - Quantization-aware training (QAT)，训练时量化，量化感知训练，量化训练时的模块输入和模块权重，训练后用于推理。 The weights are quantized ahead of time and the scale factor and bias for the activation tensors are pre-computed based on observing the behavior of the model during a calibration process. 
  - Dynamic quantization：The weights are quantized ahead of time but the activations are dynamically quantized during inference. 

量化方法：

1.  Half-Precision Floating-Point Quantization

   Half-precision floating-point quantization (FP16 quantization) converts model parameters from single-precision floating-point (FP32) format to half-precision floating-point (FP16) format to reduce the model’s size by two times and decrease the inference latency

2.  Full-Integer Quantization

   Full-integer quantization converts all model parameter values (weights and activation) to a 8-bit integer format. This quantization method notably decreases the model size and inference latency with a small accuracy degradation

   Full-integer quantization quantizes weights as well as activations by scaling them over the range of the 8-bit integer format. **It applies symmetric quantization for the weights and asymmetric quantization for the activations.** 

   **symmetric quantization** is a quantization method that sets the boundaries of parameter values to an equal range (from −1 to 1) and maps them over the range of [−127, 127].The boundaries of parameter values are called clipping ranges, and their setup procedures are called **calibration**.If the value is out of range, then the value is clipped to the nearest value.The symmetric quantization is more efficient and less computationally expensive because the zero point value is equal to 0 (Z = 0).This is why **symmetric quantization is used for weight values**.

   In the case of the **asymmetric quantization** applied to **activation values**, the clipping range is not equal (Rmin = −0.5 and Rmax = 1.5). 

   Meanwhile, **asymmetric quantization is utilized for activations since applying the range of [−128, 127] to activations provides better accuracy.**

3. Dynamic Range Quantization

   Dynamic range quantization converts all weight values from floating-point precision to integer 8-bit precision. To further reduce the inference latency, “dynamic-range” operators dynamically quantize activations based on their range to 8-bits integer and perform computations with 8-bit integer weights and activations. After multiplication and accumulation, the activation values are dequantized.

   ![三种量化方式](D:\Typora\Notes\LLM\大模型推理\三种量化方式.png)







![Symmetric-Linear-Quantization](D:\Typora\Notes\LLM\大模型推理\Symmetric-Linear-Quantization.png)

学习资源：

[详细教程](https://huggingface.co/blog/hf-bitsandbytes-integration)

### 32-Bit：4-Bytes，full precision 

FP32：1（符号位）+8（整数位）+23（小数位），普遍使用

TF32：1（符号位）+8（整数位）+10（小数位），是计算数据类型而不是存储数据类型。目前使用范围较小。

### 16-Bit：2-Bytes，half-precision

FP16：1（符号位sign）+5（整数位range）+10（小数位precision），存在着 overflowing和 underflowing 的风险，

BF16：1（符号位）+8（整数位）+7（小数位）

### 8-Bit：1-Bytes

FP8（E4M3）：1（符号位）+4（整数位）+3（小数位），适合前向计算

FP8（E5M2）：1（符号位）+5（整数位）+2（小数位），适合反向计算

INT8：signed integers，[-128, 127] ；unsigned integers，[0, 255] 

量化是一种有损压缩，因为浮点数据在量化过去之后，再反推回来，与原来的浮点数据并不相等。这些看似微小的误差往往会随着它们在模型层中的传播而累积和增长，并导致模型性能下降。

Quantization is done by essentially “rounding” from one data type to another. The two most common 8-bit quantization techniques are zero-point quantization and absolute maximum (absmax) quantization. 

Zero-point quantization and absmax quantization map the floating point values into more compact int8 (1 byte) values. First, these methods normalize the input by scaling it by a quantization constant.

#### 8-bit absmax quantization

$$
scale = \frac{2^b-1}{max(abs)} = \frac{127}{5.4} = 23.5
$$

![quant-freeze](D:\Typora\Notes\LLM\大模型推理\quant-freeze.png)

#### 8-bit zero-point quantization

使用该方法进行量化后，数值“0”在量化后再反推回来不存在量化误差。 

![int8量化](D:\Typora\Notes\LLM\大模型推理\int8量化.png)

量化后模型性能下降是由于存在异常值或是离群值（outlier features）引起的，

In essence, LLM.int8() seeks to complete the matrix multiplication computation in three steps:

1. From the input hidden states, extract the outliers (i.e. values that are larger than a certain threshold) by column.
2. Perform the matrix multiplication of the outliers in FP16 and the non-outliers in int8.
3. Dequantize the non-outlier results and add both outlier and non-outlier results together to receive the full result in FP16.

实际上，也即是存在异常值的列就使用FP16进行计算，不存在异常值的列就使用INT8进行计算，再把INT8计算结果转换到FP16表示，最后把两部分结果合并起来得到最终结果。

![Matmul](D:\Typora\Notes\LLM\大模型推理\Matmul.png)



### 4-Bit

[教程](https://huggingface.co/blog/4bit-transformers-bitsandbytes)

INT4：signed integers，[-8, 7] ；unsigned integers，[0, 15] 

FP4（E2M1）：1（符号位）+2（整数位）+1（小数位）

FP4（E3M0）：1（符号位）+3（整数位）+0（小数位）

NF4：

计算：高精度计算

存储：低精度存储

### 4-Bit QLoRA

[讲解文档](https://readpaper.feishu.cn/docx/CrMGdSVPKow5d1x1XQMcJioRnQe)，[讲解视频](https://www.bilibili.com/video/BV1AX4y1a7jY/?spm_id_from=333.788&vd_source=390d16c51af19211c9d56a848f7c3fee)

QLoRA introduces a number of innovations to save memory without sacrificing performance:

(a) **4-bit NormalFloat (NF4), a new data type** that is information theoretically optimal for normally distributed weights

(b) double quantization to reduce the average memory footprint by quantizing the quantization constants, and 

(c) paged optimizers to manage memory spikes（峰值）.

storage data type：NF4，model weights

computation data type：BF16

QLoRA dequantizes weights from the storage data type to the computation data type to perform the forward and backward passes, but only computes weight gradients for the LoRA parameters which use 16-bit bfloat. The weights are decompressed only when they are needed, therefore the memory usage stays low during training and inference.

 A rule of thumb is: use double quant if you have problems with memory, use NF4 for higher precision, and use a 16-bit dtype for faster finetuning.

#### NF4 数据类型表示以及分位数

关于NF4数据类型如何表示以及如何获得分位数的具体细节现在还没搞清楚。

#### double quantization

关于double quantization ，其实就是在量化时需要使用 *quantization constant* or *quantization scale*，也就是缩放因子，而该缩放因子往往使用16-Bit 或32-Bit数据进行表示， double quantization就是用来减少quantization scale的显存占用的。

二次量化：

第一次：输入x1，量化因子s1，量化结果q1，其中x1使用FP32数据类型表示，s1使用FP32数据类型表示，q1使用INT4数据类型表示。

第二次：输入s1，量化因子s2，量化结果q2，其中s1使用FP32数据类型表示，s2使用FP32数据类型表示，q2使用FP8数据类型表示。

由于每组数据会使用一个量化因子s1，再对s1进行量化后，显存占用会进一步降低。假设权重的每64个元素使用一个缩放因子，也就是平均每个参数需要占用 32/64 = 0.5 bit，在二次量化之后是8/64 + 32/(64\*256)= 0.127 bit。（凑够256个量化因子s1，才需要一个量化因子s2，占用显存32 bit，而凑够256个量化因子s1则需要256*64个参数），下降了 0.373 bit。

QLoRA has one low-precision **storage data type**, in our case usually 4-bit, and one **computation data type** that is usually BFloat16. In practice, this means whenever a QLORA weight tensor is used, we dequantize the tensor to BFloat16, and then perform a matrix multiplication in 16-bit.

$$
Y^{BF16} = X^{BF16}·W^{BF16}+ X^{BF16}L_1^{BF16}L^{BF16}_2
$$

$$
W^{BF16} = doubleDequant(c^{FP32}_1, c^{FP8}_2,W^{NF4})= dequant(dequant(c^{FP32}_1, c^{FP8}_2),W^{NF4})
$$

We use NF4 for **W** and FP8 for $c_2$​. We use a blocksize of 64 for **W** for higher quantization precision and a blocksize of 256 for $c_2$ to conserve memory.

量化结果q2对应 $c^{FP8}_2$，量化因子s2对应 $c^{FP32}_1$，进行反向量化后可以得到量化因子s1，$W^{NF4}$ 对应着 q1，根据量化因子s1和量化结果q1进行反量化可以得到x1，也就是$W^{BF16}$，整个量化和反量化过程就是这样。

二次量化：
$$
W^{NF4} = Quant(c^{FP32}_2,W^{BF16}) \\
c^{FP8}_2= Quant(c^{FP32}_1,c^{FP32}_2)
$$
初次量化输入x1为$W^{BF16}$,量化因子s1对应$c^{FP32}_2$，量化结果q1对应$W^{NF4} $；非线性量化

二次量化输入s1为$c^{FP32}_2$，量化因子s2对应 $c^{FP32}_1$，量化结果q2对应 $c^{FP8}_2$；

#### QLoRA量化逻辑与计算

- INT8量化逻辑：

  将权重W和激活值A分别量化到INT8，然后直接使用INT8表示进行矩阵乘法运算，然后再将计算结果反量化到FP16。

- 4-Bit量化逻辑：

  将权重W和激活值A分别用4-Bits数据类型（FP4、NF4）分位数量化到INT4索引表示，也即是存储数据类型为4-Bits，在计算时根据INT4索引表示和分位数，反量化到BF16数据类型进行矩阵乘法计算（不直接使用4-Bit进行计算，原因是int4数据类型直接参与计算较为麻烦，torch中还没有该数据类型。）。简言之，**4-Bits数据类型进行存储，16-Bits数据类型进行计算，两者之间由量化和反量化进行转换。**（这个nf4，实际上只是将参数存成了一个4bit的索引，实际计算是根据索引将参数映射为16个分位数之一来做计算，实际参与计算的还是bf16。）

![4Bit量化](D:\Typora\Notes\LLM\大模型推理\4Bit量化.png)

注解：原数据需要除以 absmax，然后将得到的结果分别映射到最近的分位数上，最后把所有分位数的索引存储下来，反量化过程则是先根据索引找到分位数值，再将其反量化到计算数据类型。

`d2DequantizeFP4` 函数可以将量化后的整数索引转化为对应的分位数

```python
def d2DequantizeFP4(val: int) -> float:
    """反量化函数，输入的是量化后的整数索引"""
    sign = -1.0 if val & 0b1000 == 8 else 1.0
    # 若其为 0b_00_
    if val & 0b0110 == 0:
        # subnormal
        if val & 0b0001 == 0: # 若val 为0b_000
            return 0.0
        else: # 若val 为0b_001
            return sign * 0.0625
    else:
        # normal
        exponent = (2.0 if val & 0b0100 == 4 else 8.0) + (0.0 if val & 0b0010 == 2 else 2.0)
        fraction = 1.5 if val & 0b0001 == 1 else 1.0

        return sign * exponent * fraction

    
result = sorted([d2DequantizeFP4(i) for i in range(16)])
print(result)

# 输出：
[-12.0, -8.0, -6.0, -4.0, -3.0, -2.0, -0.0625, 0.0, 0.0, 0.0625, 2.0, 3.0, 4.0, 6.0, 8.0, 12.0]

# 由于0 和 8 对应的二进制表示为0b0 和 0b1000，在经过反量化函数后都会得到0和-0.

for i in range(16):
    # 查看其二进制表示
    print(bin(i),i)

```

The `&` operator is a bitwise AND operator in Python. It compares each bit of the first operand to the corresponding bit of the second operand. If both bits are 1, the corresponding result bit is set to 1. Otherwise, it is set to 0.

In this case, `val & 0b0110` is performing a bitwise AND operation between `val` and `0b0110`. The `0b` prefix indicates that the number is in binary format. So `0b0110` is equivalent to decimal `6`.

The result of the bitwise AND operation is a binary number that has a 1 in each bit position where both operands have a 1. In other words, it extracts the bits at positions 2 and 3 of `val`. If both bits are 0, the result will be 0. If either bit is 1, the result will be non-zero.

So, `if val & 0b0110 == 0` checks if the bits at positions 2 and 3 of `val` are both zero.

```python
x = [-12.0, -8.0, -6.0, -4.0, -3.0, -2.0, -0.0625, 0.0, 0.0625, 2.0, 3.0, 4.0, 6.0, 8.0, 12.0]

# 执行最大最小值归一化，使其处于[-1,1]区间
print([2*(i-min(x))/(max(x)-min(x)) -1 for i in result])

# 输出
[-1.0, -0.6666666666666667, -0.5, -0.33333333333333337, -0.25, -0.16666666666666663, -0.00520833333333337, 0.0, 0.005208333333333259, 0.16666666666666674, 0.25, 0.33333333333333326, 0.5, 0.6666666666666667, 1.0]
```



量化实际测试：

```python
from bitsandbytes import functional as bnbf


def parseint8(x):
    # 右移
    # print(bin(x))
    # 去掉后四位；只取后四位
    return (x >> 4) & 0b1111, x & 0b1111

x = torch.Tensor([0.2, 0.3, 0.7, 1.8] * 16)
x = x.to("cuda")

# 量化
fp4q, fp4qs = bnbf.quantize_fp4(x)
# nf4q, nf4qs = bnbf.quantize_nf4(x)

print(fp4q.shape) 
print(fp4qs)
print([vv.item() for v in fp4q[:2] for vv in parseint8(v)])


# 反量化
fp4qx = bnbf.dequantize_fp4(fp4q, fp4qs)
# nf4qx = bnbf.dequantize_nf4(nf4q, nf4qs)

print(fp4qx[:4])

"""
输出：
torch.Size([32, 1])
[tensor([1.8000], device='cuda:0'), torch.Size([64]), torch.float32, 64, None, 'fp4']
[6, 6, 4, 3]
tensor([0.3000, 0.3000, 0.6000, 1.8000], device='cuda:0')  反量化后的数据与原数据之间存在误差，有信息损失。

"""
```

注意：关于 返回值 fp4q 有如下解释：The 8-bit tensor with packed 4-bit values.也即是一个8-bit tensor 可以解压为两个4-bit tensor，这一点从输入是64个元素，而量化压缩后变为32个元素也可以证实，因此需要 `parseint8(x)` 函数解压8-bit tensor ，从而得到两个4-bit tensor，也就是两个量化后的索引值。

`parseint8(x)` 函数运算解释：

```shell
"""
右移4位：

示例一：
原数据：102，0b1100110
右移4位后：0b0000110，也就是 2+4=6
0b1100110 & 0b1111 等价于 0b1100110 & 0b0001111 = 0b0000110

示例二：
原数据：67，0b1000011
右移4位后：0b0000100 也就是 4
0b1000011 & 0b1111 等价于 0b1000011 & 0b0001111 = 0b0000011 也就是 3
"""
```

![image-20230729194150075](C:\Users\千江映月\AppData\Roaming\Typora\typora-user-images\image-20230729194150075.png)



两种INT4方式量化区别：

BaiChuan：`torch.clip(torch.round(weight.to(scale.dtype) / scale[:, None]), -16, 15).to(dtype=torch.int32)`，直接对 `weight.to(scale.dtype) / scale[:, None])` 进行四舍五入，取得整数索引，然后将超出 $[-16, 15]$ 范围的值截断到边界上，从而获得存储整数的索引。（五舍五入获取整数索引）

QLoRA：原数据需要除以 absmax，然后将得到的结果分别映射到最近的分位数上，最后把所有分位数的索引存储下来。（取最近分位数对应整数索引）



NF4和FP4的主要区别就在于分位数的获取，











 
