基础模型

- LLaMA 
  - 再训练模型：Baichuan、Alpaca、Vicuna
- LLaMA2
- GLM
  - ChatGLM1
  - ChatGLM2
- Bloom
  - 再训练模型：Tigerbot
- GPT2
  - GPT-J
- BERT
- Transformer
- MPT
- Falcon（2023）
- PaLM（2022）
  - 代码：https://github.com/lucidrains/PaLM-pytorch/blob/main/palm_pytorch/palm_pytorch.py
- OPT

模型机构

- PaLM

  - **1)** [**SwiGLU**](https://sh-tsang.medium.com/brief-review-glu-variants-improve-transformer-9ee943115ab) **activation: is used for the MLP intermediate activations** because [SwiGLU](https://sh-tsang.medium.com/brief-review-glu-variants-improve-transformer-9ee943115ab) has been shown to significantly increase quality compared to standard [ReLU](https://sh-tsang.medium.com/brief-review-rectified-linear-units-improve-restricted-boltzmann-machines-43724ede3ecf), [GELU](https://sh-tsang.medium.com/review-gaussian-error-linear-units-gelus-d4d7347d1e11), or [Swish](https://sh-tsang.medium.com/review-swish-searching-for-activation-functions-993a9ef2b4b9) activations.

  - **2) Parallel Layers: A “Parallel” formulation in each** [**Transformer**](https://sh-tsang.medium.com/review-attention-is-all-you-need-transformer-96c787ecdec1) **block:** as in GPT-J-6B, is used, rather than the standard “serialized” formulation. Specifically, the **standard formulation** can be written as:
    $$
    y = x + MLP(LayerNorm(x+Attention(LayerNorm(x)))
    $$

  - The **parallel formulation** can be written as:
    $$
    y = x + MLP(LayerNorm(x))+Attention(LayerNorm(x))
    $$

  - The parallel formulation results in roughly **15% faster training speed** at large scales. Ablation experiments showed **a small quality degradation at 8B scale** but **no quality degradation at 62B scale**.
  - **3) Multi-Query Attention**: The **key/value projections** are **shared for each head**, i.e. “key” and “value” are projected to [1, *h*], but **“query” is still projected to shape [\*k\*, \*h\*]**. It is found to have **a neutral effect on model quality and training speed**, but results in **a significant cost savings at autoregressive decoding time**.
  - **4)** [**RoPE**](https://sh-tsang.medium.com/brief-review-roformer-enhanced-transformer-with-rotary-position-embedding-36f67a619442) **embeddings:** are used rather than absolute or relative position embeddings.
  - **5) Shared Input-Output Embeddings**: The input and output embedding matrices are shared, which is done frequently (but not universally) in past work.
  - **6) No Biases:** No biases were used in any of the dense kernels or layer norms. It is found to have **increased training stability** for large models.
  - **7) Vocabulary:** A [**SentencePiece**](https://sh-tsang.medium.com/brief-review-sentencepiece-a-simple-and-language-independent-subword-tokenizer-and-detokenizer-647f38ecb24f) vocabulary with **256k tokens** is used, which was chosen to **support the large number of languages** in the training corpus without excess tokenization.

激活函数

- ReLU
- GeLU
- Swish 
- SwiGLU
- 

