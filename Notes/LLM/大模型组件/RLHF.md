多阶段训练数据：

- https://huggingface.co/datasets/lvwerra/stack-exchange-paired/tree/main/data
- https://huggingface.co/datasets/lvwerra/stack-exchange-paired

大模型训练三个阶段：

- Supervised Fine-tuning (SFT)

  Fine-tune a pretrained model using supervised learning. This is our supervised fine-tuned model (SFT).

- Reward / preference modeling (RM)

  Initialize a new *reward model* (RM) from SFT with a new regression head (linear layer). This model takes in the input to the SFT model and it's output, and predicts a reward scalar. Fine-tune this model on a reward function/dataset.

- Reinforcement Learning from Human Feedback (RLHF)

  Now use RM to score every output of SFT, and use [PPO - Proximal Policy Optimization](https://notesonai.com/PPO+-+Proximal+Policy+Optimization) or any stable policy gradient algorithm to update the parameters of SFT, such as [TRPO - Trust-Region Policy Optimization](https://notesonai.com/TRPO+-+Trust-Region+Policy+Optimization).

数据体量

- SFT dataset：13k
- RM dataset：33k
- PPO dataset：31k
- 

### 1. SFT

看到不少基于SFT的alignment方法, 例如 RAFT, RRHF, Direct Preference Optimization (DPO) 等等，这些内容还需要持续研究。

![训练过程1](D:\Typora\Notes\LLM\大模型组件\训练过程1.jpg)

### 2. RM

- [RM教程](https://huggingface.co/blog/stackllama)

奖励模型（Reward Model）使用人类偏好进行校准，因此也常被称为偏好模型（preference model），最终训练的奖励模型可以对齐人类偏好。具体而言，奖励模型将输入文本序列映射到一个标量奖励得分，该奖励得分在数值上表示人类的偏好程度，得分越大表示越接近人类偏好，越小表示越偏离人类偏好。实际上，奖励模型训练是一个**排序任务**。

![奖励模型](D:\Typora\Notes\LLM\大模型组件\奖励模型.png)

![训练过程2](D:\Typora\Notes\LLM\大模型组件\训练过程2.jpg)

- 训练数据示例：prompt-generation pair

  ```json
  {
      "query": "联合国总部在哪里？",
      "chosen": "“联合国总部位于美国纽约州纽约市曼哈顿区东侧，其西侧边界为第一大道，南侧为东42街，北侧为东48街，东侧可以俯瞰东河。联合国总部于1949年和1950年间兴建，土地购自于当时的纽约房地产家威廉·杰肯多夫，面积阔达18英亩。2006年12月23日，联合国大会通过了名为“基本建设总计划”的决议，决定拨款18.77亿美元来整修联合国总部大楼。",
      "rejected": "联合国的15个专门机构（如教科文组织）都没有设在总部。然而，有一些“自治附属机构”（如联合国儿童基金会）的总部设在联合国总部。"
  }
  ```

  - answer来源：
    - 可以是人工编写的多个回答
    - 也可以是由 sft 模型针对query生成的多个不同回答。

- 训练数据合适条数：50k

- 奖励模型选择：

  - 可以选择经过预训练的语言模型，也就是通常的 **base** 模型
  - 可以选择经过SFT训练的模型，也就是通常的 **chat** 模型

- 奖励模型参数量：（奖励模型也要具备文本理解和生成能力）

  - OpenAI 175B LM, 6B reward model
  - DeepMind 70B Chinchilla models for both LM and reward
  - Anthropic used LM and reward models from 10B to 52B

- 奖励模型训练细节：

  - 训练数据处理：

    采样回答对：对于奖励模型，每个问题（query）需要有两个回答（chosen / rejected answer）来进行比较。而有些问题可能有几十个回答，导致存在许多可能的配对。此时，可以限制每个问题最多抽样10对回答，从而限制每个问题产生的训练样本数量。比如一个问题，模型生成了6个回答，那么就会产生$\rm C_6^2=15$对回答，此时可以从15个问答对中采样10个问答对作为训练数据。

  - 奖励模型结构

    The reward model (RM) is simply a copy of fine-tuned model, but **with a new linear layer added on top to predict the reward**. The input to the reward model is the original input, plus the output generated by the base / chat model.

    以GPT2模型为例，其奖励模型包含 `pretrained_model` 和 `value_head` 两个大模块。

    ```sh
    AutoModelForCausalLMWithValueHead(
        (pretrained_model):GPT2LMHeadModel(
          (transformer): GPT2Model(
            (wte): Embedding(50257, 768)
            (wpe): Embedding(1024, 768)
            (drop): Dropout(p=0.1, inplace=False)
            (h): ModuleList(
              (0-11): 12 x GPT2Block(
                (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (attn): GPT2Attention(
                  (c_attn): Conv1D()
                  (c_proj): Conv1D()
                  (attn_dropout): Dropout(p=0.1, inplace=False)
                  (resid_dropout): Dropout(p=0.1, inplace=False)
                )
                (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (mlp): GPT2MLP(
                  (c_fc): Conv1D()
                  (c_proj): Conv1D()
                  (act): NewGELUActivation()
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (lm_head): Linear(in_features=768, out_features=50257, bias=False)
        )
        (value_head):ValueHead(
          (dropout): Dropout(p=0.1, inplace=False)
          (summary): Linear(in_features=768, out_features=1, bias=True)
          (flatten): Flatten(start_dim=1, end_dim=-1)
        )
       )
    ```

  - 模型训练过程

    1. 构建训练样本数据：针对同一问题的多个回答，两两组合得到多对回答组合（限定最大数量，超出则进行采样），每对回答组合作为一条奖励模型的训练样本。

    2. 奖励得分计算：

       - 在DeepSpeed-Chat框架中，使用最后一个token的得分作为整个response的得分，当然也可以使用所有token得分的平均值作为最终得分。

       - 在 TRL框架中使用

    3. 定义损失计算：具体损失值计算公式为：

       **The goal of the reward model is to imitate how a human would rate a text.**There are several possible strategies to build a reward model: the most straightforward way would be to predict the annotation (e.g. a rating score or a binary value for “good”/”bad”). In practice, what works better is to predict the ranking of two examples, where the reward model is presented with two candidates $(y_j,y_k)$ for a given prompt $x$ and has to predict which one would be rated higher by a human annotator.This can be translated into the following loss function:
       $$
       loss(θ)=−\frac{1}{(C_K^2)}\rm E_{\large (x,y_j,y_k)∼D}[\log(\sigma (r_\theta(x,y_j)−r_\theta(x,y_k)))]
       $$
       where $r_θ(x, y)$ is the scalar output of the reward model for prompt $x$ and completion $y$ with parameters $θ$, $y_j$ is the preferred completion out of the pair of $y_j$ and $y_k$ , and $D$ is the comparison datase.

       奖励模型的训练目标是尽可能的使得chosen回答和rejected回答的奖励得分差值更大，为了更好的归一化差值，会对每两项差值都过一个sigmoid函数将差值拉到0~1区间之间。可以看到loss的值等于排序列表中所有「排在前面项的reward」减去「排在后面项的reward」的和，模型能够最大化chosen回答奖励得分和rejected回答奖励得分的差值，但是梯度下降是做的最小化操作，因此需要对整个loss取负对数才能实现差值最大化效果。

       - 核心部分代码实现

       ```python
       from transformers import Trainner
       
       
       class RewardTrainer(Trainer):
           def compute_loss(self, model, inputs, return_outputs=False):
               rewards_j = model(
                   input_ids=inputs["input_ids_j"], attention_mask=inputs["attention_mask_j"]
               )[0]
               rewards_k = model(
                   input_ids=inputs["input_ids_k"], attention_mask=inputs["attention_mask_k"]
               )[0]
               loss = -nn.functional.logsigmoid(rewards_j - rewards_k).mean()
               if return_outputs:
                   return loss, {"rewards_j": rewards_j, "rewards_k": rewards_k}
               return loss
       ```

       奖励模型训练完成后，就可以对SFT模型的回答给出奖励得分，如果模型有多个回答，那么就可以根据得分对这些回答进行排序。

- 存在的疑惑：
  - 如果一个问题有多个符合人类偏好的答案，那么这些答案之间的排序在训练中如何体现，可能每个答案对应的奖励得分有差异。

### 3. RLHF

该阶段是RLHF训练的核心部分，在强化学习中使用近端策略优化（PPO）算法来引入来自RM模型的奖励信号，引导优化后的大模型生成更符合人类偏好的内容。

**生成策略优化**： 给定习得的奖励模型，ChatGPT/InstructGPT 的参数将被视为一种策略，在强化学习的框架下进行训练。

- 首先，当前策略根据输入的查询采样回复。
- 然后，奖励模型针对回复的质量计算奖励，反馈回当前策略用以更新。值得注意的是，为防止上述过程的过度优化，损失函数同时引入了词级别的 KL 惩罚项。
- 此外，为了避免在公开 NLP 数据集上的性能退化，策略更新过程兼顾了预训练损失。

PPO（proximal policy optimization）：近端策略优化

在拥有SFT和RM阶段训练好的模型后，就可以进行RLHF阶段的训练，训练过程可以分为三个步骤：

1. Generate responses from prompts（SFT）
2. Rate the responses with the reward model （RM）
3. Run a reinforcement learning policy-optimization step with the ratings

![trl_loop](D:\Typora\Notes\LLM\大模型组件\trl_loop.jpg)

涉及的模型

- **Reward Model**：base model + value head，update

  训练过程中冻结模型参数，针对模型预测的每个token（状态），给出奖励得分，充当环境角色。

- **Reference Model**：sft model ，freeze 

  训练过程中冻结模型参数，用于和Finetune Model模型做对比，保证模型不偏离SFT Model太多。

- Critic Model（value model）

- **Active Model**：sft model ，update

  作为策略模型，以query作为输入，预测下一个token，训练完成后，就获得了最终可供使用的模型。

![训练过程3](D:\Typora\Notes\LLM\大模型组件\训练过程3.jpg)

DeepSpeed-Chat版本模型：

![PPO算法1](D:\Typora\Notes\LLM\大模型组件\PPO算法1.webp)



![PPO算法2](D:\Typora\Notes\LLM\大模型组件\PPO算法2.png)

核心代码实现：

```python
for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):
    question_tensors = batch["input_ids"]
        
    # sample from the policy and generate responses
    response_tensors = ppo_trainer.generate(
        question_tensors,
        return_prompt=False,
        length_sampler=output_length_sampler,
        **generation_kwargs,
    )
    batch["response"] = tokenizer.batch_decode(response_tensors, skip_special_tokens=True)

    # Compute sentiment score
    texts = [q + r for q, r in zip(batch["query"], batch["response"])]
    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)
    rewards = [torch.tensor(output[0]["score"] - script_args.reward_baseline) for output in pipe_outputs]

    # Run PPO step
    stats = ppo_trainer.step(question_tensors, response_tensors, rewards)
    # Log stats to WandB
    ppo_trainer.log_stats(stats, batch, rewards)
```

使用强化学习方法训练语言模型的一个常见问题是模型可以生成胡言乱语的话，从而误导奖励模型给予高得分。为了缓解这一问题，通常在奖励模型上增加了一个惩罚项：我们使用了一个reference model （一个只经过有监督微调的模型），并计算正在训练的新模型和reference model的KL离散度。
$$
R(x,y)=r(x,y)−\beta ·\rm KL(x,y) =r(x,y)−\beta ·\rm \log\bigg[\frac{\pi^{RL}(y|x)}{\pi^{SFT}(y|x)}\bigg]
$$
其中 $r(·)$ 表示奖励模型的奖励得分，$\rm KL(x,y)$ 表示the current policy 和 reference model 之间的 KL 离散度。



过程注解：![大模型强化学习](D:\Typora\Notes\LLM\大模型组件\大模型强化学习.png)

PPO算法：Actor-Critic（演员和裁判）

计算一个7B全参数微调所需显存：

Some quick math: in bf16, every parameter uses 2 bytes (in fp32 4 bytes) in addition to 8 bytes used, e.g., in the Adam optimizer . So a 7B parameter model would use `(2+8)*7B=70GB`

optimizer states：4

gradient：2

parameter：2

intermediate：2 中间激活值

