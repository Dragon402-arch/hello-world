## 外推型位置编码

### 1、Attention with Linear Biases (ALiBi)

使用模型：Bloom

编码类型：相对位置编码（relative positional embeddings）

![ALiBi](D:\Typora\Notes\LLM\大模型组件\ALiBi.png)

官方源码：https://github.com/ofirpress/attention_with_linear_biases

Alibi allows one to train a model on shorter sequences and inference on longer sequences. 

外推：

We define *extrapolation* as a model’s ability to continue performing well as the number of input tokens during validation increases beyond the number of tokens on which the the model was trained. 

ALiBi negatively biases attention scores with a linearly decreasing penalty proportional to the distance between the relevant key and query

#### 1.1 Bloom代码实现

[介绍](https://medium.com/@pajakamy/alibi-attention-with-linear-biases-942abe042e9f)

slope计算公式
$$
slopes = \Large  2^{\left ( -\frac {\Huge 8i} {\Huge n}\right)} = \Large  (2^{\left ( -\frac {\Huge 8} {\Huge n}\right)})^i
$$
其中 $n$ 表示 head 的个数，$i$ 表示第 $i$ 个head，$i$ 取值从1开始
$$
\tiny \pi r^2

\scriptsize \pi r^2


\small \pi r^2

\normalsize\pi r^2

\large\pi r^2

\Large\pi r^2

\LARGE\pi r^2

\huge\pi r^2

\Huge\pi r^2
$$


```python
def build_alibi_tensor(
    attention_mask: torch.Tensor, num_heads: int, dtype: torch.dtype
) -> torch.Tensor:
    """
    Args:
    Returns tensor shaped (batch_size * num_heads, 1, max_seq_len)
        attention_mask (`torch.Tensor`):
            Token-wise attention mask, this should be of shape (batch_size, max_seq_len).
        num_heads (`int`, *required*):
            number of heads
        dtype (`torch.dtype`, *optional*, default=`torch.bfloat16`):
            dtype of the output tensor
    """
    batch_size, seq_length = attention_mask.shape
    closest_power_of_2 = 2 ** math.floor(math.log2(num_heads))

    """计算公式：2^(-8/n)"""
    base = torch.tensor(
        2 ** (-(2 ** -(math.log2(closest_power_of_2) - 3))),
        device=attention_mask.device,
        dtype=torch.float32,
    )
    #
    powers = torch.arange(
        1, 1 + closest_power_of_2, device=attention_mask.device, dtype=torch.int32
    )
	# base 作为计算每个head超参数的基础，然后根绝head为1/2/3/4等，取对应的次方即可。
    slopes = torch.pow(base, powers)

    if closest_power_of_2 != num_heads:
        extra_base = torch.tensor(
            2 ** (-(2 ** -(math.log2(2 * closest_power_of_2) - 3))),
            device=attention_mask.device,
            dtype=torch.float32,
        )
        # 一般都是取 num_heads - closest_power_of_2
        num_remaining_heads = min(closest_power_of_2, num_heads - closest_power_of_2)
        extra_powers = torch.arange(
            1,
            1 + 2 * num_remaining_heads,
            2,
            device=attention_mask.device,
            dtype=torch.int32,
        )
        slopes = torch.cat([slopes, torch.pow(extra_base, extra_powers)], dim=0)

    # Note: alibi will added to the attention bias that will be applied to the query, key product of attention
    # => therefore alibi will have to be of shape (batch_size, num_heads, query_length, key_length)
    # => here we set (batch_size=1, num_heads=num_heads, query_length=1, key_length=max_length)
    # => the query_length dimension will then be broadcasted correctly
    # This is more or less identical to T5's relative position bias:
    # https://github.com/huggingface/transformers/blob/f681437203baa7671de3174b0fa583c349d9d5e1/src/transformers/models/t5/modeling_t5.py#L527
    arange_tensor = ((attention_mask.cumsum(dim=-1) - 1) * attention_mask)[:, None, :]
    print(arange_tensor)
    alibi = slopes[..., None] * arange_tensor
    print(alibi.shape)
    return alibi.reshape(batch_size * num_heads, 1, seq_length).to(dtype)
```

测试：

```python
attention_mask = torch.ones((2, 16))
attention_mask[:, 14:] = 0
dtype = torch.float
num_heads = 3

build_alibi_tensor(attention_mask, num_heads, dtype)
```



#### 1.2 BaiChuan 代码实现

```python
import math


def _get_interleave(n):
    """计算公式：2^(-8/n)"""

    def _get_interleave_power_of_2(n):
        start = 2 ** (-(2 ** -(math.log2(n) - 3)))
        ratio = start
        # return [ratio ** (i + 1) for i in range(n)]
        return [start * (ratio ** i) for i in range(n)]

    if math.log2(n).is_integer():
        return _get_interleave_power_of_2(n)
    else:
        closest_power_of_2 = 2 ** math.floor(math.log2(n))
        return (
            _get_interleave_power_of_2(closest_power_of_2)
            + _get_interleave(2 * closest_power_of_2)[0::2][: n - closest_power_of_2]
        )


def _fill_with_neg_inf(t):
    """FP16-compatible function that fills a tensor with -inf."""
    return t.float().fill_(float("-inf")).type_as(t)


def _gen_alibi_mask(n_head, max_pos):
    # 将alibi向量与attention_mask结合起来
    slopes = torch.Tensor(_get_interleave(n_head))
    alibi = (
            slopes.unsqueeze(1).unsqueeze(1) * 
            torch.arange(max_pos).unsqueeze(0).unsqueeze(0).expand(n_head, -1, -1)
    )
    # alibi = slopes[:, None, None] * torch.arange(max_pos)[None, None, :].expand(n_head, -1, -1)
    alibi = alibi.view(n_head, 1, max_pos)
    alibi_mask = torch.triu(_fill_with_neg_inf(torch.zeros([max_pos, max_pos])), 1)
    alibi_mask = alibi_mask.unsqueeze(0) + alibi
    return alibi_mask


```

后续使用:

```python
  def get_alibi_mask(self, tensor, seq_length_with_past):
        if self.first_run:
            self.first_run = False
            self.register_buffer("future_mask", _gen_alibi_mask(self.n_head, self.max_cache_pos).to(tensor),
                                 persistent=False)
        if seq_length_with_past > self.max_cache_pos:
            self.max_cache_pos = seq_length_with_past
            self.register_buffer("future_mask", _gen_alibi_mask(self.n_head, self.max_cache_pos).to(tensor),
                                 persistent=False)
        mask = self.future_mask[:self.n_head, :seq_length_with_past, :seq_length_with_past]
        return mask
    
# embed positions
attention_mask = self.get_alibi_mask(inputs_embeds, seq_length_with_past)

```



### 2、Rotary Position Embeddings (RoPE)

使用模型：GPT-J、LLaMA、ChatGLM

#### 2.1 理论基础

理论讲解：[首先看](https://kexue.fm/archives/8265)，[其次看](https://kexue.fm/archives/8130)，[然后看](https://blog.eleuther.ai/rotary-embeddings/)，[复数基础](https://zhuanlan.zhihu.com/p/85321120)，[最后看](https://wmathor.com/index.php/archives/1542/)

##### 2.1.1 预备基础

复数基础：
$$
\begin{aligned} z &= a + ib \\
& = \sqrt{a^2+b^2}· \bigg(\frac{a}{\sqrt{a^2+b^2}} + \frac{ib}{\sqrt{a^2+b^2}}\bigg) \\
&= r(cosθ+i\ sinθ) \\
&= re^{\large iθ}
\end {aligned}
$$
复数的模：$|z|=\sqrt{a^2+b^2}$

共轭复数：$z^*=a-ib= r(cosθ-i\ sinθ) = re^{-iθ}$

复数的幅角：$θ$

根据无穷级数展开式或是欧拉公式可知，$e^{iθ}= cosθ+i\ sinθ$

##### 2.1.2 推导过程

假定： 
$$
<f(\vec q,m),f(\vec k,n)> = h(\vec q,\vec k,m-n) \\
f(\vec q,0)=\vec q \\
f(\vec k,0)=\vec k
$$
其中 $<·,·>$ 表示向量内积运算，$ h(\vec q,\vec k,m-n)$ 此时表示一个实数，表示某个函数的计算结果，因为向量的内积是实数。

假定是二维向量，可以借助复数形式进行运算，可以用$R_{f()}$ 表示复数的模：

> 两个二维向量的内积，等于把它们当复数看时，一个复数与另一个复数的共轭的乘积实部，Re[]表示取结果的实部。
>
> 如向量[a,b]与[c,d]的内积等于$ac+bd$，对应的复数运算为$(a+bi)(c-di)=(ac+bd) +(bc - ad)i$

在复数中有$⟨q,k⟩=Re[qk^∗]$，$Re[]$代表复数的实部，$k^*$ 是 $k$ 的共轭复数， 则有：
$$
<f(\vec q,m),f(\vec k,n)> = Re[f(\vec q,m)·f^*(\vec k,n)] = Re[g(\vec q,\vec k,m-n)] = h(\vec q,\vec k,m-n)
$$
其中 $g(\vec q,\vec k,m-n)$ 表示一个复数（二维向量），而 $h(\vec q,\vec k,m-n)$ 表示一个实数（标量，也就是复数的虚部为0的情况）。

这里还是转不过来，

左边等于：$ R_{\large f(\vec q,m)}\Large e^{\Large iθ_{f(\vec q,m)}} · R_{\large f(\vec k,n)}\Large e^{\Large -iθ_{f(\vec k,n)}}$，其中 $ R_{\large f(\vec k,n)}\Large e^{\Large -iθ_{f(\vec k,n)}}$ 是 $ R_{\large f(\vec k,n)}\Large e^{\Large iθ_{f(\vec k,n)}}$的共轭复数

右边等于：$ R_{\large g(\vec q,\vec k,m-n)}\Large e^{\Large iθ_{g(\vec q,\vec k,m-n)}}$

于是有 ：
$$ { }
R_{\large f(\vec q,m)} ·R_{\large f(\vec k,n)} = R_{\large g(\vec q,\vec k,m-n)} \\
θ_{\large f(\vec q,m)} - θ_{\large f(\vec k,n)} = θ_{\large g(\vec q,\vec k,m-n)}
$$
对于第一个方程，令 m = n，则有：（最后一步的转换依赖于$f(\vec q,0)=\vec q ,\ 
f(\vec k,0)=\vec k$  )
$$
R_{\large f(\vec q,m)} ·R_{\large f(\vec k,m)} = R_{\large g(\vec q,\vec k,0)}=R_{\large f(\vec q,0)} ·R_{\large f(\vec k,0)} = |\vec q|·|\vec k|
$$
可知，该方程与位置变量m 、n无关，也即意味着 $R_{f()}$ 的值与m无关，因此我们可以设置 $R_{\large f(\vec q,m)}=|\vec q|$。

对于第二个方程，令 m = n，则有：（最后一步的转换依赖于$f(\vec q,0)=\vec q ,\ 
f(\vec k,0)=\vec k$  )
$$
θ_{\large f(\vec q,m)} - θ_{\large f(\vec k,m)} = θ_{\large g(\vec q,\vec k,0)} = θ_{\large f(\vec q,0)} - θ_{\large f(\vec k,0)} = θ_{\large \vec q} - θ_{\large \vec k}
$$
于是有：
$$
θ_{\large f(\vec q,m)} - θ_{\large \vec q} = θ_{\large f(\vec k,m)} - θ_{\large \vec k}
$$
定义：$φ(m)=θ_{\large f(\vec q,m)} - θ_{\large \vec q} = θ_{f(\large \vec k,m)} - θ_{\large \vec k}$，则有：
$$
φ(m)-φ(m-1)=θ_{\large f(\vec q,m)} - θ_{\large \vec q} -(θ_{\large f(\vec k,m-1)} - θ_{\large \vec k} )=θ_{\large f(\vec q,m)} -θ_{\large f(\vec k,m-1)} = θ_{\large g(\vec q,\vec k,1)}+θ_{\large \vec k}- θ_{\large \vec q}
$$
可知，$φ(m)-φ(m-1)$ 的结果是一个与 m 无关的常数，也就是个等差数列，且已知$φ(0)=0$，若令$φ(m)-φ(m-1)=θ$，则有$φ(m)=mθ$。

最后：(第四部的转换依赖于 $R_{\large f(\vec q,m)}=|\vec q|$ )
$$
\begin{aligned} 
f(\vec q,m)
&=R_{\large f(\vec q,m)}\Large e^{\Large iθ_{f(\vec q,m)}} \\
&= R_{\large f(\vec q,m)}\Large e^{\Large i(mθ+θ_{\vec q})}\\
&= R_{\large f(\vec q,m)}\Large e^{\Large iθ_{\vec q}}·e^{\Large imθ} \\
&= |\vec q|·\Large e^{\Large iθ_{\vec q}}·e^{\Large imθ} \\
&= \vec q · e^{\Large imθ} \\
&=  e^{\Large imθ} ·\vec q \\
&=(cosm\theta + i·sin m\theta )(q_0+i·q_1) \\
&= \begin{bmatrix}
\cos m\theta &-\sin m\theta \\
\sin m\theta &\cos m\theta\\
\end{bmatrix} \vec q  \\ 
&= \begin{bmatrix}
\cos m\theta &-\sin m\theta \\
\sin m\theta &\cos m\theta\\
\end{bmatrix} \begin{bmatrix}
q_0 \\
q_1\\
\end{bmatrix} \\

&=cosm\theta ·\begin{bmatrix}
q_0 \\
q_1\\
\end{bmatrix} + sinm\theta ·\begin{bmatrix}
-q_1 \\
q_0\\
\end{bmatrix} 
\end {aligned}
$$
其中 $f(\vec q,m)$ 可以看做一个复数，也可以看做是一个二维向量。

计算过程详解：

对于复数 $z=a+bi$， 可以用二维向量$[a,b]$ 来表示该复数，对应到复平面上的一个点。于是有：

$ e^{\large (im\theta)}\vec q = (cosm\theta + i·sin m\theta )(q_0+i·q_1)=(q_0cosmθ−q_1sinmθ)+i(q_0sinmθ+q_1cosmθ)$

复数结果对应的二维向量为：$\begin{bmatrix}
q_0cosmθ−q_1sinmθ \\
q_0sinmθ+q_1cosmθ\\
\end{bmatrix}$

运算等价于:$\begin{bmatrix}
\cos m\theta &-\sin m\theta \\
\sin m\theta &\cos m\theta\\
\end{bmatrix} ·\begin{bmatrix}
q_0 \\
q_1\\
\end{bmatrix}$，或是 $cosm\theta ·\begin{bmatrix}
q_0 \\
q_1\\
\end{bmatrix} + sinm\theta ·\begin{bmatrix}
-q_1 \\
q_0\\
\end{bmatrix}$

由上式可知，第1/2维共用一个$cosmθ$ 和 $sinmθ$，如果head_dim为128，也可以第1/65维共用一个 $cosmθ$ 和 $sinmθ$，

则有：
$$
\begin{aligned} f(\vec q,m) \ · f(\vec k,n) &= \vec q^⊤\begin{bmatrix}
\cos m\theta &-\sin m\theta \\
\sin m\theta &\cos m\theta\\
\end{bmatrix} ^\mathrm T  \begin{bmatrix}
\cos n\theta &-\sin n\theta \\
\sin n\theta &\cos n\theta\\
\end{bmatrix}\vec k \\
& = \vec q^⊤\begin{bmatrix}
\cos (m-n)\theta &\sin (m-n)\theta \\
-\sin (m-n)\theta &\cos (m-n)\theta\\
\end{bmatrix}\vec k  \\
& = g(\vec q,\vec k,m-n)
\end {aligned}
$$


计算公式：
$$
m·θ_i=m·10000^{\frac{\huge 2(i-1)}{\huge d}},i∈[1,2,...,\frac d2],m∈[0,1,2,...,seq\_len-1]
$$

其中 d 表示 head_dim，m表示 seq_len。

The features are grouped into pairs and handled as above. They use a different *θ* for each pair.

The paper suggests using $Θ=θ=10000^{\frac{\huge 2(i-1)}{\huge d}},i∈[1,2,...,\frac d2]$ for the $\frac {d}{2}$ pairs of features.

We pair feature $i$ with feature $\large i+\frac{d}{2}$. So for position *m* we transform $\begin{pmatrix}
x_m^{(i)} \\
x_m^{(i+\frac{d}{2})}\\
\end{pmatrix}$ to  $\begin{bmatrix}
\cos m\theta &-\sin m\theta \\
\sin m\theta &\cos m\theta\\
\end{bmatrix}\begin{pmatrix}
x_m^{(i)} \\
x_m^{(i+\frac{d}{2})}\\
\end{pmatrix}$

####  2.2 代码实现

[代码实现讲解](https://nn.labml.ai/transformers/rope/index.html)

```python
#  -*- coding: utf-8 -*-
import torch


class LlamaRotaryEmbedding(torch.nn.Module):
    def __init__(self, head_dim, max_position_embeddings=2048, base=10000, device=None):
        super().__init__()
        inv_freq = 1.0 / (
            base ** (torch.arange(0, head_dim, 2).float().to(device) / head_dim)
        )

        self.register_buffer("inv_freq", inv_freq)
        # print(inv_freq)

        # Build here to make `torch.jit.trace` work.
        self.max_seq_len_cached = max_position_embeddings
        t = torch.arange(
            self.max_seq_len_cached,
            device=self.inv_freq.device,
            dtype=self.inv_freq.dtype,
        )
        # 外积运算：（2048,1） x (1,64)
        freqs = torch.einsum("i,j->ij", t, self.inv_freq)
        # print(freqs.shape)
        # Different from paper, but it uses a different permutation in order to obtain the same calculation
        emb = torch.cat((freqs, freqs), dim=-1)
        # print(emb.shape)
        # shape (max_position_embeddings,num_heads) (2048,128)
        # print(emb.shape)
        self.register_buffer(
            "cos_cached", emb.cos()[None, None, :, :], persistent=False
        )
        self.register_buffer(
            "sin_cached", emb.sin()[None, None, :, :], persistent=False
        )

    def forward(self, x, seq_len=None):
        # x: [bs, num_attention_heads, seq_len, head_size]
        # This `if` block is unlikely to be run after we build sin/cos in `__init__`. Keep the logic here just in case.
        if seq_len > self.max_seq_len_cached:
            self.max_seq_len_cached = seq_len
            t = torch.arange(
                self.max_seq_len_cached, device=x.device, dtype=self.inv_freq.dtype
            )
            freqs = torch.einsum("i,j->ij", t, self.inv_freq)
            # Different from paper, but it uses a different permutation in order to obtain the same calculation
            emb = torch.cat((freqs, freqs), dim=-1).to(x.device)
            self.register_buffer(
                "cos_cached", emb.cos()[None, None, :, :], persistent=False
            )
            self.register_buffer(
                "sin_cached", emb.sin()[None, None, :, :], persistent=False
            )
        return (
            self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),
            self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),
        )


def rotate_half(x):
    """Rotates half the hidden dims of the input."""
    x1 = x[..., : x.shape[-1] // 2]
    x2 = x[..., x.shape[-1] // 2 :]
    return torch.cat((-x2, x1), dim=-1)


def apply_rotary_pos_emb(q, k, cos, sin, position_ids):
    """
    Args:
         q: shape(batch_size,num_heads,seq_len,head_dim)
         k: shape(batch_size,num_heads,seq_len,head_dim)
         cos: shape(1,1,seq_len,head_dim)
         sin: shape(1,1,seq_len,head_dim)
         position_ids:shape(1,seq_len)
    Returns:
        q_embed: shape(batch_size,num_heads,seq_len,head_dim)
        k_embed: shape(batch_size,num_heads,seq_len,head_dim)
    """

    # The first two dimensions of cos and sin are always 1, so we can `squeeze` them.
    # print(position_ids)
    cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]
    sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]
    cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]
    sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]
    q_embed = (q * cos) + (rotate_half(q) * sin)
    k_embed = (k * cos) + (rotate_half(k) * sin)
    return q_embed, k_embed


batch_size = 2
num_heads = 32
seq_len = 64
head_dim = 128

rotary_emb = LlamaRotaryEmbedding(head_dim=head_dim, max_position_embeddings=2048)

value_states = torch.rand(batch_size, num_heads, seq_len, head_dim)
query_states = torch.rand(batch_size, num_heads, seq_len, head_dim)
key_states = torch.rand(batch_size, num_heads, seq_len, head_dim)
cos, sin = rotary_emb(value_states, seq_len=seq_len)
# print(cos.shape,sin.shape)
position_ids = torch.arange(seq_len)[None, :]
query_states, key_states = apply_rotary_pos_emb(
    query_states, key_states, cos, sin, position_ids
)

```

Instead of adding sinusoidal embeddings at the bottom of the transformer, they multiply the keys and queries of every attention layer by sinusoidal embeddings.

Unlike the sinusoidal or learned positional embedding approach, **the rotary method injects position information into the model at every layer, not just at the initial one.** In addition, it adds no position information to the values of the self-attention sublayer. The output of a self-attention sublayer is a linearly transformed, weighted sum of the input value vectors; therefore, by not inserting position information into the values, the outputs of each transformer-layer contain no explicit position information. We suspect that this segregation of position information may be beneficial for extrapolation, and we draw inspiration from it in the design of our method







