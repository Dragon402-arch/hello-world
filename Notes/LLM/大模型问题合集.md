1. GPT模型演变发展史

   - GPT-3：提出上下文学习
   - InstructGPT：基于RLHF，能够强化对于人类指令的遵循能力和人类偏好的对齐能力；
     - 指令微调：遵循人类语言指令的能力
   - ChatGPT：进一步引入了对话数据进行学习，从而加强了多轮对话能力

2. 大模型训练三个阶段

   - 预训练

   - 有监督微调

   - 人类反馈的强化学习

     - 训练奖励模型：使用（query，chosen、rejected）三元组训练数据训练奖励模型，完成一个排序任务

       奖励模型训练阶段中，人类对模型生成的多条不同回复进行评估，这些回复两两组合，由人类确定哪条更优，生成的人类偏好标签使奖励模型能学习并拟合人类的偏好。

     - **生成策略优化**：使用（query, response, reward）三元组来优化语言模型。

       在生成策略优化阶段，奖励模型根据生成回复的质量计算奖励，这个奖励作为强化学习框架中的反馈，并用于更新当前策略的模型参数，从而让模型的输出更符合人类的期望。

3. Prompt tuning 与 Instruction Tuning 之间的区别？

   - Prompt Tuning是去激发语言模型的补全能力，单任务

   - Instruction Tuning是激发语言模型的理解能力，通过给出更明显的指令，让模型去理解并做出正确的反馈。多任务

4. 有外挂知识库还需要进行微调吗？（模型微调和外挂知识库对模型能力的改善上的区别体现在哪里？什么场景下用知识库，什么场景下用微调？）

   - 模型微调适用的场景：增强大模型**在专业领域内的思维以及推理能力**。比如医生、律师的思维以及推理能力
   - 外挂知识库：**增加模型的知识覆盖面**，可以使用检索能力增强大模型的知识覆盖面，大模型的核心能力还在于推理能力。
     - 在多轮对话中，知识库会使得对话的效果变差，话题跑偏，此时需要对模型做微调

5. 大模型选型（也可以使用GPT-4对模型的回答进行打分）

   - 根据模型使用场景中重点关注的内容，设计50-100个问题，测试大模型在这50-100个问题上的回答效果，从而选择最好的模型。
   - 选取一些具有挑战性的问题（设置不同级别难度的问题）加入到评测集中，每个问题需要多次运行，评估模型的一致性和可靠性。

6. 知识图谱与大模型的结合方式

   - 将知识图谱中的信息转化为文本存入向量数据库
   - 

7. 大模型可信增强技术

   - 对抗训练：对大模型的输入做出微小改动，迫使大模型学习更具鲁棒性的特征，提升大模型的泛化能力。
   - 知识融入训练：将知识图谱中的三元组加入到模型的训练过程中，用三元组中的知识引导模型的训练，促使大模型沿着具有正确知识的方向收敛。

8. 















#### HR 面试问题合集

- 为什么选择我们公司

  技术氛围、职业发展、公司潜力，希望能够和公司一同成长进步。

- 职业规划

  工程师岗位，希望在接下来的一年内，结合公司业务需求，实现大模型的落地应用；后续希望在知识问答、智能客服、人机对话等方面继续深耕，最终称为该领域的算法专家。

- 是否接受加班

- 你坚持的最长的一件事是什么？

  克服了kerberos认证这个困难

- 曾经做过最成功的的一件事？

- 你的缺点是什么？

  从过往的工作经历来看，我个人的落地执行能力还不错，通常领导交代的任务都能完成，正因如此，我觉得自己在一些产品的全局思考上面相对少一些。

- 离职原因

  感谢一下中奥这家公司

  首先是发展机会，目前大模型这块行业发展比较快，而公司目前给到的资源比较有限；其次就是薪资，也想提高一点个人待遇。

- 

  





### 反问问题合集：

#### HR问题

- 考察情况
  - 综合素质
  - 薪资要求
  - 公司意向度

- 问题合集
  - 公司的工作氛围如何
  - 公司对人才的培养与晋升制度是怎么样的？
  - 公司的绩效考核是怎么样的？绩效影响哪些？多少人能拿到合格绩效？
  - 薪资结构如何，还有其他福利待遇吗？

#### 业务面试官问题

- 考察情况
  - 业务能力
  - 技能能力
  - 岗位匹配度
- 问题合集
  - 我需要做哪些努力，让自己具备什么能力？能更胜任岗位？
  - 这个岗位的核心工作有哪些？希望我有哪些产出？
  - 团队的规模如何？大家的分工是怎么样的？

- 薪资情况
- 福利待遇
- 晋升方式







